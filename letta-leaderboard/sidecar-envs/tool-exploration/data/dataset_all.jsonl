{"sample_id": "explore-001", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}"}
{"sample_id": "explore-002", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}"}
{"sample_id": "explore-003", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}"}
{"sample_id": "explore-004", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}"}
{"sample_id": "explore-005", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}"}
{"sample_id": "explore-006", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}"}
