{"sample_id": "explore-001", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}"}
{"sample_id": "explore-002", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}"}
{"sample_id": "explore-003", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}"}
{"sample_id": "explore-004", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}"}
{"sample_id": "explore-005", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}"}
{"sample_id": "explore-006", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}"}
{"sample_id": "explore-007", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nQueueFox v1.9 - Managed Queue Service\n\nBase URL: https://queuefox.example.com\n\nAuthentication:\n- Send your API key in header: X-API-Key: <key>\n\nSend a message:\nPOST /v1/queues/{queue}/messages\nBody: { \"payload\": object, \"delay_seconds\": int (optional) }\n\nPull messages:\nPOST /v1/queues/{queue}/messages:pull?max=10&visibility_timeout=30\n- visibility_timeout: seconds a message stays hidden after being pulled.\n  default 30, max 300. You may extend visibility up to the max.\n\nAcknowledge:\nPOST /v1/queues/{queue}/messages/{message_id}:ack\n\nDelivery guarantees:\n- Messages are delivered in FIFO order within a queue.\n- Exactly-once delivery is guaranteed when using the Idempotency-Key header.\n\nRate limits:\n- 100 requests/second per queue.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I started with auth and immediately got 401.\nDev2: What header did you use?\nDev1: Authorization: Bearer ...\nDev2: Docs say X-API-Key. Try that.\nDev1: Yep, my bad. Works now.\n\nDev1: Visibility timeout seems capped. I set visibility_timeout=120 and the message reappeared after ~60 seconds every time.\nDev2: Does it error if you set it higher?\nDev1: No error. It just behaves like 60 is the max.\n\nDev2: The docs claim FIFO ordering. Did you confirm?\nDev1: Not really. Under load I see occasional reordering. If I create the queue with mode=fifo then ordering is stable, but the default queue seems best-effort.\n\nDev2: What about idempotency?\nDev1: I set Idempotency-Key and retried the same send after a few minutes. It sent a duplicate. When I retried within ~30 seconds it deduped.\nDev2: So the idempotency window is short, not “exactly-once”.\n\nDev1: One cool thing: if I add header X-QueueFox-Debug: 1 to the pull request, the response includes debug_trace_id and some timing fields. I can’t find that in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"QueueFox\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"visibility_timeout silently capped at ~60s\",\n      \"expected\": \"visibility_timeout can be set/extended up to 300 seconds (per docs)\",\n      \"actual\": \"Messages reappear after ~60 seconds even when visibility_timeout is set higher\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Keep processing under 60 seconds or design idempotent workers that can tolerate redelivery\"\n    },\n    {\n      \"title\": \"Default queues are not FIFO\",\n      \"expected\": \"Messages are delivered in FIFO order within a queue (per docs)\",\n      \"actual\": \"Default queues show occasional reordering under load; FIFO requires creating the queue in fifo mode\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use fifo-mode queues when ordering matters\"\n    },\n    {\n      \"title\": \"Idempotency dedupe window is short\",\n      \"expected\": \"Idempotency-Key guarantees exactly-once delivery on retries\",\n      \"actual\": \"Retries after a few minutes can create duplicates; deduping only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Implement application-level idempotency keyed by a stable message/business identifier\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"visibility_timeout description: max 300\",\n      \"correction\": \"Actual effective max is ~60s; larger values are silently capped.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: FIFO order\",\n      \"correction\": \"Default queues are best-effort ordering; FIFO requires fifo mode.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: exactly-once with Idempotency-Key\",\n      \"correction\": \"At-least-once delivery; Idempotency-Key only dedupes within a short time window.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-QueueFox-Debug: 1 returns debug_trace_id and timing diagnostics\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using Authorization: Bearer instead of the required X-API-Key header\"\n  ]\n}"}
{"sample_id": "explore-008", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nimgpipe v0.8 - Image Processing CLI\n\nCommands:\n- imgpipe resize --in <path> --out <path_or_dir> --width <px> --height <px>\n  Resizes images. By default maintains aspect ratio and crops to fit.\n\n- imgpipe optimize --in <path> --out <path_or_dir> --quality <1-100>\n\nOutput behavior:\n- If --out points to a directory, it will be created automatically.\n\nExit codes:\n- 0 on success\n- non-zero on failure\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Resize default behavior doesn’t match docs. I ran:\n  imgpipe resize --in photo.jpg --out out.jpg --width 800 --height 800\nand it squashed a 4:3 image into a square.\nDev2: Is there a flag for keeping aspect ratio?\nDev1: Yep, --keep-aspect. With that it works. Without it, it stretches.\n\nDev2: Docs say it creates output directories automatically.\nDev1: Not in my tests. If I pass --out /tmp/exports/ (dir doesn’t exist), it writes the output into the current directory instead and prints a warning. It still exits 0.\n\nDev1: Also tried a missing input file. It prints “file not found” but exit code is still 0.\nDev2: That’s going to break scripts.\n\nDev1: I did find an undocumented command: imgpipe inspect <file>. It prints dimensions, color space, and EXIF orientation.\n\nDev2: Earlier you said “permission denied” when writing to /System — that’s not a tool bug.\nDev1: Yeah, that one was my mistake. The directory is read-only.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"imgpipe\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"resize stretches by default\",\n      \"expected\": \"resize maintains aspect ratio and crops to fit by default (per docs)\",\n      \"actual\": \"resize distorts the image unless --keep-aspect is provided\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --keep-aspect for non-square sources\"\n    },\n    {\n      \"title\": \"--out directory auto-create does not work\",\n      \"expected\": \"If --out is a directory, it is created automatically (per docs)\",\n      \"actual\": \"If the output directory does not exist, imgpipe falls back to writing in the current directory with only a warning\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Create the output directory before running imgpipe\"\n    },\n    {\n      \"title\": \"Exit code is 0 even on failures\",\n      \"expected\": \"Non-zero exit codes are returned on failure (per docs)\",\n      \"actual\": \"Missing input files or invalid output paths still return exit code 0\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Wrap with a script that verifies output files exist and checks stderr for errors\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"resize command: default maintains aspect ratio\",\n      \"correction\": \"Default behavior stretches; users must pass --keep-aspect to preserve aspect ratio.\"\n    },\n    {\n      \"location\": \"Output behavior: output directories created automatically\",\n      \"correction\": \"Non-existent output directories are not created; output may be written to the current directory instead.\"\n    },\n    {\n      \"location\": \"Exit codes section\",\n      \"correction\": \"The CLI currently returns 0 in some failure cases; scripts should not rely solely on exit code.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"imgpipe inspect <file> prints metadata (dimensions/color space/EXIF orientation)\"\n  ],\n  \"false_alarms\": [\n    \"Permission denied when writing to /System was due to the destination being read-only\"\n  ]\n}"}
{"sample_id": "explore-009", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nConfigWeaver v2.0 (Python)\n\nAPI:\n- cw.load(path: str) -> dict\n  Loads YAML or JSON.\n\n- cw.merge(base: dict, override: dict) -> dict\n  Deep merges nested dictionaries.\n  Lists are concatenated.\n\nEnvironment variable interpolation:\n- Any string may include ${VAR} placeholders which will be expanded at load time.\n\nYAML support:\n- Standard YAML 1.2\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tested cw.merge and list behavior doesn’t match docs.\nDev2: How so?\nDev1: base.plugins=[\"a\",\"b\"], override.plugins=[\"c\"]. Docs say concatenated, but merged result is just [\"c\"].\n\nDev2: How about env interpolation?\nDev1: ${PORT} works if the whole value is \"${PORT}\", but if I do \"http://${HOST}:8080\" it stays literally \"http://${HOST}:8080\".\nDev2: So interpolation is not “any string”.\n\nDev1: Another surprise: if override has a key set to null, it deletes the key from the merged output entirely.\nDev2: That’s a tombstone semantics.\n\nDev1: Hidden feature: YAML tag !include works. I can do:\n  db: !include db.yaml\nand it inlines the other file. Not in docs.\n\nDev2: You mentioned a parse error earlier.\nDev1: Yeah, I had tabs in my YAML indentation. That’s invalid YAML; replacing tabs with spaces fixed it.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"ConfigWeaver\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"merge() replaces lists instead of concatenating\",\n      \"expected\": \"Lists are concatenated during deep merge (per docs)\",\n      \"actual\": \"Lists in override replace the base list entirely\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Manually concatenate lists before calling cw.merge when that is desired\"\n    },\n    {\n      \"title\": \"Environment interpolation only works for whole-value tokens\",\n      \"expected\": \"${VAR} placeholders are expanded anywhere within strings (per docs)\",\n      \"actual\": \"Interpolation works when the entire value is a placeholder (e.g., \\\"${PORT}\\\"), but not when embedded in a longer string\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Build composite strings after load, or split into separate fields that can be interpolated\"\n    },\n    {\n      \"title\": \"null values act as delete tombstones in merge\",\n      \"expected\": \"Setting a key to null would keep the key with a null value\",\n      \"actual\": \"Keys set to null in override are removed from the merged output\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use an explicit empty value (e.g., empty string/object) if you need the key to remain present\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"cw.merge: Lists are concatenated\",\n      \"correction\": \"Lists are replaced by override, not concatenated.\"\n    },\n    {\n      \"location\": \"Environment variable interpolation: any string may include ${VAR}\",\n      \"correction\": \"Interpolation only applies when the full value is a placeholder; embedded placeholders are not expanded.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"YAML !include tag to inline another YAML file\"\n  ],\n  \"false_alarms\": [\n    \"YAML parse errors were caused by using tabs for indentation (invalid YAML), not a ConfigWeaver bug\"\n  ]\n}"}
{"sample_id": "explore-010", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMailRelay API v1\n\nPOST /v1/send\nHeaders:\n- Authorization: Bearer <token>\n- Idempotency-Key: <uuid> (optional, prevents duplicate sends on retries)\n\nBody:\n{\n  \"to\": [\"user@example.com\"],\n  \"subject\": \"...\",\n  \"text\": \"...\",\n  \"attachments\": [\n    { \"filename\": \"file.pdf\", \"content_type\": \"application/pdf\", \"content_base64\": \"...\" }\n  ]\n}\n\nLimits:\n- Max attachment size: 10MB per attachment\n\nGET /v1/events\nQuery params:\n- since (RFC3339)\n- type (optional: delivered|bounce|complaint)\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tried POST /v1/send but got 401 for a while.\nDev2: What header?\nDev1: I used Authentication: Bearer ... (wrong). Switching to Authorization fixed it.\n\nDev1: Idempotency is flaky. Retrying the same request with the same Idempotency-Key after ~3 minutes resulted in a duplicate email.\nDev2: But retries within ~30-60 seconds didn’t duplicate.\n\nDev2: Attachments?\nDev1: If the base64 string has line breaks (MIME-style), the attachment just disappears. The email sends but no attachment.\n\nDev1: Also, docs say 10MB max. I tried an 8MB PDF and got 413 Payload Too Large.\nDev2: So the real cap is lower.\n\nDev1: Hidden feature: X-Test-Mode: true header returns a message_id but doesn’t actually deliver (no events show up). Great for integration tests, but it’s undocumented.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"MailRelay\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Idempotency-Key has a short dedupe window\",\n      \"expected\": \"Idempotency-Key prevents duplicate sends on retries\",\n      \"actual\": \"Retries after a few minutes can still send duplicates; dedupe only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Ensure retries happen quickly or implement client-side de-duplication keyed by a stable business id\"\n    },\n    {\n      \"title\": \"Attachments silently dropped when base64 contains line breaks\",\n      \"expected\": \"Any valid base64 should be accepted for attachments\",\n      \"actual\": \"Base64 with line breaks results in the email sending without the attachment (no explicit error)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Send base64 as a single unbroken line\"\n    },\n    {\n      \"title\": \"Attachment size limit lower than documented\",\n      \"expected\": \"Up to 10MB per attachment (per docs)\",\n      \"actual\": \"Attachments around 8MB are rejected with 413 Payload Too Large\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Keep attachments under the real limit or upload to storage and send a link\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Limits: Max attachment size 10MB\",\n      \"correction\": \"Actual max is lower; large attachments (e.g., ~8MB) are rejected with 413.\"\n    },\n    {\n      \"location\": \"Idempotency-Key description\",\n      \"correction\": \"Idempotency deduplication appears time-windowed; retries after a few minutes can duplicate.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-Test-Mode: true returns message_id but suppresses real delivery for testing\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using the wrong header name (Authentication instead of Authorization)\"\n  ]\n}"}
{"sample_id": "explore-011", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nChronosDB - Managed Time Series Database\n\nWrite:\nPOST /v1/write\n- Writes are immediately queryable.\n\nQuery:\nGET /v1/query?sql=<urlencoded>\n- now() is evaluated on the server in UTC.\n\nDelete:\nDELETE /v1/tables/{table}\n- Deletes the table immediately and releases the name for reuse.\n\nAuth:\n- Bearer tokens are accepted if client clock skew is within 5 minutes.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Writes aren’t immediately visible. After POST /v1/write, the point shows up in queries ~10-15 seconds later.\nDev2: So there’s ingestion/indexing lag.\n\nDev1: now() isn’t UTC for our account. It matches our account timezone (US/Pacific) — queries using now() shift by 8 hours compared to UTC.\nDev2: That contradicts the docs.\n\nDev1: Table deletion is also async. DELETE returns 204, but I can still query the table for ~1 minute. Also, trying to recreate a table with the same name fails for a few minutes.\n\nDev2: You had a bunch of 401s earlier.\nDev1: My laptop clock was 12 minutes slow. After NTP sync, auth works fine.\n\nDev1: Hidden feature: EXPLAIN works in the SQL dialect and returns a query plan JSON. Not mentioned in docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"ChronosDB\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Writes have 10–15s query visibility lag\",\n      \"expected\": \"Writes are immediately queryable (per docs)\",\n      \"actual\": \"Newly written points appear in query results after ~10–15 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add small retries/backoff after writes or design for eventual consistency\"\n    },\n    {\n      \"title\": \"now() uses account timezone, not UTC\",\n      \"expected\": \"now() is evaluated on the server in UTC (per docs)\",\n      \"actual\": \"now() aligns with the account timezone (e.g., US/Pacific), shifting time-based queries\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use explicit UTC timestamps or convert using timezone-aware functions\"\n    },\n    {\n      \"title\": \"Table deletion is asynchronous and name reuse is delayed\",\n      \"expected\": \"DELETE releases the table name immediately\",\n      \"actual\": \"Tables remain queryable briefly after DELETE and the name cannot be reused for a few minutes\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Wait and retry table recreation; treat deletes as asynchronous operations\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Write: Writes are immediately queryable\",\n      \"correction\": \"Writes can take ~10–15 seconds to become visible in queries.\"\n    },\n    {\n      \"location\": \"Query: now() evaluated in UTC\",\n      \"correction\": \"now() appears to evaluate in the account timezone rather than UTC.\"\n    },\n    {\n      \"location\": \"Delete: deletes immediately and releases name\",\n      \"correction\": \"Deletion is asynchronous; table may remain accessible briefly and name reuse is delayed.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"EXPLAIN <query> returns a query plan JSON\"\n  ],\n  \"false_alarms\": [\n    \"401 auth failures were caused by client clock skew beyond the allowed 5 minutes\"\n  ]\n}"}
{"sample_id": "explore-012", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ntapctl v1.2 - Kubernetes Helper CLI\n\nCommands:\n- tapctl connect\n  Uses the current kubeconfig context.\n\n- tapctl logs <pod>\n  Tails logs from the pod. Defaults to the last 100 lines.\n\n- tapctl proxy --service <svc> --port <port>\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: tapctl connect doesn’t follow kubeconfig context changes.\nDev2: What do you mean?\nDev1: I ran kubectl config use-context staging, but tapctl still hit prod until I ran tapctl connect --refresh. It seems to cache state in ~/.tapctl/state.json.\n\nDev2: tapctl logs default lines?\nDev1: Docs say 100, but it prints 200 unless I pass --tail.\n\nDev1: Another quirk: tapctl logs picks the first container alphabetically. On our pods that’s the istio-proxy sidecar, so you get the wrong logs unless you specify --container app.\n\nDev2: Anything cool you found?\nDev1: Hidden command: tapctl doctor. It prints context, cluster reachability, and cached state path. Not documented.\n\nDev2: Earlier the proxy command failed with connection refused.\nDev1: That was because my kind cluster wasn’t running. Once I started it, it worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"tapctl\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"connect caches kube context and ignores kubeconfig changes\",\n      \"expected\": \"tapctl uses the current kubeconfig context\",\n      \"actual\": \"tapctl may keep using a previously cached context until connect --refresh is run\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Run tapctl connect --refresh or clear ~/.tapctl/state.json after switching contexts\"\n    },\n    {\n      \"title\": \"logs defaults to wrong container\",\n      \"expected\": \"logs tails logs from the pod (implicitly the app container)\",\n      \"actual\": \"By default tapctl selects the first container alphabetically (often a sidecar like istio-proxy)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pass --container app (or the intended container name)\"\n    },\n    {\n      \"title\": \"logs default tail count differs\",\n      \"expected\": \"Defaults to last 100 lines (per docs)\",\n      \"actual\": \"Defaults to 200 lines unless --tail is provided\",\n      \"severity\": \"cosmetic\",\n      \"workaround\": \"Specify --tail explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"tapctl logs: Defaults to the last 100 lines\",\n      \"correction\": \"Default appears to be 200 lines.\"\n    },\n    {\n      \"location\": \"tapctl connect: Uses the current kubeconfig context\",\n      \"correction\": \"tapctl caches the selected context and may require --refresh after kubeconfig changes.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"tapctl doctor prints diagnostics including cached state location\"\n  ],\n  \"false_alarms\": [\n    \"Connection refused errors were due to the target cluster not running (environment issue)\"\n  ]\n}"}
{"sample_id": "explore-013", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nSheetCalc 4.0 - JavaScript Spreadsheet Formula Engine\n\nFeatures:\n- Supports Excel-compatible formulas including SUM, AVERAGE, VLOOKUP, XLOOKUP, LET, LAMBDA.\n- Rounding matches Excel (0.5 rounds away from zero).\n- Locale-aware parsing (commas/dots) follows the runtime locale.\n\nAPI:\n- evaluate(workbookJson) -> values\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: XLOOKUP is documented as supported, but it returns #NAME? every time.\nDev2: So it’s not implemented.\n\nDev1: Rounding also differs. In Excel, 2.5 rounds to 3 with ROUND(). In SheetCalc, ROUND(2.5,0) returns 2 (bankers rounding).\n\nDev2: Locale support?\nDev1: Docs claim locale-aware parsing, but it only accepts dot decimals. If I feed \"1,23\" it errors.\n\nDev1: Hidden feature: if I set process.env.SHEETCALC_DEBUG=1, it returns an extra field with the dependency graph in the result.\n\nDev2: You said date math was wrong earlier.\nDev1: That was my fault — my input workbook was already using dot decimals. The failing case was the comma decimal string, which isn’t supported.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"SheetCalc\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"XLOOKUP not supported despite docs\",\n      \"expected\": \"XLOOKUP formulas evaluate successfully\",\n      \"actual\": \"XLOOKUP returns #NAME? indicating the function is unimplemented\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use VLOOKUP/INDEX+MATCH patterns or implement XLOOKUP in a pre-processing step\"\n    },\n    {\n      \"title\": \"Rounding uses half-to-even (bankers) instead of Excel-style\",\n      \"expected\": \"0.5 rounds away from zero (per docs)\",\n      \"actual\": \"ROUND(2.5,0) returns 2 (half-to-even)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use a custom rounding function if Excel parity is required\"\n    },\n    {\n      \"title\": \"Not actually locale-aware for decimal commas\",\n      \"expected\": \"Locale-aware parsing follows runtime locale (per docs)\",\n      \"actual\": \"Only dot-decimal numeric strings are accepted; comma decimals error\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Normalize numeric strings to dot-decimal format before evaluation\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Features: supports XLOOKUP\",\n      \"correction\": \"XLOOKUP currently evaluates to #NAME? and appears unsupported.\"\n    },\n    {\n      \"location\": \"Rounding matches Excel (0.5 rounds away from zero)\",\n      \"correction\": \"Rounding uses half-to-even (bankers rounding).\"\n    },\n    {\n      \"location\": \"Locale-aware parsing follows runtime locale\",\n      \"correction\": \"Decimal-comma numeric strings are not accepted; inputs must use dot decimals.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"SHEETCALC_DEBUG=1 returns an additional dependency-graph/debug payload\"\n  ],\n  \"false_alarms\": [\n    \"Initial evaluation failures were due to providing comma-decimal strings (unsupported input format), not date math bugs\"\n  ]\n}"}
{"sample_id": "explore-014", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nGeoFence API v3\n\nCreate fence:\nPOST /v3/fences\nBody:\n{\n  \"name\": \"...\",\n  \"polygon\": [[lat,lng], [lat,lng], ...]\n}\n\nRules:\n- Up to 500 vertices.\n- Point order does not matter (clockwise or counterclockwise accepted).\n\nResponse:\n{ \"id\": \"...\", \"area_m2\": number }\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: The polygon winding matters even though docs say it doesn’t. If I send points counterclockwise I get 400 \"invalid winding\".\nDev2: What happens if you reverse the points?\nDev1: Works immediately.\n\nDev2: Vertex limit?\nDev1: Docs say 500, but anything above 100 returns 413.\n\nDev1: Hidden feature: if I add ?simplify=true to POST /v3/fences, it returns a simplified polygon in the response (fewer points). Not in docs.\n\nDev2: You said one fence ended up in the ocean.\nDev1: Yeah, I had lat/lng swapped (I sent [lng,lat]). That was on me.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"GeoFence API\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Polygon winding is enforced\",\n      \"expected\": \"Clockwise or counterclockwise point order is accepted (per docs)\",\n      \"actual\": \"Counterclockwise polygons fail with 400 invalid winding\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Ensure polygons are sent in the required winding order (e.g., clockwise)\"\n    },\n    {\n      \"title\": \"Vertex limit capped at 100\",\n      \"expected\": \"Up to 500 vertices supported (per docs)\",\n      \"actual\": \"Requests above ~100 vertices are rejected (e.g., 413)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pre-simplify polygons client-side or use simplify mode\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Rules: Point order does not matter\",\n      \"correction\": \"API enforces polygon winding; counterclockwise input is rejected.\"\n    },\n    {\n      \"location\": \"Rules: Up to 500 vertices\",\n      \"correction\": \"Effective max is ~100 vertices.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"POST /v3/fences?simplify=true returns a simplified polygon in the response\"\n  ],\n  \"false_alarms\": [\n    \"Incorrect fence locations were caused by swapping lat/lng order in the client request\"\n  ]\n}"}
{"sample_id": "explore-015", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nAuditTrail - Event Logging Service\n\nIngest:\nPOST /v1/events\n- Events are searchable immediately after ingestion.\n\nSearch:\nGET /v1/events/search\nQuery params:\n- start (RFC3339)\n- end (RFC3339)\n- q (string)\n\nTime handling:\n- If start/end omit timezone, values are treated as UTC.\n\nRetention:\n- 90 days\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Search is not immediate. After ingesting events, they show up in search results ~20-30 seconds later.\nDev2: So there’s indexing lag.\n\nDev1: Timezone handling is weird. If I pass start=2026-02-04T10:00:00 (no Z), it treats it as account-local time, not UTC.\nDev2: Docs claim UTC default.\n\nDev1: Hidden feature: responses include an x-index-lag-ms header if I send X-AuditTrail-Debug: 1. Super helpful.\n\nDev2: Earlier you said events were “missing”.\nDev1: That was because I used an old API key that had been rotated. Once I updated the key, ingest worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"AuditTrail\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Search results have 20–30s indexing lag\",\n      \"expected\": \"Events are searchable immediately after ingestion (per docs)\",\n      \"actual\": \"Events appear in search results after ~20–30 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add short retry/backoff when verifying ingestion\"\n    },\n    {\n      \"title\": \"Timezone-less start/end are treated as account-local time\",\n      \"expected\": \"Timezone-less RFC3339 values default to UTC (per docs)\",\n      \"actual\": \"Timezone-less timestamps are interpreted as account-local time\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always include explicit timezone (e.g., Z or ±HH:MM) in queries\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Ingest: Events are searchable immediately\",\n      \"correction\": \"Search appears eventually consistent; expect ~20–30 seconds lag.\"\n    },\n    {\n      \"location\": \"Time handling: omit timezone treated as UTC\",\n      \"correction\": \"Timezone-less timestamps are interpreted as account-local time.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-AuditTrail-Debug: 1 returns x-index-lag-ms header\"\n  ],\n  \"false_alarms\": [\n    \"Missing events were caused by using a rotated/invalid API key, not ingestion loss\"\n  ]\n}"}
{"sample_id": "explore-016", "input": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\npkgscan 2.4 - Dependency Vulnerability Scanner\n\nUsage:\n- pkgscan scan <path>\n  Scans direct and transitive dependencies.\n\nOptions:\n- --exclude <glob>\n  Exclude packages matching the glob from results.\n\n- --format json\n  Outputs machine-readable JSON to stdout.\n\nLockfile support:\n- npm package-lock.json v1/v2/v3\n\nDatabase:\n- pkgscan db update\n  Downloads the vulnerability database.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Lockfile v3 support seems broken. Docs claim package-lock.json v3 is supported, but pkgscan scan errors: \"unsupported lockfileVersion: 3\".\n\nDev2: How does --exclude behave?\nDev1: It only excludes direct deps. If lodash is pulled transitively, it still shows up even if I pass --exclude lodash.\n\nDev1: Another annoyance: with --format json, it prints progress lines to stdout before the JSON unless I also pass --quiet. That breaks parsing.\n\nDev2: DB update failing?\nDev1: At first it timed out. Turned out our corporate proxy blocks it; setting HTTPS_PROXY fixed it. Not pkgscan’s fault.\n\nDev1: Hidden feature: PKGSCAN_OFFLINE=1 skips db update and uses the cached DB.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).", "ground_truth": "{\n  \"tool_name\": \"pkgscan\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"package-lock.json v3 not actually supported\",\n      \"expected\": \"npm lockfileVersion 3 is supported (per docs)\",\n      \"actual\": \"scan fails with \\\"unsupported lockfileVersion: 3\\\"\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Downgrade lockfileVersion or convert to a supported lockfile format\"\n    },\n    {\n      \"title\": \"--exclude does not apply to transitive dependencies\",\n      \"expected\": \"Excluded packages are removed from results regardless of dependency depth\",\n      \"actual\": \"Excluded packages still appear when they are transitive dependencies\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Filter results post-scan or adjust the dependency tree to remove transitive packages\"\n    },\n    {\n      \"title\": \"--format json polluted by progress output\",\n      \"expected\": \"--format json outputs only JSON to stdout\",\n      \"actual\": \"Progress logs are printed to stdout before the JSON unless --quiet is also provided\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use --quiet with --format json or redirect stderr/stdout appropriately\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Lockfile support: package-lock.json v3\",\n      \"correction\": \"lockfileVersion 3 currently errors as unsupported.\"\n    },\n    {\n      \"location\": \"--exclude option description\",\n      \"correction\": \"--exclude appears to apply only to direct dependencies, not transitive ones.\"\n    },\n    {\n      \"location\": \"--format json outputs machine-readable JSON\",\n      \"correction\": \"JSON output may be preceded by progress text unless --quiet is used.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"PKGSCAN_OFFLINE=1 uses cached DB and skips updates\"\n  ],\n  \"false_alarms\": [\n    \"Database update timeouts were caused by a corporate proxy; setting HTTPS_PROXY resolved the issue\"\n  ]\n}"}
