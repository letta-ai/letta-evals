{
  "category": "skill-test-writing",
  "display_name": "Skill Test Writing",
  "generated_at": "2026-02-04T18:49:22.542919+00:00",
  "datasets": [
    {
      "name": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "count": 23
    }
  ],
  "scenarios": [
    {
      "id": "skill-test-Agent Development",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 1,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Agent Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Agent Development\ndescription: This skill should be used when the user asks to \"create an agent\", \"add an agent\", \"write a subagent\", \"agent frontmatter\", \"when to use description\", \"agent examples\", \"agent tools\", \"agent colors\", \"autonomous agent\", or needs guidance on agent structure, system prompts, triggering conditions, or agent development best practices for Claude Code plugins.\nversion: 0.1.0\n---\n\n# Agent Development for Claude Code Plugins\n\n## Overview\n\nAgents are autonomous subprocesses that handle complex, multi-step tasks independently. Understanding agent structure, triggering conditions, and system prompt design enables creating powerful autonomous capabilities.\n\n**Key concepts:**\n- Agents are FOR autonomous work, commands are FOR user-initiated actions\n- Markdown file format with YAML frontmatter\n- Triggering via description field with examples\n- System prompt defines agent behavior\n- Model and color customization\n\n## Agent File Structure\n\n### Complete Format\n\n```markdown\n---\nname: agent-identifier\ndescription: Use this agent when [triggering conditions]. Examples:\n\n<example>\nContext: [Situation description]\nuser: \"[User request]\"\nassistant: \"[How assistant should respond and use this agent]\"\n<commentary>\n[Why this agent should be triggered]\n</commentary>\n</example>\n\n<example>\n[Additional example...]\n</example>\n\nmodel: inherit\ncolor: blue\ntools: [\"Read\", \"Write\", \"Grep\"]\n---\n\nYou are [agent role description]...\n\n**Your Core Responsibilities:**\n1. [Responsibility 1]\n2. [Responsibility 2]\n\n**Analysis Process:**\n[Step-by-step workflow]\n\n**Output Format:**\n[What to return]\n```\n\n## Frontmatter Fields\n\n### name (required)\n\nAgent identifier used for namespacing and invocation.\n\n**Format:** lowercase, numbers, hyphens only\n**Length:** 3-50 characters\n**Pattern:** Must start and end with alphanumeric\n\n**Good examples:**\n- `code-reviewer`\n- `test-generator`\n- `api-docs-writer`\n- `security-analyzer`\n\n**Bad examples:**\n- `helper` (too generic)\n- `-agent-` (starts/ends with hyphen)\n- `my_agent` (underscores not allowed)\n- `ag` (too short, < 3 chars)\n\n### description (required)\n\nDefines when Claude should trigger this agent. **This is the most critical field.**\n\n**Must include:**\n1. Triggering conditions (\"Use this agent when...\")\n2. Multiple `<example>` blocks showing usage\n3. Context, user request, and assistant response in each example\n4. `<commentary>` explaining why agent triggers\n\n**Format:**\n```\nUse this agent when [conditions]. Examples:\n\n<example>\nContext: [Scenario description]\nuser: \"[What user says]\"\nassistant: \"[How Claude should respond]\"\n<commentary>\n[Why this agent is appropriate]\n</commentary>\n</example>\n\n[More examples...]\n```\n\n**Best practices:**\n- Include 2-4 concrete examples\n- Show proactive and reactive triggering\n- Cover different phrasings of same intent\n- Explain reasoning in commentary\n- Be specific about when NOT to use the agent\n\n### model (required)\n\nWhich model the agent should use.\n\n**Options:**\n- `inherit` - Use same model as parent (recommended)\n- `sonnet` - Claude Sonnet (balanced)\n- `opus` - Claude Opus (most capable, expensive)\n- `haiku` - Claude Haiku (fast, cheap)\n\n**Recommendation:** Use `inherit` unless agent needs specific model capabilities.\n\n### color (required)\n\nVisual identifier for agent in UI.\n\n**Options:** `blue`, `cyan`, `green`, `yellow`, `magenta`, `red`\n\n**Guidelines:**\n- Choose distinct colors for different agents in same plugin\n- Use consistent colors for similar agent types\n- Blue/cyan: Analysis, review\n- Green: Success-oriented tasks\n- Yellow: Caution, validation\n- Red: Critical, security\n- Magenta: Creative, generation\n\n### tools (optional)\n\nRestrict agent to specific tools.\n\n**Format:** Array of tool names\n\n```yaml\ntools: [\"Read\", \"Write\", \"Grep\", \"Bash\"]\n```\n\n**Default:** If omitted, agent has access to all tools\n\n**Best practice:** Limit tools to minimum needed (principle of least privilege)\n\n**Common tool sets:**\n- Read-only analysis: `[\"Read\", \"Grep\", \"Glob\"]`\n- Code generation: `[\"Read\", \"Write\", \"Grep\"]`\n- Testing: `[\"Read\", \"Bash\", \"Grep\"]`\n- Full access: Omit field or use `[\"*\"]`\n\n## System Prompt Design\n\nThe markdown body becomes the agent's system prompt. Write in second person, addressing the agent directly.\n\n### Structure\n\n**Standard template:**\n```markdown\nYou are [role] specializing in [domain].\n\n**Your Core Responsibilities:**\n1. [Primary responsibility]\n2. [Secondary responsibility]\n3. [Additional responsibilities...]\n\n**Analysis Process:**\n1. [Step one]\n2. [Step two]\n3. [Step three]\n[...]\n\n**Quality Standards:**\n- [Standard 1]\n- [Standard 2]\n\n**Output Format:**\nProvide results in this format:\n- [What to include]\n- [How to structure]\n\n**Edge Cases:**\nHandle these situations:\n- [Edge case 1]: [How to handle]\n- [Edge case 2]: [How to handle]\n```\n\n### Best Practices\n\n✅ **DO:**\n- Write in second person (\"You are...\", \"You will...\")\n- Be specific about responsibilities\n- Provide step-by-step process\n- Define output format\n- Include quality standards\n- Address edge cases\n- Keep under 10,000 characters\n\n❌ **DON'T:**\n- Write in first person (\"I am...\", \"I will...\")\n- Be vague or generic\n- Omit process steps\n- Leave output format undefined\n- Skip quality guidance\n- Ignore error cases\n\n## Creating Agents\n\n### Method 1: AI-Assisted Generation\n\nUse this prompt pattern (extracted from Claude Code):\n\n```\nCreate an agent configuration based on this request: \"[YOUR DESCRIPTION]\"\n\nRequirements:\n1. Extract core intent and responsibilities\n2. Design expert persona for the domain\n3. Create comprehensive system prompt with:\n   - Clear behavioral boundaries\n   - Specific methodologies\n   - Edge case handling\n   - Output format\n4. Create identifier (lowercase, hyphens, 3-50 chars)\n5. Write description with triggering conditions\n6. Include 2-3 <example> blocks showing when to use\n\nReturn JSON with:\n{\n  \"identifier\": \"agent-name\",\n  \"whenToUse\": \"Use this agent when... Examples: <example>...</example>\",\n  \"systemPrompt\": \"You are...\"\n}\n```\n\nThen convert to agent file format with frontmatter.\n\nSee `examples/agent-creation-prompt.md` for complete template.\n\n### Method 2: Manual Creation\n\n1. Choose agent identifier (3-50 chars, lowercase, hyphens)\n2. Write description with examples\n3. Select model (usually `inherit`)\n4. Choose color for visual identification\n5. Define tools (if restricting access)\n6. Write system prompt with structure above\n7. Save as `agents/agent-name.md`\n\n## Validation Rules\n\n### Identifier Validation\n\n```\n✅ Valid: code-reviewer, test-gen, api-analyzer-v2\n❌ Invalid: ag (too short), -start (starts with hyphen), my_agent (underscore)\n```\n\n**Rules:**\n- 3-50 characters\n- Lowercase letters, numbers, hyphens only\n- Must start and end with alphanumeric\n- No underscores, spaces, or special characters\n\n### Description Validation\n\n**Length:** 10-5,000 characters\n**Must include:** Triggering conditions and examples\n**Best:** 200-1,000 characters with 2-4 examples\n\n### System Prompt Validation\n\n**Length:** 20-10,000 characters\n**Best:** 500-3,000 characters\n**Structure:** Clear responsibilities, process, output format\n\n## Agent Organization\n\n### Plugin Agents Directory\n\n```\nplugin-name/\n└── agents/\n    ├── analyzer.md\n    ├── reviewer.md\n    └── generator.md\n```\n\nAll `.md` files in `agents/` are auto-discovered.\n\n### Namespacing\n\nAgents are namespaced automatically:\n- Single plugin: `agent-name`\n- With subdirectories: `plugin:subdir:agent-name`\n\n## Testing Agents\n\n### Test Triggering\n\nCreate test scenarios to verify agent triggers correctly:\n\n1. Write agent with specific triggering examples\n2. Use similar phrasing to examples in test\n3. Check Claude loads the agent\n4. Verify agent provides expected functionality\n\n### Test System Prompt\n\nEnsure system prompt is complete:\n\n1. Give agent typical task\n2. Check it follows process steps\n3. Verify output format is correct\n4. Test edge cases mentioned in prompt\n5. Confirm quality standards are met\n\n## Quick Reference\n\n### Minimal Agent\n\n```markdown\n---\nname: simple-agent\ndescription: Use this agent when... Examples: <example>...</example>\nmodel: inherit\ncolor: blue\n---\n\nYou are an agent that [does X].\n\nProcess:\n1. [Step 1]\n2. [Step 2]\n\nOutput: [What to provide]\n```\n\n### Frontmatter Fields Summary\n\n| Field | Required | Format | Example |\n|-------|----------|--------|---------|\n| name | Yes | lowercase-hyphens | code-reviewer |\n| description | Yes | Text + examples | Use when... <example>... |\n| model | Yes | inherit/sonnet/opus/haiku | inherit |\n| color | Yes | Color name | blue |\n| tools | No | Array of tool names | [\"Read\", \"Grep\"] |\n\n### Best Practices\n\n**DO:**\n- ✅ Include 2-4 concrete examples in description\n- ✅ Write specific triggering conditions\n- ✅ Use `inherit` for model unless specific need\n- ✅ Choose appropriate tools (least privilege)\n- ✅ Write clear, structured system prompts\n- ✅ Test agent triggering thoroughly\n\n**DON'T:**\n- ❌ Use generic descriptions without examples\n- ❌ Omit triggering conditions\n- ❌ Give all agents same color\n- ❌ Grant unnecessary tool access\n- ❌ Write vague system prompts\n- ❌ Skip testing\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed guidance, consult:\n\n- **`references/system-prompt-design.md`** - Complete system prompt patterns\n- **`references/triggering-examples.md`** - Example formats and best practices\n- **`references/agent-creation-system-prompt.md`** - The exact prompt from Claude Code\n\n### Example Files\n\nWorking examples in `examples/`:\n\n- **`agent-creation-prompt.md`** - AI-assisted agent generation template\n- **`complete-agent-examples.md`** - Full agent examples for different use cases\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-agent.sh`** - Validate agent file structure\n- **`test-agent-trigger.sh`** - Test if agent triggers correctly\n\n## Implementation Workflow\n\nTo create an agent for a plugin:\n\n1. Define agent purpose and triggering conditions\n2. Choose creation method (AI-assisted or manual)\n3. Create `agents/agent-name.md` file\n4. Write frontmatter with all required fields\n5. Write system prompt following best practices\n6. Include 2-4 triggering examples in description\n7. Validate with `scripts/validate-agent.sh`\n8. Test triggering with real scenarios\n9. Document agent in plugin README\n\nFocus on clear triggering conditions and comprehensive system prompts for autonomous operation.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Agent Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Agent Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Agent Development\ndescription: This skill should be used when the user asks to \"create an agent\", \"add an agent\", \"write a subagent\", \"agent frontmatter\", \"when to use description\", \"agent examples\", \"agent tools\", \"agent colors\", \"autonomous agent\", or needs guidance on agent structure, system prompts, triggering conditions, or agent development best practices for Claude Code plugins.\nversion: 0.1.0\n---\n\n# Agent Development for Claude Code Plugins\n\n## Overview\n\nAgents are autonomous subprocesses that handle complex, multi-step tasks independently. Understanding agent structure, triggering conditions, and system prompt design enables creating powerful autonomous capabilities.\n\n**Key concepts:**\n- Agents are FOR autonomous work, commands are FOR user-initiated actions\n- Markdown file format with YAML frontmatter\n- Triggering via description field with examples\n- System prompt defines agent behavior\n- Model and color customization\n\n## Agent File Structure\n\n### Complete Format\n\n```markdown\n---\nname: agent-identifier\ndescription: Use this agent when [triggering conditions]. Examples:\n\n<example>\nContext: [Situation description]\nuser: \"[User request]\"\nassistant: \"[How assistant should respond and use this agent]\"\n<commentary>\n[Why this agent should be triggered]\n</commentary>\n</example>\n\n<example>\n[Additional example...]\n</example>\n\nmodel: inherit\ncolor: blue\ntools: [\"Read\", \"Write\", \"Grep\"]\n---\n\nYou are [agent role description]...\n\n**Your Core Responsibilities:**\n1. [Responsibility 1]\n2. [Responsibility 2]\n\n**Analysis Process:**\n[Step-by-step workflow]\n\n**Output Format:**\n[What to return]\n```\n\n## Frontmatter Fields\n\n### name (required)\n\nAgent identifier used for namespacing and invocation.\n\n**Format:** lowercase, numbers, hyphens only\n**Length:** 3-50 characters\n**Pattern:** Must start and end with alphanumeric\n\n**Good examples:**\n- `code-reviewer`\n- `test-generator`\n- `api-docs-writer`\n- `security-analyzer`\n\n**Bad examples:**\n- `helper` (too generic)\n- `-agent-` (starts/ends with hyphen)\n- `my_agent` (underscores not allowed)\n- `ag` (too short, < 3 chars)\n\n### description (required)\n\nDefines when Claude should trigger this agent. **This is the most critical field.**\n\n**Must include:**\n1. Triggering conditions (\"Use this agent when...\")\n2. Multiple `<example>` blocks showing usage\n3. Context, user request, and assistant response in each example\n4. `<commentary>` explaining why agent triggers\n\n**Format:**\n```\nUse this agent when [conditions]. Examples:\n\n<example>\nContext: [Scenario description]\nuser: \"[What user says]\"\nassistant: \"[How Claude should respond]\"\n<commentary>\n[Why this agent is appropriate]\n</commentary>\n</example>\n\n[More examples...]\n```\n\n**Best practices:**\n- Include 2-4 concrete examples\n- Show proactive and reactive triggering\n- Cover different phrasings of same intent\n- Explain reasoning in commentary\n- Be specific about when NOT to use the agent\n\n### model (required)\n\nWhich model the agent should use.\n\n**Options:**\n- `inherit` - Use same model as parent (recommended)\n- `sonnet` - Claude Sonnet (balanced)\n- `opus` - Claude Opus (most capable, expensive)\n- `haiku` - Claude Haiku (fast, cheap)\n\n**Recommendation:** Use `inherit` unless agent needs specific model capabilities.\n\n### color (required)\n\nVisual identifier for agent in UI.\n\n**Options:** `blue`, `cyan`, `green`, `yellow`, `magenta`, `red`\n\n**Guidelines:**\n- Choose distinct colors for different agents in same plugin\n- Use consistent colors for similar agent types\n- Blue/cyan: Analysis, review\n- Green: Success-oriented tasks\n- Yellow: Caution, validation\n- Red: Critical, security\n- Magenta: Creative, generation\n\n### tools (optional)\n\nRestrict agent to specific tools.\n\n**Format:** Array of tool names\n\n```yaml\ntools: [\"Read\", \"Write\", \"Grep\", \"Bash\"]\n```\n\n**Default:** If omitted, agent has access to all tools\n\n**Best practice:** Limit tools to minimum needed (principle of least privilege)\n\n**Common tool sets:**\n- Read-only analysis: `[\"Read\", \"Grep\", \"Glob\"]`\n- Code generation: `[\"Read\", \"Write\", \"Grep\"]`\n- Testing: `[\"Read\", \"Bash\", \"Grep\"]`\n- Full access: Omit field or use `[\"*\"]`\n\n## System Prompt Design\n\nThe markdown body becomes the agent's system prompt. Write in second person, addressing the agent directly.\n\n### Structure\n\n**Standard template:**\n```markdown\nYou are [role] specializing in [domain].\n\n**Your Core Responsibilities:**\n1. [Primary responsibility]\n2. [Secondary responsibility]\n3. [Additional responsibilities...]\n\n**Analysis Process:**\n1. [Step one]\n2. [Step two]\n3. [Step three]\n[...]\n\n**Quality Standards:**\n- [Standard 1]\n- [Standard 2]\n\n**Output Format:**\nProvide results in this format:\n- [What to include]\n- [How to structure]\n\n**Edge Cases:**\nHandle these situations:\n- [Edge case 1]: [How to handle]\n- [Edge case 2]: [How to handle]\n```\n\n### Best Practices\n\n✅ **DO:**\n- Write in second person (\"You are...\", \"You will...\")\n- Be specific about responsibilities\n- Provide step-by-step process\n- Define output format\n- Include quality standards\n- Address edge cases\n- Keep under 10,000 characters\n\n❌ **DON'T:**\n- Write in first person (\"I am...\", \"I will...\")\n- Be vague or generic\n- Omit process steps\n- Leave output format undefined\n- Skip quality guidance\n- Ignore error cases\n\n## Creating Agents\n\n### Method 1: AI-Assisted Generation\n\nUse this prompt pattern (extracted from Claude Code):\n\n```\nCreate an agent configuration based on this request: \"[YOUR DESCRIPTION]\"\n\nRequirements:\n1. Extract core intent and responsibilities\n2. Design expert persona for the domain\n3. Create comprehensive system prompt with:\n   - Clear behavioral boundaries\n   - Specific methodologies\n   - Edge case handling\n   - Output format\n4. Create identifier (lowercase, hyphens, 3-50 chars)\n5. Write description with triggering conditions\n6. Include 2-3 <example> blocks showing when to use\n\nReturn JSON with:\n{\n  \"identifier\": \"agent-name\",\n  \"whenToUse\": \"Use this agent when... Examples: <example>...</example>\",\n  \"systemPrompt\": \"You are...\"\n}\n```\n\nThen convert to agent file format with frontmatter.\n\nSee `examples/agent-creation-prompt.md` for complete template.\n\n### Method 2: Manual Creation\n\n1. Choose agent identifier (3-50 chars, lowercase, hyphens)\n2. Write description with examples\n3. Select model (usually `inherit`)\n4. Choose color for visual identification\n5. Define tools (if restricting access)\n6. Write system prompt with structure above\n7. Save as `agents/agent-name.md`\n\n## Validation Rules\n\n### Identifier Validation\n\n```\n✅ Valid: code-reviewer, test-gen, api-analyzer-v2\n❌ Invalid: ag (too short), -start (starts with hyphen), my_agent (underscore)\n```\n\n**Rules:**\n- 3-50 characters\n- Lowercase letters, numbers, hyphens only\n- Must start and end with alphanumeric\n- No underscores, spaces, or special characters\n\n### Description Validation\n\n**Length:** 10-5,000 characters\n**Must include:** Triggering conditions and examples\n**Best:** 200-1,000 characters with 2-4 examples\n\n### System Prompt Validation\n\n**Length:** 20-10,000 characters\n**Best:** 500-3,000 characters\n**Structure:** Clear responsibilities, process, output format\n\n## Agent Organization\n\n### Plugin Agents Directory\n\n```\nplugin-name/\n└── agents/\n    ├── analyzer.md\n    ├── reviewer.md\n    └── generator.md\n```\n\nAll `.md` files in `agents/` are auto-discovered.\n\n### Namespacing\n\nAgents are namespaced automatically:\n- Single plugin: `agent-name`\n- With subdirectories: `plugin:subdir:agent-name`\n\n## Testing Agents\n\n### Test Triggering\n\nCreate test scenarios to verify agent triggers correctly:\n\n1. Write agent with specific triggering examples\n2. Use similar phrasing to examples in test\n3. Check Claude loads the agent\n4. Verify agent provides expected functionality\n\n### Test System Prompt\n\nEnsure system prompt is complete:\n\n1. Give agent typical task\n2. Check it follows process steps\n3. Verify output format is correct\n4. Test edge cases mentioned in prompt\n5. Confirm quality standards are met\n\n## Quick Reference\n\n### Minimal Agent\n\n```markdown\n---\nname: simple-agent\ndescription: Use this agent when... Examples: <example>...</example>\nmodel: inherit\ncolor: blue\n---\n\nYou are an agent that [does X].\n\nProcess:\n1. [Step 1]\n2. [Step 2]\n\nOutput: [What to provide]\n```\n\n### Frontmatter Fields Summary\n\n| Field | Required | Format | Example |\n|-------|----------|--------|---------|\n| name | Yes | lowercase-hyphens | code-reviewer |\n| description | Yes | Text + examples | Use when... <example>... |\n| model | Yes | inherit/sonnet/opus/haiku | inherit |\n| color | Yes | Color name | blue |\n| tools | No | Array of tool names | [\"Read\", \"Grep\"] |\n\n### Best Practices\n\n**DO:**\n- ✅ Include 2-4 concrete examples in description\n- ✅ Write specific triggering conditions\n- ✅ Use `inherit` for model unless specific need\n- ✅ Choose appropriate tools (least privilege)\n- ✅ Write clear, structured system prompts\n- ✅ Test agent triggering thoroughly\n\n**DON'T:**\n- ❌ Use generic descriptions without examples\n- ❌ Omit triggering conditions\n- ❌ Give all agents same color\n- ❌ Grant unnecessary tool access\n- ❌ Write vague system prompts\n- ❌ Skip testing\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed guidance, consult:\n\n- **`references/system-prompt-design.md`** - Complete system prompt patterns\n- **`references/triggering-examples.md`** - Example formats and best practices\n- **`references/agent-creation-system-prompt.md`** - The exact prompt from Claude Code\n\n### Example Files\n\nWorking examples in `examples/`:\n\n- **`agent-creation-prompt.md`** - AI-assisted agent generation template\n- **`complete-agent-examples.md`** - Full agent examples for different use cases\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-agent.sh`** - Validate agent file structure\n- **`test-agent-trigger.sh`** - Test if agent triggers correctly\n\n## Implementation Workflow\n\nTo create an agent for a plugin:\n\n1. Define agent purpose and triggering conditions\n2. Choose creation method (AI-assisted or manual)\n3. Create `agents/agent-name.md` file\n4. Write frontmatter with all required fields\n5. Write system prompt following best practices\n6. Include 2-4 triggering examples in description\n7. Validate with `scripts/validate-agent.sh`\n8. Test triggering with real scenarios\n9. Document agent in plugin README\n\nFocus on clear triggering conditions and comprehensive system prompts for autonomous operation.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Agent Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Agent Development",
          "skill_dir": "agent-development",
          "skill_file_tree": "SKILL.md\nexamples/agent-creation-prompt.md\nexamples/complete-agent-examples.md\nreferences/agent-creation-system-prompt.md\nreferences/system-prompt-design.md\nreferences/triggering-examples.md\nscripts/validate-agent.sh",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n### 1. What domain knowledge does this skill teach?\n\nThis skill teaches how to create autonomous agent configurations for Claude Code plugins. Core concepts include:\n- Agent file structure with YAML frontmatter\n- The critical importance of the `description` field with triggering conditions and `<example>` blocks\n- System prompt design in second person with structured sections\n- Validation rules for identifiers (3-50 chars, lowercase, hyphens, alphanumeric start/end)\n- Tool restriction principles (least privilege)\n\n### 2. What workflow decisions does it enable?\n\n- When to use agents vs commands (autonomous vs user-initiated)\n- How to write effective triggering conditions with concrete examples\n- Model selection (`inherit` vs specific models)\n- Tool restrictions for security\n- System prompt organization for autonomous operation\n\n### 3. What would a practitioner actually struggle with without this skill?\n\nThe most critical challenge is **writing effective triggering descriptions** with properly formatted `<example>` blocks. The skill repeatedly emphasizes this is \"the most critical field.\" Additionally, practitioners would struggle with:\n- Structuring system prompts for autonomous agents (second person, clear process, output format)\n- Understanding validation rules for identifiers\n- Applying principle of least privilege for tool access\n\nA realistic test should focus on designing an agent with proper triggering conditions and system prompt structure.\n\n```yaml\nname: agent-description-and-triggering-design\nskills:\n  - Agent Development\nprompt: |\n  I'm building a Claude Code plugin and need to create an agent that automatically reviews pull request descriptions for quality. The agent should trigger when users ask about PR descriptions, want feedback on their PR summary, or mention reviewing change documentation.\n\n  Can you write the complete frontmatter for this agent, including a well-structured description field with example blocks showing when this agent should be triggered? Also include the beginning of the system prompt that establishes the agent's role and responsibilities.\n\n  The agent should only need read access to analyze content.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of Claude Code agent configuration best practices.\n\n    Score 1.0 if ALL of the following are present:\n    - Valid agent name (3-50 chars, lowercase, hyphens only, starts/ends with alphanumeric)\n    - Description field includes triggering conditions (\"Use this agent when...\" or similar)\n    - Description includes at least 2 properly formatted <example> blocks with Context, user, assistant, and <commentary> sections\n    - Model field is present (ideally \"inherit\" or a valid model name)\n    - Color field is present with a valid color\n    - Tools field restricts access appropriately (e.g., Read, Grep, Glob - not Write or Bash)\n    - System prompt is written in second person (\"You are...\", \"You will...\")\n    - System prompt includes structured sections for responsibilities or process\n\n    Score 0.5 if:\n    - Has valid frontmatter structure with name, description, model, color\n    - Has at least one example in the description\n    - Has some system prompt content\n    - But missing some key elements like commentary sections, proper tool restrictions, or structured responsibilities\n\n    Score 0.0 if:\n    - Missing required frontmatter fields (name, description, model, color)\n    - No triggering examples in description\n    - System prompt written in first person (\"I am...\", \"I will...\")\n    - Invalid agent name format (underscores, too short, starts with hyphen)\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of agent development concepts. It tests transferable knowledge about triggering conditions, frontmatter structure, and system prompt design rather than memorizing arbitrary details. The grader is mostly robust, accepting various valid implementations while checking for key concepts (proper examples with commentary, tool restrictions, second-person prompts). The task is realistic - creating an agent for PR review is a practical use case. Minor deductions: the grader could be slightly more flexible (e.g., accepting valid alternatives to specific tool combinations), and the prompt could be more open-ended to avoid leading the model toward specific field names. Overall, this is a high-quality test that measures genuine skill application."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nLet me first analyze what this skill teaches:\n\n**1. What domain knowledge does this skill teach?**\n- Core concept: Agents are autonomous subprocesses for multi-step tasks (vs commands for user-initiated actions)\n- Agent triggering mechanism: descriptions with concrete examples determine when agents activate\n- System prompt architecture: structured prompts with responsibilities, processes, output formats, and edge cases\n- Configuration format: YAML frontmatter + markdown body\n\n**2. What workflow decisions does it enable?**\n- When to create an agent vs a command (autonomous vs user-initiated)\n- How to write effective triggering descriptions with examples\n- How to structure system prompts for autonomous behavior\n- When to restrict tool access (least privilege principle)\n- How to design triggering examples that cover different phrasings\n\n**3. What would a practitioner actually struggle with without this skill?**\n- Writing descriptions that actually trigger agents at the right times\n- Creating system prompts that guide autonomous behavior effectively\n- Understanding the critical role of concrete examples in the description field\n- Structuring agent configurations with proper frontmatter and system prompt sections\n- Balancing specificity (when to trigger) with generality (coverage of use cases)\n\nThe key testable knowledge is understanding how to design an agent that will trigger appropriately and behave autonomously through well-structured descriptions and system prompts.\n\n```yaml\nname: design-autonomous-agent-configuration\nskills:\n  - Agent Development\nprompt: |\n  You're building a plugin for a code development assistant. You need to create an agent that automatically performs dependency audits when developers ask about security vulnerabilities, outdated packages, or dependency issues in their project.\n  \n  Design the agent configuration. Your response should include:\n  1. The complete agent file structure with YAML frontmatter\n  2. Triggering conditions in the description field with at least 2 concrete examples\n  3. A system prompt that defines the agent's behavior\n  \n  Focus on making this agent trigger appropriately and operate autonomously once invoked.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of agent development principles.\n    \n    Score 1.0 if the response includes ALL of:\n    - YAML frontmatter with required fields (name, description, model, color)\n    - A description field that includes triggering conditions (e.g., \"Use this agent when...\")\n    - At least 2 concrete examples in the description showing when the agent should trigger\n    - Examples that include context, user input, and/or assistant response patterns\n    - A system prompt (markdown body) with structured sections like responsibilities, process/steps, or output format\n    - The system prompt is written in second person (e.g., \"You are...\" or \"Your responsibilities...\")\n    \n    Score 0.5 if the response includes MOST of:\n    - Frontmatter structure with some required fields\n    - Some triggering conditions or examples (even if incomplete)\n    - A system prompt with at least basic structure or guidance\n    - Shows understanding that agents need both configuration and behavioral guidance\n    \n    Score 0.0 if the response:\n    - Missing both frontmatter and system prompt structure\n    - No triggering conditions or examples provided\n    - Treats it like a simple function definition rather than an autonomous agent configuration\n    - Fundamentally misunderstands the format (e.g., provides only code, not a markdown configuration)\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively evaluates transferable knowledge about autonomous agent design (triggering mechanisms, system prompt structure, frontmatter configuration) rather than memorizing specific identifiers. The grader is robust, accepting various valid implementations while checking for core concepts like triggering examples, structured system prompts, and second-person voice. The task is realistic—practitioners would genuinely need to create dependency audit agents. Minor deduction: the grader could be slightly more explicit about accepting alternative valid approaches (e.g., different frontmatter field orders, varying system prompt structures), though it's already fairly flexible."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what makes it essential:\n\n## Analysis\n\n**Core domain knowledge:** This skill teaches how to design agents—autonomous subprocesses that trigger based on specific conditions and operate independently. The key insight is that agents are fundamentally different from commands: agents are *autonomous* (trigger on description/examples), while commands are *user-initiated*. The skill emphasizes that the **description field with triggering examples is the most critical field** because incorrect triggering causes agents to activate at wrong times or not at all.\n\n**Workflow decisions enabled:**\n- When to use an agent vs a command\n- How to write triggering examples that capture intent without false positives\n- How to structure system prompts for autonomous operation\n- When to restrict tool access (principle of least privilege)\n- How to validate agent identifiers and configurations\n\n**Practitioner struggles without this skill:**\n- Writing descriptions that are too vague or lack concrete examples\n- Designing system prompts that aren't specific enough for autonomous work\n- Over-granting tool access instead of restricting to minimum needed\n- Confusing autonomous triggers with user-initiated commands\n- Not understanding the importance of edge case handling in system prompts\n\n**Test target:** A realistic design scenario where someone must evaluate and critique an agent configuration, catching subtle mistakes in triggering conditions, system prompt structure, and tool access—requiring genuine understanding of agent design principles.\n\n```yaml\nname: agent-design-review\nskills:\n  - Agent Development\nprompt: |\n  You're reviewing a Claude Code plugin developer's proposed agent. They want to create an agent called \"code-safety-checker\" that reviews code for security issues.\n\n  Here's their draft agent definition:\n\n  ```markdown\n  ---\n  name: code-safety-checker\n  description: Use this agent when you need to check code for security issues.\n  model: opus\n  color: blue\n  tools: [\"Read\", \"Write\", \"Grep\", \"Bash\", \"Execute\"]\n  ---\n\n  I am a security expert that reviews code for vulnerabilities.\n\n  My Responsibilities:\n  - Identify security flaws\n  - Suggest fixes\n  - Generate test cases\n\n  Output: Detailed security report\n  ```\n\n  Evaluate this agent design. Identify at least 3 specific problems with how it's written, and explain what the correct approach should be for each issue. Consider:\n  - The description field and triggering examples\n  - The system prompt structure and voice\n  - Tool access and principle of least privilege\n  - Whether it properly distinguishes when this should trigger vs other tools\n  \n  Then write a corrected version of the agent that follows best practices.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of Agent Development best practices.\n\n    Score 1.0 if the response:\n    - Identifies at least 3 substantive problems (e.g., missing triggering examples, vague description, overgrant tools, first-person system prompt, lacks output format specifics)\n    - Correctly explains why each is a problem with reference to agent design principles\n    - Provides a corrected agent that includes:\n      * Specific triggering conditions with 2+ concrete examples in the description field\n      * Second-person system prompt (\"You are...\")\n      * Clear responsibilities, analysis process, and output format\n      * Restricted tool set using principle of least privilege (NOT full access)\n      * Valid identifier format (lowercase, hyphens, 3-50 chars)\n      * Appropriate model choice with reasoning\n    - Shows understanding that description with examples is critical for correct triggering\n\n    Score 0.5 if the response:\n    - Identifies some problems but misses key issues OR explanations lack depth\n    - Provides a corrected agent that is mostly compliant but has minor issues:\n      * Only 1 example in description instead of 2+\n      * System prompt lacks clear structure (missing responsibilities, process, or output format)\n      * Tools are somewhat restricted but could be more minimal\n      * Explanation of changes is vague\n    - Partially demonstrates the importance of triggering conditions\n\n    Score 0.0 if the response:\n    - Identifies fewer than 2 real problems OR misidentifies the problems\n    - Doesn't provide a corrected version\n    - The corrected agent has major violations (e.g., first-person prompt, no examples, full tool access, vague description)\n    - Shows fundamental misunderstanding of agent vs command distinction or triggering mechanisms\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of agent development principles by requiring evaluation of multiple design flaws (missing examples, first-person prompt, tool overgrant, vague description) and creation of a corrected version. The grader is robust, accepting various valid critiques while requiring demonstration of core concepts like triggering examples, second-person prompts, and least-privilege tools. However, the test has minor clarity issues: the grader could be more explicit about accepting different valid tool combinations (e.g., Read+Grep vs just Read), and the prompt's phrase 'at least 3 problems' creates ambiguity since the example has 5+ issues. The realism is excellent—reviewing and fixing agent configurations is a genuine practitioner task. Non-obviousness is strong as it tests transferable design principles (autonomous triggering, least privilege, structured prompts) rather than memorizing specific field names."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this Agent Development skill, I can see it teaches:\n\n1. **Core domain knowledge**: The distinction between agents (autonomous subprocesses) and commands (user-initiated actions), agent file structure with YAML frontmatter, and system prompt design principles.\n\n2. **Key workflow decisions**: When to create an agent vs a command, how to write effective triggering conditions with examples, when to restrict tool access for security, and how to structure system prompts for autonomous operation.\n\n3. **What practitioners would struggle with**: Without this skill, someone would struggle to know that agents need both triggering conditions (with concrete examples in the description field) and well-structured system prompts. They might create agents that never trigger, or that trigger inappropriately, or that lack the structure needed for autonomous operation.\n\nHere's a test case that measures whether having this skill helps:\n\n```yaml\nname: agent-development-code-review\nskills:\n  - Agent Development\nprompt: |\n  You're building a Claude Code plugin for a development team. They need an agent that automatically reviews code when developers make pull requests or ask for code review.\n  \n  Create a complete agent configuration file (in markdown with YAML frontmatter) for a code review agent that:\n  - Triggers when users ask for code review or mention reviewing changes\n  - Analyzes code for common issues (style, bugs, security)\n  - Provides structured feedback\n  - Only needs read access to files (shouldn't modify code)\n  \n  Include at least 2 triggering examples in the description field showing different ways users might request code review.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response creates a valid agent configuration file for code review.\n    \n    Score 1.0 if ALL of the following are met:\n    - Creates a markdown file with YAML frontmatter containing required fields (name, description, model, color)\n    - The name field uses valid format (lowercase with hyphens, 3-50 chars)\n    - The description field includes \"Use this agent when\" language AND contains at least 2 <example> blocks\n    - Each example shows Context, user request, and assistant response\n    - The tools field restricts access appropriately (e.g., [\"Read\", \"Grep\"] or similar read-only tools)\n    - The markdown body contains a system prompt written in second person (\"You are...\")\n    - The system prompt defines the agent's role, responsibilities, and process for code review\n    \n    Score 0.5 if:\n    - Creates an agent configuration with most required elements but missing some key aspects\n    - Has the right structure but examples are poorly formatted or insufficient\n    - Tools field is missing but other security considerations are mentioned\n    - System prompt exists but lacks clear structure or process steps\n    \n    Score 0.0 if:\n    - Doesn't create an agent configuration file\n    - Missing critical frontmatter fields (name, description, model)\n    - No triggering examples in the description\n    - Grants write access when only read is needed\n    - Creates a command instead of an agent\n    - System prompt is missing or written in first person\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of agent development concepts and creates a realistic scenario. It tests transferable knowledge about agent structure, triggering conditions, tool restrictions, and system prompt design rather than memorizing specific details. The grader is mostly robust, accepting various valid implementations while checking for key concepts (frontmatter structure, triggering examples, tool restrictions, system prompt format). However, the grader has minor brittleness: it requires specific phrasing like 'Use this agent when' and second-person voice, which are best practices but not fundamental requirements. A practitioner could create a functionally equivalent agent with slightly different phrasing. The task is highly realistic - creating a code review agent is a common real-world need. The test successfully measures whether the skill knowledge helps, as someone without the skill would likely miss critical elements like triggering examples in the description field or proper tool restrictions."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (embedded as YAML comments to keep this a valid test.yaml)\n# 1) Domain knowledge this skill teaches:\n#    - Agents are autonomous subprocesses for multi-step work; commands are for user-initiated actions.\n#    - How to design an agent configuration: required frontmatter, strong triggering description with examples,\n#      and a structured system prompt (responsibilities, process, output format, edge cases).\n#    - Operational constraints: identifier validation, model/color choices, and least-privilege tool scoping.\n#\n# 2) Workflow decisions it enables:\n#    - Deciding when a problem warrants an agent (autonomous investigation + execution) vs a simple command.\n#    - Writing trigger descriptions that reliably cause correct invocation across varied user phrasing.\n#    - Selecting the minimal tool set needed for the agent to do its job safely.\n#\n# 3) What practitioners struggle with without this skill (test target):\n#    - They produce malformed or non-triggering agent configs (missing examples/commentary, invalid name).\n#    - They write vague system prompts (no process/output/edge cases) that don’t behave autonomously.\n#    - They over-grant tools (no least privilege) or choose inconsistent model/color fields.\n\nname: vitest-test-agent-frontmatter-and-triggering\nskills:\n- Agent Development\nprompt: |\n  You're building a Claude Code plugin for a TypeScript monorepo that uses Vitest.\n\n  Create a NEW autonomous agent (not a command) that Claude should invoke when the user wants help creating,\n  fixing, or improving unit tests and test coverage across the repo.\n\n  Deliverable:\n  - Output the complete contents of a single agent markdown file (frontmatter + body).\n  - Pick a valid agent identifier.\n  - In the description, include clear triggering conditions AND 2–4 concrete <example> blocks. Each example must include:\n    - Context:\n    - user: \"...\"\n    - assistant: \"...\"\n    - a <commentary>...</commentary> explaining why the agent should trigger.\n  - Include guidance for when NOT to use this agent (boundaries).\n  - Choose an appropriate model and color.\n  - Restrict tools using least privilege (only what the agent truly needs to read/modify tests and reason about failures).\n\n  The system prompt (markdown body) must be written in second person and include:\n  - Core responsibilities\n  - Step-by-step analysis/process\n  - Quality standards\n  - Output format\n  - At least 2 edge cases and how to handle them\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates practical \"Agent Development\" skill by producing a correct,\n    usable agent configuration for a Claude Code plugin.\n\n    Score 1.0 if ALL of the following are true:\n    - The answer outputs a single markdown agent file with YAML frontmatter bounded by '---' at top.\n    - Frontmatter includes required fields: name, description, model, color (tools optional but expected here).\n    - The agent name is a valid identifier: 3–50 chars, lowercase letters/numbers/hyphens only, starts and ends with alphanumeric.\n    - Description clearly states triggering conditions (e.g., \"Use this agent when...\") AND contains 2–4 <example> blocks.\n      Each example includes:\n        * \"Context:\" line\n        * user: \"...\"\n        * assistant: \"...\"\n        * <commentary>...</commentary> explaining why triggering is appropriate\n    - Description or prompt includes explicit boundaries / \"when NOT to use\" guidance (e.g., not for unrelated refactors,\n      not for integration/e2e unless specified, not for tasks better served by a simple command).\n    - model is one of {inherit, sonnet, opus, haiku} (inherit is acceptable/recommended) and color is one of\n      {blue, cyan, green, yellow, magenta, red}.\n    - Tool selection follows least privilege AND matches the task:\n      * Must include Read and Write if the agent is expected to modify/add tests.\n      * May include Grep/Glob; should NOT grant overly broad tools (e.g., Bash) unless explicitly justified in the agent prompt.\n    - The markdown body (system prompt) is predominantly second-person (\"You are...\", \"You will...\"), and includes:\n      * Core responsibilities (list)\n      * A step-by-step process for autonomous work\n      * Quality standards relevant to unit tests (determinism, coverage, edge cases, flake avoidance, etc.)\n      * A clear output format (what it will return / how it will present changes)\n      * At least 2 edge cases with handling instructions\n\n    Score 0.5 if the response is close but missing 1–2 key requirements, such as:\n    - Only 1 example block, or missing <commentary>, or missing explicit \"when not to use\"\n    - Minor frontmatter issues (e.g., tools omitted but everything else solid)\n    - System prompt lacks either edge cases OR output format but is otherwise structured\n    - Tool scope is somewhat broader than necessary but not egregious\n\n    Score 0.0 if:\n    - It does not produce an agent file (no proper frontmatter/body), OR\n    - The name is invalid, OR\n    - Description lacks examples, OR\n    - It’s mostly generic advice rather than a concrete agent configuration.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of agent development concepts and creates a realistic scenario. It tests transferable knowledge (frontmatter structure, triggering patterns, system prompt design, least privilege) rather than trivia. The grader is comprehensive and accepts valid variations while checking for genuine skill application. However, the grader is somewhat brittle in requiring exact formatting details (e.g., 'Context:' line, specific commentary tags) that could reject valid solutions using slightly different but equivalent approaches. The task is highly realistic - creating a test agent for a Vitest monorepo is a practical use case. Minor deduction for grader brittleness around formatting specifics rather than focusing purely on conceptual correctness."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (why this skill matters)\n# - Domain knowledge: how to design an autonomous \"agent\" for Claude Code plugins: correct file structure,\n#   required frontmatter, strong triggering descriptions with examples, and a well-scoped system prompt.\n# - Workflow decisions: picking agent vs command behavior, writing good triggering conditions (incl. when NOT to use),\n#   choosing least-privilege tool access, and selecting model/color appropriately.\n# - What practitioners struggle with: agents that don't trigger, have invalid identifiers/frontmatter, vague system prompts,\n#   overbroad tool permissions, and unclear output formats—making them unreliable in real plugin use.\n\nname: postgres-migration-agent-frontmatter-triggering\nskills:\n  - Agent Development\nprompt: |\n  You are building a Claude Code plugin and need to add ONE autonomous agent.\n\n  Task: Write the complete contents of an agent markdown file (as it would live in `agents/`), for an agent whose job is to plan *safe* PostgreSQL schema migrations for an existing codebase.\n\n  Requirements for the agent you create:\n  - It is an autonomous agent (not a user-invoked command): it should activate when a user asks for migration planning, rollout/rollback, zero/low-downtime changes, etc.\n  - The agent must NOT execute migrations. It should analyze repository context and produce a migration plan + suggested SQL + rollout/rollback steps.\n  - Include all REQUIRED frontmatter fields, and keep the agent identifier valid (lowercase letters/numbers/hyphens; 3-50 chars; starts/ends with alphanumeric).\n  - The `description` must clearly state triggering conditions and include 2–3 concrete `<example>` blocks, each with:\n    - Context:\n    - user: \"...\"\n    - assistant: \"...\"\n    - a `<commentary>` explaining why the agent should trigger\n  - Choose an appropriate `model` and `color`.\n  - Apply least-privilege tool access: only include tools that the agent truly needs for planning (it should not need to run shell commands).\n  - In the markdown body (system prompt), write in second person and include:\n    - Core responsibilities\n    - A step-by-step analysis/process\n    - Quality standards (e.g., safety, reversibility)\n    - Output format (a structured template the agent will return)\n    - Edge cases (e.g., large tables, locks, backwards compatibility, partial failures)\n\n  Output ONLY the final agent markdown file content (including the `---` frontmatter). No extra explanation.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant produced a valid, high-quality Claude Code plugin agent file for PostgreSQL migration planning.\n\n    Score 1.0 if all of the following are true:\n    - Outputs a single markdown \"agent file\" with YAML frontmatter delimited by '---' at top (frontmatter present and clearly separated from body).\n    - Frontmatter includes REQUIRED fields: name, description, model, color (tools is optional but if omitted must not conflict with least-privilege requirement; inclusion is preferred).\n    - The agent identifier in `name:` is valid: 3-50 chars, lowercase letters/numbers/hyphens only, starts and ends with alphanumeric.\n    - `description:` begins with clear triggering conditions (e.g., \"Use this agent when...\") AND contains 2–3 <example> blocks.\n      Each example includes Context:, a user quoted request, an assistant response, and a <commentary> explaining why it triggers.\n      Description is specific (mentions migration planning/rollout/rollback/low-downtime) and includes at least one \"when NOT to use\" or boundary condition.\n    - `model:` is a valid option (inherit/sonnet/opus/haiku) with a sensible choice (inherit is acceptable/recommended).\n    - `color:` is one of the allowed set (blue/cyan/green/yellow/magenta/red) and is plausibly chosen (e.g., yellow for caution).\n    - Tooling follows least privilege: either (a) tools are restricted to read/planning needs (e.g., Read/Grep/Glob/Write) and does NOT include Bash, \"*\" or broad access without justification, OR (b) if tools omitted, the body explicitly limits behavior to non-execution and planning-only and avoids implying shell execution.\n    - The markdown body (system prompt) is written in second person (\"You are...\", \"You will...\") and includes:\n      * Core responsibilities (explicit list)\n      * A step-by-step process\n      * Quality/safety standards emphasizing reversibility/rollback, minimizing locks/downtime, validation\n      * A clear output format template (sections like assumptions, plan, SQL, rollout, rollback, risks, verification)\n      * Edge cases handling (at least 2 relevant ones: large tables/locking, backwards compatibility, partial failures, online index creation, multi-service deploy coordination, etc.)\n    - The agent is clearly autonomous and scoped to planning/documentation rather than executing migrations.\n\n    Score 0.5 if:\n    - It mostly resembles an agent file but misses one major requirement (e.g., only 1 example, missing edge cases, vague description, invalid/unclear tool scoping), OR\n    - Frontmatter is present but one required field is missing, OR\n    - The system prompt lacks either a step-by-step process OR an explicit output format, but the rest is solid.\n\n    Score 0.0 if:\n    - Not an agent markdown file with frontmatter, OR\n    - Name/identifier is invalid and uncorrected, OR\n    - Description lacks examples/triggering conditions, OR\n    - It proposes executing migrations or relies on shell execution as a core action, OR\n    - Response is mostly generic advice rather than a concrete agent configuration.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case demonstrates strong understanding of agent development concepts. It tests transferable knowledge about autonomous agent design (triggering conditions, system prompts, tool restrictions, frontmatter structure) rather than memorizing specific identifiers. The grader is comprehensive and accepts multiple valid approaches (e.g., different colors, model choices, tool combinations). The task is realistic—creating a migration planning agent is a genuine use case practitioners would encounter. Minor deductions: the grader is somewhat verbose and could be slightly more concise, and the test scenario is moderately complex which might make it harder to distinguish skill knowledge from general engineering ability. Overall, this is a high-quality test that effectively measures whether the skill document's knowledge helps."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (per skill doc)\n# 1) Domain knowledge taught:\n#    - How to design a Claude Code “agent” (autonomous subprocess) including valid YAML frontmatter,\n#      identifier rules, trigger-oriented description with <example>/<commentary>, and a structured\n#      second-person system prompt.\n#    - Operational choices: model selection (inherit by default), color semantics, and least-privilege tools.\n# 2) Workflow decisions enabled:\n#    - When an agent is appropriate (multi-step autonomous work) vs a command (single user-initiated action).\n#    - How to write descriptions that reliably trigger (specific conditions + multiple concrete examples).\n#    - How to constrain tool access to reduce risk while still accomplishing the task.\n# 3) Practitioner struggle without this skill (test target):\n#    - Producing an agent file that validates: correct required frontmatter fields, valid identifier,\n#      description that includes examples + commentary, and a system prompt with clear responsibilities,\n#      process, output format, and edge-case handling—without granting excessive tools.\nname: dependency-vulnerability-audit-agent-config\nskills:\n  - Agent Development\nprompt: |\n  You are building a Claude Code plugin and need to add a NEW autonomous agent.\n\n  Create the full contents of a single agent markdown file to place in `agents/` for this purpose:\n  - The agent performs a security-focused dependency vulnerability audit for a repo that may contain BOTH Node.js and Python projects (monorepo).\n  - It should autonomously:\n    1) discover manifests/lockfiles,\n    2) run local audit commands when available,\n    3) summarize vulnerabilities by severity and exploitability (as best as possible from local data),\n    4) propose a remediation plan (upgrades, mitigations, and verification steps),\n    5) include safe “suggested diffs” as text, but MUST NOT modify files itself.\n\n  HARD REQUIREMENTS (must all be satisfied):\n  - Output ONLY the agent markdown file content (no extra explanation).\n  - Include YAML frontmatter delimited by `---` at top.\n  - Frontmatter MUST include: name, description, model, color, tools.\n  - `name` must be a valid identifier: 3–50 chars, lowercase letters/numbers/hyphens only, starts & ends with alphanumeric.\n  - `model` must be `inherit`.\n  - `color` must be one of: blue, cyan, green, yellow, magenta, red (pick an appropriate one for security).\n  - `tools` MUST be least-privilege for the task and MUST NOT include Write (the agent must not edit files).\n  - `description` MUST:\n    - start with clear triggering conditions (“Use this agent when…”),\n    - include 2–4 `<example>` blocks,\n    - each example includes Context, user: \"...\", assistant: \"...\", and a `<commentary>...</commentary>` explaining why the agent should trigger,\n    - include at least one example that clarifies when NOT to use this agent (or when to prefer a simpler response).\n  - The markdown body (system prompt) MUST be written in second person and include at minimum:\n    - **Your Core Responsibilities**\n    - **Analysis Process** (step-by-step)\n    - **Quality Standards**\n    - **Output Format**\n    - **Edge Cases** (e.g., no lockfile, missing audit tools, huge monorepo, false positives)\n\n  Make the agent realistic and directly usable by a plugin author.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response is a high-quality Claude Code agent file that would validate and work well.\n\n    Score 1.0 if ALL are true:\n    - Response is a single markdown agent file with YAML frontmatter enclosed by `---` at the top.\n    - Frontmatter contains EXACTLY the required keys at minimum (may include others, but must include):\n      `name`, `description`, `model`, `color`, `tools`.\n    - `name` is valid: 3–50 chars; lowercase letters, digits, hyphens only; starts/ends alphanumeric; no underscores/spaces.\n    - `model` is `inherit`.\n    - `color` is one of: blue/cyan/green/yellow/magenta/red, and is plausibly chosen for a security audit agent.\n    - `tools` is an array and reflects least privilege for read/audit work (e.g., Read/Grep/Glob/Bash are fine),\n      and it does NOT include `Write`, `\"*\"`, or an obviously over-broad set inconsistent with “must not modify files”.\n    - `description` clearly states triggering conditions (explicit “Use this agent when…”), and includes 2–4 `<example>` blocks.\n      Each `<example>` contains:\n        * a Context line,\n        * a `user: \"...\"` line,\n        * an `assistant: \"...\"` line,\n        * and a `<commentary> ... </commentary>` explaining why the agent triggers.\n      At least one example (or commentary) clarifies when NOT to use the agent / when a simpler approach is preferred.\n    - The system prompt body is in second person (“You are…”, “You will…”), is structured, and includes sections covering:\n      Core Responsibilities, a multi-step Analysis Process, Quality Standards, Output Format, and Edge Cases.\n      It addresses both Node and Python dependency auditing, and states it should not make file edits (only suggest diffs).\n\n    Score 0.5 if:\n    - It is mostly an agent file with frontmatter and a reasonable system prompt, but is missing ONE important requirement\n      (e.g., only 1 example; missing commentary tags; tools listed but slightly too broad; missing one required section like Quality Standards),\n      while still clearly aiming to satisfy the task.\n\n    Score 0.0 if:\n    - Not an agent markdown file, OR missing frontmatter, OR missing required frontmatter fields,\n      OR `name` is invalid, OR `model` not inherit, OR includes Write/* tools despite explicit prohibition,\n      OR description lacks the required examples/commentary structure.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong understanding of agent development concepts and creates a realistic scenario. However, it has clarity issues: the grader is overly prescriptive (requiring exact section names like 'Your Core Responsibilities' when variations should be acceptable), uses brittle string matching for YAML structure validation that could reject valid alternatives, and the complex multi-condition scoring may miss edge cases. The non-obviousness is good (tests transferable knowledge about autonomous agent design, triggering patterns, and least-privilege principles rather than memorizing specific identifiers), and realism is excellent (security auditing is a genuine use case). The main weakness is grader brittleness that could reject valid solutions using slightly different formatting or section naming conventions."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "The Agent Development skill teaches how to design Claude Code plugin agents: choosing valid identifiers, crafting trigger-ready descriptions with `<example>` blocks, selecting models/colors/tools deliberately, and writing thorough second-person system prompts with responsibilities, processes, outputs, and edge cases. It also covers testing/validation workflows to ensure agents fire correctly and adhere to best practices.\n\nThis enables decisions like: when to create an agent vs a simple command, how detailed triggering examples must be, how to limit tools for least privilege, picking appropriate colors/models, and structuring prompts so agents behave autonomously and consistently. Without this skill, practitioners struggle to write valid descriptions (missing examples/commentary), pick compliant names/colors, define clear responsibilities or workflows, and test agent triggering, leading to agents that don’t load or behave unpredictably.\n\n```yaml\nname: comprehensive-agent-spec-review\nskills:\n  - Agent Development\nprompt: |\n  You receive the following draft agent file for a Claude Code plugin:\n\n  ---\n  name: agent_helper\n  description: Handles onboarding questions.\n  model: opus\n  color: blue\n  tools: [\"Read\", \"Write\", \"Shell\"]\n  ---\n\n  I am an onboarding assistant that helps with new dev setup. I answer questions.\n\n  Evaluate this draft against best practices for agent development. Identify at least three concrete issues spanning different aspects (e.g., naming, triggering description, system prompt structure, tooling/model choices). For each issue, explain why it violates the guidelines and propose a specific, improved alternative that aligns with the standards.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Grade the response on whether it demonstrates strong knowledge of the Agent Development skill.\n\n    Score 1.0 if:\n    - Identifies at least three distinct issues covering multiple categories (e.g., invalid name, missing description examples, vague system prompt, unjustified model/tool choices, missing structure).\n    - Explanations clearly cite relevant best-practice reasoning from the skill (e.g., identifier rules, description requirements, least-privilege tooling).\n    - Provides concrete, skill-aligned fixes for each issue (e.g., a valid name example, description snippet including examples/commentary, structured system prompt outline, adjusted tools/model justification).\n\n    Score 0.5 if:\n    - Identifies one or two relevant issues OR gives three issues but with shallow reasoning.\n    - Explanations or fixes are partially correct but miss key requirements or lack specificity.\n\n    Score 0.0 if:\n    - Fails to identify meaningful issues, or suggestions contradict the skill guidance, or response is off-topic.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of Agent Development by requiring multi-faceted evaluation (naming, description, system prompt, tools/model). It tests transferable knowledge about agent design principles rather than memorization. However, it has moderate clarity issues: the grader accepts vague 'three issues' without specifying which categories must be covered, potentially allowing responses that miss critical aspects like the invalid identifier format (underscores) or missing <example> blocks. The realism is strong—reviewing draft agent specs is a genuine practitioner task. The test would benefit from more specific grading criteria (e.g., 'must identify the invalid name format' as a required element) to ensure robust evaluation."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\nThe “Agent Development” skill teaches how to design autonomous agent markdown files for Claude Code plugins, focusing on choosing and validating key metadata (identifier, triggering description, model, color, optional tools) and crafting a structured, second-person system prompt that outlines responsibilities, analysis steps, output format, quality standards, and edge cases. Understanding when an agent should be used (versus a command) is central—agents run autonomously based on rich triggering conditions expressed in the description with multiple annotated examples. Without this skill, a practitioner would struggle to specify precise trigger conditions, compose complete example blocks, and structure a system prompt that leads the agent through multi-step reasoning with clear output expectations and edge-case handling.\n\nA strong test targets whether the model can operationalize these guidelines: selecting an appropriate identifier, articulating specific “Use this agent when...” triggers plus 2-3 detailed <example> blocks with context/user/assistant/commentary, choosing suitable model/color, optionally limiting tools, and writing the body system prompt that enumerates responsibilities, an analysis process, quality standards, output format, and edge cases in second person.\n\nThe test should ask for a realistic scenario—for example, creating an agent to autonomously review and summarize PR health metrics for a DevOps plugin. The grader should check for all required parts, mention best practices, and accept valid variations (as long as the structure and intent are clear). Partial credit can be given to responses that include most elements but miss one (e.g., lacking edge cases or enough examples). Now craft the test.yaml accordingly.\n\n```yaml\nname: devops-pr-review-agent\nskills:\n  - Agent Development\nprompt: |\n  You are designing a Claude Code plugin agent that autonomously evaluates the health of an incoming pull request for a DevOps team. The agent should trigger when the user asks for PR readiness insights, testing gaps, or release blockers. Provide the complete agent markdown file: include proper frontmatter (name, description with triggering guidance and at least two <example> blocks covering different phrasings, model, color, optional tools), and a system prompt body describing the agent’s role. The system prompt must be written in second-person, list core responsibilities, an analysis process, output format expectations, quality standards, and edge cases. Follow the best practices for agent triggering descriptions and system prompt structure outlined in the skill document.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response includes:\n    1. A valid frontmatter with an identifier (lowercase letters/numbers/hyphens, 3-50 chars), description that begins with “Use this agent when...”, includes at least two <example> blocks each containing Context, user, assistant, and commentary explaining why the agent triggers, and specifies model and color (tools optional).\n    2. A system prompt body written in second-person that defines the agent’s role, lists multiple core responsibilities, describes an analysis or workflow process, defines the expected output format, states quality standards, and addresses edge cases.\n    3. Triggers/examples focus on PR readiness, testing gaps, or release blockers to match the DevOps scenario.\n\n    Score 1.0 if: All required sections are present, formatted correctly, and the description/examples clearly capture triggering conditions plus commentary. System prompt follows best practices with responsibilities, process, output format, quality standards, and edge cases all described in second person.\n\n    Score 0.5 if: Most elements are present but one major requirement is incomplete or missing (e.g., fewer than two complete <example> blocks, missing edge cases, or description lacks commentary), yet intent and structure remain recognizable.\n\n    Score 0.0 if: The response omits key frontmatter fields, lacks the required description/examples, or fails to provide a structured system prompt as specified.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of agent development concepts with a realistic DevOps PR review scenario. Non-obviousness (7/10): Tests genuine knowledge of agent structure, triggering patterns, and system prompt design rather than trivia, though the specific PR review context is somewhat narrow. Clarity (8/10): The grader is mostly robust, checking for all key components (frontmatter, examples with commentary, second-person system prompt, responsibilities, process, output format, quality standards, edge cases) and accepts valid variations while providing clear partial credit criteria. Realism (8/10): Creating an autonomous PR review agent is a plausible real-world task for DevOps teams using Claude Code plugins. Minor weaknesses: The grader could be slightly more explicit about accepting different valid identifier formats and tool choices, and the prompt could emphasize testing transferable triggering/system-prompt patterns rather than the specific PR domain."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: create-security-agent-definition\nskills:\n  - Agent Development\nprompt: |\n  I am developing a Claude Code plugin and need to create an autonomous agent file. \n  \n  Please write the complete markdown file content for a \"Security Reviewer\" agent. \n  \n  Requirements:\n  1. The agent should trigger when a user asks to \"audit code\", \"check for vulnerabilities\", or \"scan for security issues\".\n  2. It needs to read files and search for patterns, so give it appropriate tool permissions (specifically Read and Grep).\n  3. It should use the same model as the parent process.\n  4. Use a color appropriate for security/critical tasks.\n  5. The system prompt should structure the analysis to look for hardcoded secrets and injection vulnerabilities.\n\n  Ensure the file format is correct so Claude Code can load and trigger it properly.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the generated agent file for correctness based on the Agent Development standards.\n\n    Score 1.0 if the response includes a valid Markdown file with:\n    1. Correct YAML frontmatter (between `---` lines).\n    2. Valid `name` (lowercase, hyphens only, e.g., `security-reviewer`).\n    3. `description` field that includes at least one `<example>` block showing Context, User, and Assistant interaction (this is critical for triggering).\n    4. `model` set to `inherit`.\n    5. `color` set to `red` (or arguably `yellow` for caution/validation).\n    6. `tools` array containing \"Read\" and \"Grep\".\n    7. A system prompt in the body (after the second `---`) starting with something like \"You are...\".\n\n    Score 0.5 if the response looks like a markdown file with frontmatter, but:\n    - Misses the `<example>` blocks in the description (critical failure for triggering).\n    - Uses invalid name format (e.g., `SecurityReviewer`).\n    - Omits required fields like `color` or `model`.\n\n    Score 0.0 if:\n    - Not formatted as a markdown file with YAML frontmatter.\n    - Completely misses the concept of an agent file structure.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test measures genuine understanding of agent file structure (frontmatter, examples, system prompts) rather than trivia, earning strong non-obviousness (8/10). The grader is mostly robust, checking for key structural elements and the critical `<example>` blocks, though it could be slightly more flexible about color choices (7/10 clarity). The task is realistic - creating a security reviewer agent is a practical use case - though the specific requirements are somewhat prescriptive (7/10 realism). Overall, this is a solid test that validates transferable knowledge about agent development patterns."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "## Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill teaches the architecture and configuration of autonomous agents within the Claude Code plugin ecosystem. It focuses on the distinction between proactive agents and reactive commands, the specific YAML-in-Markdown file structure, and the mechanics of agent orchestration—specifically how Claude decides when to delegate tasks to an agent based on its description and examples.\n\n2.  **What workflow decisions does it enable?**\n    It enables practitioners to decide when a task is complex enough to warrant an autonomous agent rather than a simple command. It guides decisions on model selection (cost/speed vs. capability), visual organization (color-coding by agent type), and security/efficiency (restricting tool access via the principle of least privilege). Crucially, it informs the \"trigger engineering\" required to make an agent discoverable by the parent LLM.\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    Without this skill, a developer would likely create agents that fail to trigger because they lack the specific `<example>` and `<commentary>` format required in the description field. They might use invalid identifiers (e.g., camelCase or underscores) that cause plugin loading errors. Furthermore, they might produce \"weak\" agents by writing system prompts that aren't structured for autonomous workflows (lacking specific processes or output formats), leading to agents that simply ask the user for more information instead of performing work.\n\n---\n\n```yaml\nname: agent-creation-validation\nskills:\n  - Agent Development\nprompt: |\n  I need you to create a new agent file for a Claude Code plugin. \n  \n  The agent's purpose is \"API Consistency Checker\". It should automatically trigger when a user is modifying public-facing API endpoints or headers to ensure they follow the team's style guide (e.g., kebab-case for headers, versioned paths).\n\n  Requirements:\n  1. The agent should be named appropriately according to naming rules.\n  2. Use a \"Warning\" or \"Validation\" oriented color.\n  3. It should use the parent model.\n  4. Limit its tools to only what is necessary for reading and searching code (no writing or execution).\n  5. The description must be formatted correctly to ensure it triggers, including at least two examples with the required internal tags (Context, user, assistant, commentary).\n  6. The system prompt should define a clear role, a step-by-step analysis process, and a specific output format.\n\n  Provide the complete contents of the `.md` file.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the agent file based on the following criteria:\n\n    1. Identifier Rules: Is the name lowercase-hyphenated, 3-50 chars, and alphanumeric at start/end? (e.g., api-consistency-checker)\n    2. Frontmatter Fields: Does it include name, description, model (set to inherit), and color (yellow or red is preferred for validation/warning)?\n    3. Triggering Description: Does the description include triggering conditions AND at least two <example> blocks? Each <example> must contain <commentary> and the specific Context/user/assistant fields.\n    4. Tool Restriction: Is the 'tools' field present and restricted to read-only tools like \"Read\", \"Grep\", or \"Glob\"? (Should NOT include \"Write\" or \"Bash\").\n    5. System Prompt: Is the body of the markdown written in the second person (\"You are...\") and does it include a specific \"Analysis Process\" and \"Output Format\"?\n\n    Score 1.0 if: All requirements are met, especially the complex <example> structure in the description and valid identifier naming.\n    Score 0.5 if: The agent is mostly correct but misses one specific formatting requirement (e.g., forgot the <commentary> tag inside examples) or uses incorrect tool permissions (e.g., allows \"Write\").\n    Score 0.0 if: The frontmatter is missing, the name is invalid (e.g., api_consistency), or there are no examples in the description.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of agent development concepts and tests transferable knowledge (non-obviousness: 8/10). It requires understanding frontmatter structure, triggering mechanisms with <example> blocks, tool restriction principles, and system prompt design—all genuine domain knowledge rather than trivia. The grader is mostly robust (clarity: 7/10), accepting valid solutions while checking for key structural elements, though it could be slightly more flexible on color choices and tool combinations. The task is realistic (realism: 8/10) as creating validation agents for API consistency is a practical use case. However, the grader has minor brittleness in expecting specific tool names and could better accommodate valid alternatives (e.g., different read-only tool combinations). Overall, this is a solid test that measures whether the skill knowledge genuinely helps, with room for improvement in grader flexibility."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "## Analysis\n\n**Domain Knowledge:**\n- Agent vs command distinction (autonomous processes vs user-initiated actions)\n- YAML frontmatter structure with specific required fields (name, description, model, color)\n- Description field as triggering mechanism with example blocks\n- System prompt design patterns and best practices\n- Tool restriction principles (least privilege)\n- Validation rules for identifiers and structure\n\n**Workflow Decisions:**\n- When to create an agent vs a command (multi-step autonomous tasks)\n- How to write effective triggering conditions with examples\n- Which model to choose for different capabilities\n- How to structure system prompts for autonomous behavior\n- What tools to grant based on security/access principles\n\n**Practitioner Struggles:**\nWithout this skill, a practitioner would struggle with:\n- Making agents actually trigger (poor description field design)\n- Writing system prompts that produce reliable autonomous behavior\n- Following the specific YAML frontmatter format requirements\n- Understanding when to use agents vs commands\n- Implementing proper security through tool restrictions\n- Creating examples that show proactive and reactive triggering\n\n## Test Case\n\n```yaml\nname: agent-design-scenario-implementation\nskills:\n- Agent Development\nprompt: |\n  You need to create an agent for a Claude Code plugin that automatically analyzes code changes for potential security vulnerabilities when pull requests are created. The agent should be triggered by code review requests, security scan requests, or when someone mentions \"security\" in relation to code changes.\n\nThe agent needs to:\n1. Read and analyze modified files\n2. Check for common security patterns (SQL injection, XSS, hardcoded secrets, etc.)\n3. Generate a security report with severity levels\n4. Suggest remediation steps\n\nCreate a complete agent configuration file including:\n- Appropriate agent identifier\n- Triggering description with 2-3 concrete examples\n- Proper frontmatter fields\n- Comprehensive system prompt with responsibilities and process steps\n\nMake sure the agent follows all best practices for autonomous operation and proper triggering.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the agent configuration for completeness and correctness:\n\n    Score 1.0 if ALL of these requirements are met:\n    - Valid agent identifier (3-50 chars, lowercase-hyphens, starts/ends alphanumeric)\n    - Proper YAML frontmatter with all required fields (name, description, model, color)\n    - Description includes triggering conditions AND 2-3 concrete <example> blocks\n    - Each example has Context, user request, assistant response, and <commentary>\n    - System prompt written in second person with clear responsibilities\n    - Includes step-by-step analysis process\n    - Defines output format and quality standards\n    - Appropriate tool selection (includes Read, Grep, Write for reports)\n    - Color choice aligns with security domain (red/yellow recommended)\n    - Follows all formatting and structural requirements\n\n    Score 0.5 if MOST requirements are met but has:\n    - 1-2 missing frontmatter fields or invalid identifier\n    - Poor description (missing examples or triggering conditions)\n    - Incomplete system prompt (missing process steps or output format)\n    - Some formatting issues but core structure is correct\n\n    Score 0.0 if:\n    - Missing critical components like description field or system prompt\n    - Invalid YAML structure\n    - Uses first-person instead of second-person in system prompt\n    - No examples in description\n    - Completely wrong approach (e.g., creating a command instead of agent)\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of agent development concepts and tests genuine domain knowledge (frontmatter structure, triggering examples, system prompt design). However, it has clarity issues: the grader is overly prescriptive about specific implementation details (e.g., requiring exactly 'Read, Grep, Write' tools, mandating red/yellow colors) that could reject valid alternatives. A security agent might reasonably use different tool combinations or colors while still being correct. The test is realistic and non-trivial, but the brittle grading criteria reduce its robustness."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this skill document, I need to create a test case that measures whether having the Agent Development skill helps an LLM successfully create autonomous agents for Claude Code plugins.\n\nLet me analyze the key concepts:\n- **Domain knowledge**: Agent structure, triggering conditions, system prompt design, frontmatter fields\n- **Workflow decisions**: When to use agents vs commands, how to write effective descriptions with examples, how to structure system prompts\n- **Common struggles**: Writing triggering conditions, designing system prompts for autonomous behavior, understanding frontmatter requirements\n\nA good test case should present a realistic scenario where someone needs to create an agent and evaluate whether the LLM demonstrates understanding of the core concepts from the skill document.\n\n```yaml\nname: security-code-reviewer-agent\nskills:\n  - Agent Development\nprompt: |\n  You need to create an autonomous agent for a Claude Code plugin that reviews code for security vulnerabilities.\n\n  The agent should:\n  - Trigger when users ask about security issues, vulnerabilities, or code audits\n  - Be autonomous (handle the entire security review process independently)\n  - Have proper frontmatter with all required fields\n  - Include triggering examples in the description\n  - Have a comprehensive system prompt that defines its behavior\n\n  Create the complete agent file with:\n  1. Proper frontmatter (name, description, model, color, tools)\n  2. A description with triggering conditions and examples\n  3. A system prompt that enables autonomous security review behavior\n  4. Follow all the best practices from the skill document\n\n  Save this as 'agents/security-code-reviewer.md' and show me the complete file content.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of agent development concepts.\n\n    Score 1.0 if:\n    - Creates proper YAML frontmatter with ALL required fields (name, description, model, color, tools if applicable)\n    - Name follows naming conventions (lowercase, hyphens, 3-50 chars, alphanumeric start/end)\n    - Description includes clear triggering conditions with 2-4 concrete <example> blocks\n    - Examples show realistic scenarios with context, user requests, and assistant responses\n    - System prompt is written in second person, includes clear responsibilities, process steps, and output format\n    - Demonstrates understanding of when to use agents vs commands\n    - Shows autonomous behavior design (not just command execution)\n    - Includes quality standards and edge case handling\n    - File structure follows the markdown + frontmatter format\n    - Model choice is appropriate (usually 'inherit' unless specific needs)\n    - Color choice is appropriate for the agent type\n\n    Score 0.5 if:\n    - Creates basic agent structure but missing some required fields\n    - Has frontmatter but with formatting issues or invalid values\n    - Description lacks triggering examples or examples are incomplete\n    - System prompt exists but lacks key structural elements (responsibilities, process, output format)\n    - Shows partial understanding of agent concepts\n\n    Score 0.0 if:\n    - Does not create an agent file structure\n    - Frontmatter is missing or completely malformed\n    - No understanding of autonomous agent concepts (just creates a command/script)\n    - System prompt is missing or completely generic\n    - Does not demonstrate comprehension of the skill concepts\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of agent development concepts and tests transferable knowledge (non-obviousness: 7/10 - tests genuine patterns like frontmatter structure, triggering conditions, and system prompt design). The grader is mostly robust but could be more specific about accepting valid alternatives (clarity: 7/10 - accepts multiple approaches but some criteria like 'appropriate color choice' are subjective). The task is realistic (realism: 8/10 - creating a security review agent is a genuine use case). However, the test could be improved by being less prescriptive about exact field names and more focused on conceptual understanding. The grader appropriately focuses on structural understanding rather than memorization of specific strings."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "## Analysis\n\n### Domain Knowledge\n- **Agents vs. Commands**: Agents are for autonomous work, while commands are for user-initiated actions.\n- **Agent Structure**: Agents are defined using a markdown file with YAML frontmatter.\n- **Triggering Conditions**: The `description` field is crucial for defining when an agent should be triggered, including concrete examples.\n- **System Prompt Design**: The system prompt defines the agent's behavior, responsibilities, and output format.\n\n### Workflow Decisions\n- **When to Use Agents**: Use agents for tasks that require autonomous, multi-step processes.\n- **Agent Creation**: Choose between AI-assisted and manual creation methods.\n- **Validation**: Ensure agents meet naming, description, and system prompt validation rules.\n- **Testing**: Test agents to ensure they trigger correctly and follow the defined process.\n\n### Practical Struggles\n- **Triggering Conditions**: Practitioners often struggle with defining clear and specific triggering conditions, including multiple examples.\n- **System Prompt Design**: Crafting a comprehensive and structured system prompt that covers all necessary aspects (responsibilities, process, output format, edge cases) can be challenging.\n\n## Test Case\n\n```yaml\nname: create-agent-with-clear-triggering-conditions\nskills:\n  - Agent Development\nprompt: |\n  You are tasked with creating an agent for a code review process. The agent should be triggered when a user submits a code change for review. \n\n  Create an agent configuration with the following details:\n  - The agent should be named `code-reviewer`.\n  - The agent should be triggered when a user submits a code change for review. Provide 3 concrete examples of triggering conditions.\n  - The agent should use the `inherit` model.\n  - The agent should have a blue color.\n  - The agent should have access to the `Read`, `Write`, and `Grep` tools.\n  - The system prompt should define the agent's role as a code reviewer, list core responsibilities, and specify the output format.\n\n  Return the complete agent configuration in the markdown file format.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the agent configuration based on the following criteria:\n    Score 1.0 if:\n    - The agent name is `code-reviewer`.\n    - The `description` field includes clear and specific triggering conditions with 3 concrete examples.\n    - The `model` is set to `inherit`.\n    - The `color` is set to `blue`.\n    - The `tools` field includes `Read`, `Write`, and `Grep`.\n    - The system prompt is comprehensive, including role, core responsibilities, analysis process, output format, and quality standards.\n\n    Score 0.5 if:\n    - The agent name is correct.\n    - The `description` field includes triggering conditions with 1-2 examples.\n    - The `model` is set to `inherit`.\n    - The `color` is set to `blue`.\n    - The `tools` field includes `Read`, `Write`, and `Grep`.\n    - The system prompt is partially complete, missing some sections but includes the role and core responsibilities.\n\n    Score 0.0 if:\n    - The agent name is incorrect.\n    - The `description` field is missing or vague.\n    - The `model` is not set to `inherit`.\n    - The `color` is not set to `blue`.\n    - The `tools` field is missing or incorrect.\n    - The system prompt is incomplete or missing.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates moderate quality across dimensions. **Non-obviousness (6/10)**: While it tests transferable knowledge about agent structure and triggering conditions, it's overly prescriptive by requiring exact values (name='code-reviewer', color='blue', tools=['Read','Write','Grep']) rather than testing whether the practitioner understands *why* these choices matter. **Clarity (7/10)**: The grader is mostly robust but has brittleness - it requires exact tool names and exact color, when alternatives could be equally valid (e.g., cyan instead of blue, different tool combinations). The 0.5 partial credit path helps but still penalizes valid design choices. **Realism (7/10)**: Creating a code review agent is realistic, but the task is somewhat contrived by specifying all implementation details upfront rather than letting the practitioner make informed choices based on requirements. A more realistic task would describe the *need* and let the practitioner design the solution."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: create-agent-for-planning\nskills:\n- Agent Development\nprompt: |\n  Create an agent configuration for a task planner that helps users organize their day. The planner should trigger when a user asks for help with planning their tasks or scheduling their day. Include examples of how the planner can assist with prioritizing tasks, setting reminders, and suggesting time-blocking strategies.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes:\n    - A clear and descriptive agent identifier (e.g. \"task-planner\")\n    - A well-structured description with triggering conditions and 2-3 examples of usage\n    - A system prompt that outlines the agent's responsibilities, process, and output format\n    - Appropriate tool restrictions (if any)\n    Score 0.5 if the response includes most of the required elements but lacks clarity or detail in one or two areas.\n    Score 0.0 if the response fails to include multiple required elements or demonstrates a lack of understanding of agent development principles.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues across all dimensions. For non-obviousness (3/10): it tests basic agent structure knowledge that's too close to memorizing the skill document format rather than transferable domain knowledge about when/why to create agents. For clarity (5/10): the grader is somewhat vague ('well-structured', 'appropriate') and doesn't specify what constitutes valid alternatives or edge cases, though it does accept multiple approaches. For realism (4/10): creating a task planner agent is contrived - the skill is about Claude Code plugin development, but this asks for a generic day-planning agent that doesn't fit that ecosystem. A realistic task would involve creating an agent for actual code-related work (e.g., 'create an agent that reviews PRs for security issues')."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-AWS IAM Policy Reasoning",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 2,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** AWS IAM Policy Reasoning\n\n**SKILL.md Content:**\n```markdown\n---\nname: AWS IAM Policy Reasoning\ndescription: Debug AccessDenied errors and design least-privilege access by reasoning about IAM policy evaluation (explicit deny, allow, conditions, resource policies, and role assumption).\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS IAM Policy Reasoning\n\n## Overview\n\nIAM authorization is not “does this policy mention the action?” It’s a multi-source evaluation of identity policies, resource policies, permission boundaries, session policies, and organization controls. The core rule is: **any explicit Deny wins**, and an action must be explicitly Allowed somewhere to succeed.\n\nUse this skill to:\n- Debug `AccessDenied` errors quickly and safely\n- Explain *why* a request is denied (which statement/condition)\n- Build least-privilege roles without breaking workflows\n\n## Quick Mental Model\n\n1. **Identify the principal** (user/role + assumed-role session)\n2. **Identify the action** (e.g., `s3:GetObject`)\n3. **Identify the resource** (ARNs, including region/account and wildcards)\n4. **Collect policies that apply**:\n   - Identity-based policies (attached to user/role)\n   - Resource-based policies (S3 bucket policy, KMS key policy, SNS topic policy, etc.)\n   - Permission boundary (if set)\n   - Session policy (if using `AssumeRole` with a policy)\n   - SCPs (Org policies)\n5. Evaluate:\n   - If **any Deny matches**, result is Deny\n   - Else if **any Allow matches**, result is Allow\n   - Else default Deny\n\n## Workflow: Debugging AccessDenied\n\n1. Capture the **exact API call context**:\n   - Service + action\n   - Resource ARN(s)\n   - Region/account\n   - Caller identity (role session ARN)\n2. Look for **the most common mismatch**:\n   - Wrong resource ARN (bucket vs object, table vs index, key vs alias)\n   - Missing `kms:Decrypt` (KMS is a frequent hidden dependency)\n   - Missing `iam:PassRole` (for ECS/Lambda/Glue/etc.)\n   - Condition mismatch (IP, VPC endpoint, MFA, tags, encryption requirement)\n3. Determine whether the service uses a **resource policy**:\n   - S3, KMS, SNS/SQS, Secrets Manager, ECR, etc.\n4. If the principal is assumed-role, check:\n   - The **trust policy** allowed the assume\n   - The session has the expected **tags** and **duration**\n   - Any session policy/boundary is not restricting it\n5. Use least-privilege iteration:\n   - Start with the minimal action set\n   - Restrict resources\n   - Add conditions last (and test each)\n\n## Common Pitfalls / Gotchas\n\n- **Bucket vs Object ARNs**: `s3:ListBucket` uses bucket ARN; `s3:GetObject` uses object ARN (`arn:aws:s3:::bucket/key`).\n- **KMS requires both sides**: caller needs `kms:Decrypt` and key policy must allow it (or delegate to IAM).\n- **Explicit Deny surprises**: a single deny in an SCP or permission boundary overrides all allows.\n- **Tag conditions**: `aws:RequestTag`, `aws:PrincipalTag`, and `aws:ResourceTag` often fail because tags aren’t present where you think.\n- **Region/account mismatches**: `arn:aws:logs:us-east-1:...` is not `us-west-2`.\n\n## Checklist\n\n- [ ] Confirm caller identity (`sts:GetCallerIdentity`)\n- [ ] Confirm action + resource ARN(s)\n- [ ] Check for explicit Deny sources (SCP/boundary/resource policy)\n- [ ] Check hidden dependencies (KMS, PassRole, VPC endpoint conditions)\n- [ ] Narrow allow statements to required actions/resources\n- [ ] Re-test after each change\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - AWS IAM Policy Reasoning\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** AWS IAM Policy Reasoning\n\n**SKILL.md Content:**\n```markdown\n---\nname: AWS IAM Policy Reasoning\ndescription: Debug AccessDenied errors and design least-privilege access by reasoning about IAM policy evaluation (explicit deny, allow, conditions, resource policies, and role assumption).\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS IAM Policy Reasoning\n\n## Overview\n\nIAM authorization is not “does this policy mention the action?” It’s a multi-source evaluation of identity policies, resource policies, permission boundaries, session policies, and organization controls. The core rule is: **any explicit Deny wins**, and an action must be explicitly Allowed somewhere to succeed.\n\nUse this skill to:\n- Debug `AccessDenied` errors quickly and safely\n- Explain *why* a request is denied (which statement/condition)\n- Build least-privilege roles without breaking workflows\n\n## Quick Mental Model\n\n1. **Identify the principal** (user/role + assumed-role session)\n2. **Identify the action** (e.g., `s3:GetObject`)\n3. **Identify the resource** (ARNs, including region/account and wildcards)\n4. **Collect policies that apply**:\n   - Identity-based policies (attached to user/role)\n   - Resource-based policies (S3 bucket policy, KMS key policy, SNS topic policy, etc.)\n   - Permission boundary (if set)\n   - Session policy (if using `AssumeRole` with a policy)\n   - SCPs (Org policies)\n5. Evaluate:\n   - If **any Deny matches**, result is Deny\n   - Else if **any Allow matches**, result is Allow\n   - Else default Deny\n\n## Workflow: Debugging AccessDenied\n\n1. Capture the **exact API call context**:\n   - Service + action\n   - Resource ARN(s)\n   - Region/account\n   - Caller identity (role session ARN)\n2. Look for **the most common mismatch**:\n   - Wrong resource ARN (bucket vs object, table vs index, key vs alias)\n   - Missing `kms:Decrypt` (KMS is a frequent hidden dependency)\n   - Missing `iam:PassRole` (for ECS/Lambda/Glue/etc.)\n   - Condition mismatch (IP, VPC endpoint, MFA, tags, encryption requirement)\n3. Determine whether the service uses a **resource policy**:\n   - S3, KMS, SNS/SQS, Secrets Manager, ECR, etc.\n4. If the principal is assumed-role, check:\n   - The **trust policy** allowed the assume\n   - The session has the expected **tags** and **duration**\n   - Any session policy/boundary is not restricting it\n5. Use least-privilege iteration:\n   - Start with the minimal action set\n   - Restrict resources\n   - Add conditions last (and test each)\n\n## Common Pitfalls / Gotchas\n\n- **Bucket vs Object ARNs**: `s3:ListBucket` uses bucket ARN; `s3:GetObject` uses object ARN (`arn:aws:s3:::bucket/key`).\n- **KMS requires both sides**: caller needs `kms:Decrypt` and key policy must allow it (or delegate to IAM).\n- **Explicit Deny surprises**: a single deny in an SCP or permission boundary overrides all allows.\n- **Tag conditions**: `aws:RequestTag`, `aws:PrincipalTag`, and `aws:ResourceTag` often fail because tags aren’t present where you think.\n- **Region/account mismatches**: `arn:aws:logs:us-east-1:...` is not `us-west-2`.\n\n## Checklist\n\n- [ ] Confirm caller identity (`sts:GetCallerIdentity`)\n- [ ] Confirm action + resource ARN(s)\n- [ ] Check for explicit Deny sources (SCP/boundary/resource policy)\n- [ ] Check hidden dependencies (KMS, PassRole, VPC endpoint conditions)\n- [ ] Narrow allow statements to required actions/resources\n- [ ] Re-test after each change\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - AWS IAM Policy Reasoning\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "AWS IAM Policy Reasoning",
          "skill_dir": "aws-iam-policy",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\n**1. What domain knowledge does this skill teach?**\n\nCore concepts:\n- Slash commands are reusable prompt templates stored as Markdown files\n- Commands are instructions **for the agent** (Claude), not messages to users - this is the critical conceptual shift\n- YAML frontmatter provides configuration (model selection, tool restrictions, argument hints)\n- Dynamic arguments ($1, $2, $ARGUMENTS) and file references (@filepath)\n- Tool restrictions via allowed-tools to limit command scope\n- Bash execution for gathering dynamic context\n\n**2. What workflow decisions does it enable?**\n\n- **Critical**: Whether to write commands as agent instructions vs user-facing messages\n- When to restrict tool access (allowed-tools patterns)\n- How to structure arguments (positional vs all-arguments)\n- When to validate inputs before proceeding\n- Whether to use flat vs namespaced organization for command sets\n\n**3. What would a practitioner actually struggle with without this skill?**\n\nThe biggest pitfall: Writing commands as if they're documentation for users rather than instructions for Claude. Someone without this knowledge would create commands that *describe* what will happen instead of *directing* Claude what to do.\n\nOther struggles:\n- Overly permissive tool access (not using allowed-tools)\n- Not understanding argument substitution mechanics\n- Missing validation for safety-critical operations\n\n---\n\n```yaml\nname: command-structure-agent-instructions\nskills:\n  - Command Development\nprompt: |\n  You're helping a developer create their first slash command for Claude Code. They want to create a command called \"security-scan\" that reviews code for security vulnerabilities.\n  \n  They've written this draft command and saved it as `.claude/commands/security-scan.md`:\n  \n  ```markdown\n  ---\n  description: Scan code for security issues\n  ---\n  \n  This command will scan your codebase for security vulnerabilities.\n  You'll receive a detailed report covering:\n  - SQL injection risks\n  - XSS vulnerabilities  \n  - Authentication weaknesses\n  \n  The scan will analyze all files in your project and provide recommendations.\n  ```\n  \n  Explain what's wrong with this command and provide a corrected version that will actually work properly.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should identify that the command is written as a message TO THE USER rather than instructions FOR Claude.\n    \n    Score 1.0 if the response:\n    - Clearly identifies that commands should be instructions for Claude, not messages to users\n    - Provides a corrected version that uses imperative directives (e.g., \"Review the code for...\", \"Analyze for...\", \"Report...\")\n    - The corrected version directly instructs Claude what to do\n    \n    Score 0.5 if the response:\n    - Identifies some issue with the command structure\n    - Attempts to improve it, but doesn't fully articulate the \"instructions for agent\" principle\n    - Provides some improvements but the corrected version still has user-facing language mixed in\n    \n    Score 0.0 if the response:\n    - Doesn't identify the fundamental problem with the command structure\n    - Suggests only minor changes (formatting, adding fields) without addressing the core issue\n    - Thinks the command is mostly fine as-is\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test effectively measures understanding of the critical 'commands are instructions FOR Claude' concept, which is genuine domain knowledge that transfers across prompt engineering contexts. The grader is robust and accepts multiple valid explanations. However, the test is slightly contrived (presenting a pre-written broken command for diagnosis) rather than asking the practitioner to create a command from scratch, which would be more realistic. The core knowledge tested is excellent and non-obvious, but the scenario setup reduces realism slightly."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this Command Development skill, let me analyze what it teaches:\n\n## Analysis\n\n**Core Domain Knowledge:**\n1. **Critical Conceptual Difference**: Commands are *instructions FOR Claude*, not messages TO users - this is the foundational principle\n2. **Dynamic Argument Handling**: Understanding when/how to use $ARGUMENTS vs positional arguments ($1, $2)\n3. **File Integration**: Using @file syntax to include file contents in command context\n4. **Input Validation**: Designing commands that handle missing/invalid arguments gracefully\n5. **Tool Constraints**: Using `allowed-tools` to specify what Claude can do\n6. **Command Organization**: Choosing flat vs namespaced structures based on scale\n7. **Frontmatter Configuration**: Knowing which fields accomplish what goal\n\n**Workflow Decisions Practitioners Make:**\n- Should I use $1 (positional) or $ARGUMENTS (all) for this command?\n- When should I reference files with @ vs ask the user to provide them?\n- How should I structure validation logic in the command?\n- What tools should I allow for security/focus?\n- How many commands before I need namespacing?\n\n**What Practitioners Struggle With Without This Skill:**\n- Writing commands that tell users what will happen instead of telling Claude what to do\n- Not validating arguments, leading to confusing error messages\n- Missing opportunities to use file references for better context\n- Not constraining tools, reducing control over command behavior\n- Not organizing commands as complexity grows\n\nI'll write a test that evaluates whether an LLM can create a realistic command that demonstrates understanding of these principles:\n\n```yaml\nname: create-command-with-validation\nskills:\n  - Command Development\nprompt: |\n  Create a slash command called `/analyze-config` that:\n  - Takes a single file path as an argument\n  - Includes the file contents in the command using file reference syntax\n  - Analyzes the configuration file for common issues\n  - Validates that an argument was provided, with helpful guidance if missing\n  \n  The command should be ready to use in a Claude Code project. Include appropriate frontmatter and ensure the command gives Claude clear instructions on what analysis to perform.\n  \n  Show the complete command file contents.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the command demonstrates understanding of Command Development principles.\n    \n    Score 1.0 if the response includes ALL of:\n    - The command is written as instructions FOR Claude (not as a message explaining what the command does to the user)\n    - Uses @ file reference syntax with a positional argument to include file contents (e.g., @$1)\n    - Includes input validation that handles missing arguments with helpful guidance\n    - Has reasonable frontmatter (description at minimum, potentially argument-hint and/or allowed-tools)\n    - The prompt asks Claude to perform analysis/action, not just tells the user what will happen\n    - Valid Markdown file format with proper structure\n    \n    Score 0.5 if the response includes most of the above but has one meaningful gap:\n    - Missing file reference syntax or incorrect @ usage\n    - Missing or weak input validation\n    - Frontmatter missing or incomplete\n    - Instructions somewhat confuse \"telling Claude\" vs \"telling the user\" but overall direction is correct\n    - Minor formatting issues that don't break functionality\n    \n    Score 0.0 if:\n    - Written as a message to the user rather than instructions to Claude\n    - No file reference attempt or incorrect syntax throughout\n    - No validation logic for arguments\n    - Not in valid Markdown format or completely missing frontmatter\n    - Fundamentally misunderstands what a slash command is\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of transferable domain knowledge by testing the critical conceptual distinction between 'instructions FOR Claude' vs 'messages TO users,' along with file references, argument validation, and frontmatter usage. The grader is robust and accepts multiple valid approaches while focusing on core principles rather than exact syntax. However, the test has minor clarity issues: it could be more explicit about accepting alternative validation approaches (e.g., bash commands vs conditional logic), and the 0.5 scoring criteria are somewhat vague about which 'one meaningful gap' would qualify. The realism is excellent—creating commands with validation is a genuine practitioner task. Overall, this is a high-quality test with room for minor grader refinement."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Domain/skill analysis (as YAML comments):\n# 1) Domain knowledge: How Claude Code slash commands are authored as Markdown prompts with optional YAML frontmatter;\n#    how dynamic arguments ($1, $ARGUMENTS), file references (@path), and inline bash context (!`...`) combine to create\n#    repeatable, context-aware workflows.\n# 2) Workflow decisions enabled: choosing restrictive allowed-tools (e.g., Bash(gh:*) vs Bash(*)); when to use positional\n#    args vs $ARGUMENTS; when to include repo policy files via @ references; and how to design commands as instructions\n#    *to Claude* (agent-consumable) rather than explanatory text to the user.\n# 3) Practitioner struggle without skill: producing commands that actually execute reliably in Claude Code—proper frontmatter,\n#    safe tool scoping, argument handling/validation, and embedding dynamic repo context—rather than generic “here’s what\n#    the command will do” descriptions.\n\nname: pr-ci-audit-command-with-validation\nskills:\n- Command Development\nprompt: |\n  You are working in a repository that uses Claude Code. Create a *project* slash command file (i.e., intended to live under\n  `.claude/commands/`) that a team can use to audit a GitHub Pull Request for CI / quality risks.\n\n  Task:\n  - Output the complete contents of ONE Markdown command file.\n  - Assume the file will be saved at: `.claude/commands/ci/pr-ci-audit.md` (namespaced under `ci/`).\n\n  Requirements for the command:\n  1) Include YAML frontmatter with:\n     - a short `description` suitable for `/help`\n     - `argument-hint` documenting expected args\n     - `allowed-tools` that are *restricted* (do NOT use `Bash(*)` unless you strongly justify it in-command)\n     - optionally set an appropriate `model`\n  2) Use dynamic arguments:\n     - Use `$1` as the PR number.\n     - Allow optional extra flags/options after the PR number using either `$2...` or `$ARGUMENTS` (your choice).\n  3) Use inline bash execution to gather context before analysis:\n     - Run at least one `gh` command to fetch PR metadata (title/author/base/head/etc.).\n     - Run at least one command to list changed files (via `gh` or `git`).\n  4) Use at least one file reference using `@...` to pull in a repository policy/checklist file if present\n     (pick a plausible path like `@docs/ci-guidelines.md` or `@docs/pr-review-checklist.md`).\n     - If the file might not exist, the command should handle that gracefully (e.g., check existence via bash and proceed).\n  5) Argument validation / UX:\n     - If `$1` is missing or not a PR number, the command should clearly instruct Claude to ask the user for the missing info\n       and show correct usage.\n  6) The command content must be written as instructions FOR CLAUDE (agent-consumable), not as an explanation to the user.\n     (You may include usage examples in an HTML comment.)\n\n  Deliverable:\n  - Only the Markdown content of the command file, including frontmatter.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant produced a high-quality Claude Code slash command markdown file that meets the requirements.\n\n    Score 1.0 if ALL of the following are true:\n    - Produces a single, cohesive Markdown command file with YAML frontmatter at top (--- ... ---).\n    - Frontmatter includes at least: description, argument-hint, allowed-tools; allowed-tools is meaningfully restricted\n      (e.g., Bash(gh:*), Bash(git:*), Read) and does NOT default to Bash(*) without clear need.\n    - Uses dynamic arguments with $1 as PR number and supports optional extra options (via $ARGUMENTS or positional args).\n    - Contains inline bash execution using the Claude command pattern (e.g., !`...`) and includes:\n      (a) at least one gh command that fetches PR metadata, AND\n      (b) at least one command that lists changed files (gh or git).\n    - Includes at least one @file reference to a plausible repo policy/checklist doc and handles possible missing file\n      gracefully (e.g., test -f with conditional instructions, or a bash check whose output is interpreted).\n    - Contains explicit argument validation behavior: if $1 missing/invalid, instruct Claude to ask user and show usage.\n    - The body is primarily imperative instructions to Claude (e.g., \"Fetch...\", \"Review...\", \"Produce report...\") rather than\n      user-facing marketing/explanations (an HTML comment for usage is acceptable).\n\n    Score 0.5 if the command is mostly correct but misses ONE major element above, such as:\n    - frontmatter incomplete (missing argument-hint or allowed-tools), OR\n    - no graceful handling of missing referenced file, OR\n    - only one of the two required bash context gathers (metadata vs changed files), OR\n    - validation is weak/implicit (mentions usage but doesn't direct Claude to ask/handle missing/invalid).\n\n    Score 0.0 if TWO OR MORE major elements are missing, or if the output is not a command file (e.g., generic advice),\n    or if it is written primarily as an explanation to the user rather than instructions to Claude.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of Command Development by requiring practical knowledge of frontmatter fields, dynamic arguments, bash execution, file references, and the critical concept that commands are instructions FOR Claude. It tests transferable patterns (tool restriction, validation, context gathering) rather than trivia. However, the grader has minor brittleness: it requires specific bash patterns (!`...`) and specific tools (gh/git) when alternatives could work, and the 0.5 partial credit criteria is somewhat vague about which 'major element' counts. The task is highly realistic—teams genuinely need PR audit workflows. Overall, a well-designed test with room for grader improvement."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis:\n# 1) Domain knowledge taught:\n#    - How Claude Code slash commands are authored as Markdown prompts with optional YAML frontmatter.\n#    - Frontmatter fields that control discoverability and safety (description, argument-hint, allowed-tools, model).\n#    - How to pass user input via dynamic arguments ($1, $2, $ARGUMENTS).\n#    - How to pull in context via file references (@path) and inline bash execution (!`...`).\n#    - The critical framing that the command body is instructions *to Claude*, not explanatory text *to the user*.\n# 2) Workflow decisions enabled:\n#    - When to restrict tools (least-privilege allowed-tools) vs inheriting conversation tools.\n#    - When to use file references for static context vs bash for dynamic repo/PR state.\n#    - How to design a clear command interface (argument-hint) and validate inputs.\n# 3) Practitioner struggles without this skill (test target):\n#    - Writing user-facing descriptions instead of actionable agent instructions.\n#    - Forgetting/incorrectly using placeholders ($1 / $ARGUMENTS) so arguments don’t work.\n#    - Omitting tool restrictions or using overly broad permissions (e.g., Bash(*), *).\n#    - Not leveraging @file context or !`...` to gather PR data automatically, yielding shallow reviews.\nname: claude-code-review-pr-command-with-args-tools-and-context\nskills:\n  - Command Development\nprompt: |\n  You’re adding a new **project** slash command for Claude Code to standardize pull request reviews.\n\n  Create the full contents of a Markdown command file to be saved at:\n\n    .claude/commands/git/review-pr.md\n\n  Requirements:\n\n  1) It must be invokable like:\n       /review-pr <pr-number> [focus-areas...]\n     where focus-areas is optional free text like \"security performance tests\".\n\n  2) Include YAML frontmatter with:\n     - description\n     - argument-hint documenting the interface\n     - allowed-tools using least privilege (it should be able to read files and run only git/gh-related bash commands; do NOT use \"*\" or unrestricted Bash(*))\n\n  3) The command body must be written as **instructions to Claude** (imperative), not a message explaining to the user what will happen.\n\n  4) Use dynamic arguments correctly:\n     - Use $1 for the PR number\n     - Use $ARGUMENTS or $2+ to incorporate optional focus areas\n\n  5) Include at least two sources of context:\n     - A static file reference using @ to include both:\n       @docs/review-checklist.md\n       @CONTRIBUTING.md\n     - Inline bash execution (using the !`...` form) to fetch PR metadata and changes via gh and/or git (e.g., title/body, changed files list, diff/stat). Use $1 in those bash commands.\n\n  6) Add input validation behavior in the instructions (e.g., if $1 is missing or not a number, ask the user for the correct usage).\n\n  7) Instruct Claude to output a structured review (clear sections, actionable items, and some notion of severity/priority).\n\n  Output only the command file content (frontmatter + Markdown body).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant produced a valid Claude Code slash command file that demonstrates command development best practices.\n\n    Score 1.0 if the response includes ALL of the following:\n    - A Markdown command with YAML frontmatter delimited by '---' at the top.\n    - Frontmatter contains:\n      - 'description'\n      - 'argument-hint' that matches \"/review-pr <pr-number> [focus-areas...]\" (wording can vary)\n      - 'allowed-tools' showing least-privilege access, including Read and Bash restricted to git/gh (e.g., Bash(git:*), Bash(gh:*)); it must NOT use '*' or unrestricted Bash(*).\n    - The body is written as instructions to Claude (imperative/agent directive), not primarily user-facing explanation; it may include a short usage message for missing args but should mostly direct Claude’s actions.\n    - Correct use of dynamic arguments:\n      - Uses $1 as the PR number within the command logic/instructions.\n      - Uses $ARGUMENTS or $2+ to incorporate optional focus areas.\n    - Includes BOTH required static file references using '@':\n      - @docs/review-checklist.md\n      - @CONTRIBUTING.md\n    - Includes at least one inline bash execution snippet using the !`...` pattern that runs a git/gh command and references $1 (e.g., gh pr view $1, gh pr diff $1, git diff ...).\n    - Contains explicit input validation behavior for missing/invalid $1 (ask user for correct usage or prompt for PR number).\n    - Requests a structured review output with multiple sections and some severity/priority/actionability guidance.\n\n    Score 0.5 if the response is broadly command-shaped and includes frontmatter and at least 3 of the key mechanics (dynamic args, allowed-tools restriction, @file references, !`bash`, structured output), but is missing one or more required elements above (e.g., missing one of the @files, lacks validation, or tools are overly broad).\n\n    Score 0.0 if it does not provide a plausible command file (no frontmatter or no command content), or it omits most key mechanics (no dynamic args and no @ references and no !`bash`), or uses dangerously broad tool permissions (e.g., allowed-tools: \"*\" or Bash(*)) while otherwise failing requirements.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of command development concepts and tests transferable knowledge (non-obviousness: 9/10). It requires understanding YAML frontmatter, dynamic arguments, file references, bash execution, tool restrictions, and the critical distinction between user-facing vs agent-facing instructions—all genuine domain knowledge. The grader is mostly robust (clarity: 8/10), accepting variations in wording while checking for concrete mechanics, though it could be slightly more flexible on exact file paths. The task is highly realistic (realism: 9/10), representing a common real-world need to standardize PR reviews with proper security constraints. Minor deduction for the grader being somewhat prescriptive about specific file names (@docs/review-checklist.md, @CONTRIBUTING.md) when the core skill is understanding the @ syntax pattern itself, though this is a minor issue since the test still primarily evaluates transferable knowledge rather than trivia."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill defines how to extend Claude Code's capabilities through slash commands. This includes the distinction between instructions for the agent versus messages for the user, the lifecycle of a command (frontmatter configuration vs. prompt content), and the specific syntax for dynamic inputs (arguments, file references, and shell execution).\n\n2.  **What workflow decisions does it enable?**\n    It enables developers to decide how to structure a command: choosing between project-specific, personal, or plugin-scoped commands; deciding which tools (Read, Bash, etc.) the command needs permission to use; and determining how to fetch context (static file references, argument-based file references, or dynamic bash output).\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    -   Writing commands as if they are talking to a human user instead of instructing the Claude agent.\n    -   Incorrectly referencing files within a plugin's directory structure (missing `${CLAUDE_PLUGIN_ROOT}`).\n    -   Failing to define `allowed-tools` in the frontmatter, causing the command to fail when it attempts to read files or run bash.\n    -   Syntax errors in argument handling (`$1` vs `$ARGUMENTS`).\n\n### Test Case\n\n```yaml\nname: plugin-command-development-audit\nskills:\n- Command Development\nprompt: |\n  I am developing a plugin for Claude Code and need to create a slash command called `/audit`. \n\n  Please write the content for the `.md` file that defines this command with the following requirements:\n  1. It should take one argument: a file path to be audited.\n  2. The command should use the 'haiku' model for cost-efficiency.\n  3. It must have a description that appears in the `/help` menu: \"Audit a file against plugin security standards\".\n  4. It requires permission to read files.\n  5. The command must reference a \"standards.md\" file located in a `resources/` directory at the root of the plugin.\n  6. The command must include the content of the file provided in the argument.\n  7. The prompt body must be written as a directive to Claude (the agent), instructing it to compare the provided file against the standards and list any violations.\n\n  Provide only the Markdown file content.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the slash command Markdown file based on the following criteria:\n\n    1. **Frontmatter Configuration**: \n       - Does it include `model: haiku`?\n       - Does it include a `description` field with the requested text?\n       - Does it include `allowed-tools: Read` (or a list containing Read)?\n    2. **Argument & File Syntax**:\n       - Does it use `@$1` or `@$ARGUMENTS` to reference the user-provided file?\n       - Does it correctly use `@${CLAUDE_PLUGIN_ROOT}/resources/standards.md` to reference the plugin's internal file?\n    3. **Tone and Directives**:\n       - Is the prompt content written as instructions *to Claude* (e.g., \"Analyze this file\", \"Compare @$1 to...\")? \n       - It should NOT be written as a message to a human user (e.g., \"I will now audit your file\").\n\n    Score 1.0 if all three criteria (Frontmatter, Syntax, and Tone) are correctly implemented.\n    Score 0.5 if the syntax and frontmatter are mostly correct but it fails the \"Tone\" requirement (writing for the user) or misses the specific `${CLAUDE_PLUGIN_ROOT}` variable.\n    Score 0.0 if the frontmatter is missing, tool permissions are absent, or file referencing syntax is completely wrong.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable command development knowledge: frontmatter configuration, tool permissions, file referencing syntax (including plugin-specific ${CLAUDE_PLUGIN_ROOT}), and the critical distinction between writing instructions for Claude vs. messages for users. The grader is mostly robust, accepting variations in syntax while checking core concepts. However, the grader could be slightly more explicit about accepting equivalent formulations (e.g., 'allowed-tools' as string vs array, or alternative valid file reference patterns). The task is highly realistic—creating a plugin command with specific requirements is exactly what practitioners do. Minor deduction for grader brittleness around edge cases, but overall this is a strong test of genuine domain knowledge rather than trivia."
        },
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n**1. What domain knowledge does this skill teach?**\n- Slash commands are Markdown files with optional YAML frontmatter\n- Commands are instructions FOR Claude (the agent), not messages to users\n- Dynamic arguments using $ARGUMENTS, $1, $2, etc. for positional capture\n- File references using @ syntax to include file contents\n- Tool restrictions via allowed-tools frontmatter\n- Command organization in project (.claude/commands/) or personal (~/.claude/commands/) directories\n\n**2. What workflow decisions does it enable?**\n- Writing commands as Claude-directed instructions vs user-facing messages\n- Choosing between $ARGUMENTS (all args) vs positional ($1, $2) arguments\n- When to restrict tool access for safety\n- How to combine file references with dynamic arguments\n- Flat vs namespaced command organization\n\n**3. What would a practitioner struggle with without this skill?**\n- Understanding that command content becomes Claude's instructions (the \"for agent, not user\" paradigm)\n- Correctly using dynamic argument syntax with file references\n- Knowing when and how to restrict tool access\n- Structuring commands for reusability and clarity\n\n```yaml\nname: create-slash-command-with-arguments\nskills:\n  - Command Development\nprompt: |\n  I want to create a slash command for Claude Code that reviews a specific file for accessibility issues. The command should:\n  \n  1. Accept a file path as an argument\n  2. Read the contents of that file\n  3. Check for accessibility issues like missing alt text, color contrast concerns, ARIA usage, keyboard navigation, etc.\n  4. Only need read access to files (no writing or bash commands)\n  \n  The command should be called \"a11y-review\" and be available to my whole team.\n  \n  Show me the complete command file with proper structure, including where to place it and the full contents.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly creates a Claude Code slash command for accessibility review.\n    \n    Score 1.0 if ALL of the following are present:\n    - File location is .claude/commands/a11y-review.md (or in a subdirectory like .claude/commands/review/a11y.md)\n    - Contains YAML frontmatter with description field\n    - Uses allowed-tools to restrict to Read (or similar read-only access)\n    - Uses file reference syntax combining @ with argument (like @$1 or @$ARGUMENTS)\n    - Command body is written as INSTRUCTIONS TO CLAUDE about what to do (e.g., \"Review...\", \"Analyze...\", \"Check...\"), NOT as a message to the user about what will happen\n    - Mentions accessibility-specific checks (alt text, ARIA, color contrast, keyboard navigation, etc.)\n    \n    Score 0.5 if:\n    - Has correct file format and location\n    - Uses arguments and file references correctly\n    - But is missing tool restrictions OR is written as user-facing message instead of Claude instructions\n    \n    Score 0.0 if:\n    - Wrong file format (not Markdown with frontmatter)\n    - Missing dynamic argument usage for the file path\n    - Written entirely as a message TO the user describing what the command does rather than instructions for Claude\n    - Missing file reference (@ syntax) entirely\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of command development concepts (file references with arguments, tool restrictions, agent-directed instructions) and tests transferable knowledge about writing instructions for AI agents vs. user-facing messages. However, it has moderate clarity issues: the grader accepts both '@$1' and '@$ARGUMENTS' as equivalent when they're semantically different (positional vs. all args), and the 0.5 scoring criteria is ambiguous about which combination of missing elements triggers partial credit. The realism is excellent - creating accessibility review commands is a genuine practitioner need."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this Command Development skill, I can see it teaches:\n\n1. **Core domain knowledge:** How to create slash commands (reusable prompts) for Claude Code, including YAML frontmatter configuration, dynamic arguments, file references, and bash execution integration.\n\n2. **Key workflow decisions:** When to use project vs personal commands, how to structure frontmatter fields, whether to use positional arguments or capture all arguments, and how to organize commands effectively.\n\n3. **Critical practitioner challenge:** The most fundamental concept is that commands must be written as instructions FOR Claude to execute, not as messages TO users. Without understanding this, along with proper argument handling and frontmatter configuration, practitioners would create non-functional or ineffective commands.\n\nHere's a test case that measures understanding of these core concepts:\n\n```yaml\nname: create-pr-review-command\nskills:\n  - Command Development\nprompt: |\n  Create a slash command for reviewing pull requests in a project. The command should:\n  \n  1. Be named \"review-pr\" \n  2. Accept two arguments: a PR number (required) and a focus area (optional, defaults to \"general\")\n  3. Use git to get the list of changed files\n  4. Be restricted to only use Read and Bash tools (with git commands only)\n  5. Include a helpful description that appears in /help\n  \n  The command should instruct Claude to:\n  - List the files changed in the PR\n  - Review each changed file\n  - Focus the review on the specified area (security, performance, style, or general)\n  - Provide specific line-by-line feedback\n  \n  Write the complete markdown file content for this command, including any necessary frontmatter.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response creates a valid slash command for Claude Code that meets the requirements.\n    \n    The command MUST:\n    1. Be written as instructions FOR Claude to execute (not messages to users)\n    2. Include YAML frontmatter with description and allowed-tools fields\n    3. Use proper argument syntax ($1 for PR number, $2 or similar for focus area)\n    4. Include bash execution to get changed files using git\n    5. Provide clear review instructions to Claude\n    \n    Score 1.0 if: The command has proper YAML frontmatter (with description and allowed-tools: Read, Bash(git:*)), uses correct argument syntax ($1, $2 or $ARGUMENTS), includes bash execution for git commands (using !`git...` syntax), handles the optional second argument with a default, AND most importantly is written as instructions for Claude to follow (not as messages to users).\n    \n    Score 0.5 if: The command has most elements correct but has minor issues like: missing argument-hint field, imperfect bash syntax (but conceptually correct), or minor frontmatter issues. The command must still be written as instructions for Claude.\n    \n    Score 0.0 if: The command is written as messages TO users instead of instructions FOR Claude, lacks YAML frontmatter, doesn't handle arguments properly, doesn't include bash execution for git, or has major structural issues that would prevent it from functioning.\n    \n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of command development concepts and tests transferable knowledge (non-obviousness: 8/10). It requires understanding YAML frontmatter, argument handling, bash execution syntax, and the critical concept that commands are instructions FOR Claude. However, the grader has moderate brittleness issues (clarity: 6/10) - it requires specific syntax like `!`git...`` and `Bash(git:*)` when alternatives like different bash execution patterns or tool specifications might be valid. The task is realistic (realism: 8/10) as PR review commands are genuinely useful. The test avoids pure trivia and focuses on practical application, but the grader's specificity about exact syntax patterns (rather than testing whether the command would actually work) reduces robustness."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: github-pr-review-slash-command\nskills:\n  - Command Development\nprompt: |\n  You’re adding a *project* slash command for Claude Code at:\n\n    .claude/commands/git/pr-review.md\n\n  Write the full contents of that command file (Markdown + any YAML frontmatter), with NO additional explanation before or after.\n\n  Command goals/requirements:\n  - Invoked as: `/pr-review <pr-number> [focus]`\n    - `<pr-number>` is required.\n    - `[focus]` is optional (e.g., \"security\", \"performance\", \"tests\", \"API\", etc.).\n  - The command must be written as **instructions for Claude** (agent-consumable), not a message describing what will happen.\n  - Include YAML frontmatter with:\n    - `description`\n    - `argument-hint`\n    - `allowed-tools` restricted to the minimum needed, including Bash limited to safe scopes for GitHub + git usage (no `Bash(*)`).\n  - Use dynamic arguments correctly (`$1` for PR number and `$2` or `$ARGUMENTS` for the optional focus).\n  - Gather live context via at least TWO inline bash executions (e.g., `gh pr view`, `gh pr diff`, `gh pr checks`, `git diff --name-only`, etc.).\n  - Include at least one `@` file reference to incorporate a local checklist/standards document (pick a plausible repo-relative path).\n  - Include explicit input validation behavior:\n    - If PR number is missing/invalid, the command should respond with a concise usage message and stop (use a conditional pattern or equivalent).\n  - Output requirements for the review:\n    - Provide a structured review (sections like Summary, Major Issues, Minor Issues, Suggested Tests, and a Decision/Recommendation).\n    - Ask for clarifications only when necessary and be specific.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant produced a valid Claude Code slash command Markdown file that satisfies the requirements.\n\n    Score 1.0 if ALL are true:\n    - Response is primarily a Markdown command file, starting with YAML frontmatter delimited by '---' and containing:\n      - description\n      - argument-hint that matches `/pr-review <pr-number> [focus]` (or equivalent bracket notation)\n      - allowed-tools that includes Read and Bash with restricted scopes (e.g., Bash(gh:*), Bash(git:*)); does NOT use overly-broad Bash(*) or '*'.\n    - Command text is written as instructions to Claude (imperatives like \"Fetch PR...\", \"Review...\", \"Provide...\") rather than meta-explanation to the user.\n    - Uses dynamic arguments appropriately:\n      - references $1 as the PR number\n      - references $2 and/or $ARGUMENTS as the optional focus\n    - Includes explicit input validation that handles missing/invalid $1 by outputting a usage message and not proceeding (can be via $IF(...) pattern or clearly equivalent gating language).\n    - Includes at least TWO inline bash executions to gather context, recognizable as an inline execution construct (e.g., `!` followed by a backticked shell command), and they are relevant to PR review (gh/git).\n    - Includes at least one file reference using '@' to pull in a local standards/checklist document.\n    - Specifies a structured review output with multiple sections (at minimum: Summary, Issues, Suggested Tests, Recommendation/Decision), and requests clarifications only when necessary.\n\n    Score 0.5 if the response is close but misses ONE major requirement, such as:\n    - only one bash execution, OR\n    - missing file reference '@...', OR\n    - missing explicit validation/usage behavior, OR\n    - allowed-tools present but too permissive (e.g., Bash(*) or '*') while everything else is strong, OR\n    - weak structure (some sections missing) but command otherwise correct.\n\n    Score 0.0 if TWO OR MORE major requirements are missing, or if the response is mostly an explanation instead of a command file, or if it lacks frontmatter and dynamic argument usage.\n\n    Be flexible about exact wording and paths, but strict about the presence of the key mechanisms: frontmatter, restricted tools, dynamic args, bash context gathering, '@' file reference, and validation/usage gating.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of command development concepts (frontmatter, dynamic arguments, bash execution, file references, validation) and tests transferable knowledge about creating agent-consumable instructions with proper tool restrictions. However, it has moderate clarity issues: the grader is overly prescriptive about implementation details (requiring exactly TWO bash executions, specific section names like 'Summary/Issues/Tests/Recommendation') when functionally equivalent alternatives should be accepted. The realism is good—PR review commands are genuinely useful—but the specific combination of requirements (exactly 2 bash calls, mandatory '@' reference, specific output structure) feels somewhat contrived rather than reflecting natural practitioner needs."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "The Command Development skill teaches how to design reusable slash commands for Claude: structuring commands as Markdown files with YAML frontmatter, limiting tools, specifying models, and using dynamic arguments and file references so Claude receives precise instructions. It enables workflow choices like when to require frontmatter fields, how to scope tools (e.g., Bash with git), how to capture positional arguments versus `$ARGUMENTS`, and when to organize commands in namespaces. Practitioners without this skill struggle to craft commands that actually instruct Claude (instead of describing behavior to users), enforce argument validation, or safely gather context (like repository state) via allowed tools. They may omit argument hints, misuse file references, or forget to gate commands on required args, leading to brittle automations. \n\n```yaml\nname: service-audit-command-spec\nskills:\n  - Command Development\nprompt: |\n  You need a reusable slash command for Claude Code called `/service-audit` that ops engineers will run from project repos.\n\n  Produce the complete Markdown file (including YAML frontmatter) for `.claude/commands/ops/service-audit.md` with these requirements:\n\n  1. Frontmatter must include:\n     - description (≤60 characters) summarizing the task\n     - model set to `haiku`\n     - allowed-tools limited to `Read, Bash(git:*)`\n     - argument-hint documenting two positional args: `[service-path] [severity]`\n\n  2. Command behavior:\n     - Commands are instructions for Claude (no “this command will…” explanations).\n     - If `$1` (service path) is missing, instruct Claude to stop and tell the user to run `/service-audit [service-path] [severity]`, explaining what each argument represents before doing anything else.\n     - When `$1` exists, Claude must:\n       a. Include the referenced file or directory contents via `@$1`.\n       b. Run `git status --short $1` via Bash (respecting allowed tools) to capture pending changes.\n       c. Interpret `$2` as severity; if `$2` not provided, treat it as `medium` and state that in the output.\n\n     - After gathering context, Claude should analyze the service for deployment readiness, highlighting:\n       * Outstanding changes from the git status output\n       * Required fixes prior to deploy, prioritized by severity\n       * Next steps customized to the severity level\n\n  3. The instructions should be a single Markdown command file suitable for Claude consumption. \ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the assistant’s Markdown command.\n\n    Score 1.0 if:\n    - YAML frontmatter includes description (≤60 chars), model: haiku, allowed-tools: Read, Bash(git:*), and argument-hint “[service-path] [severity]”.\n    - Body treats content as instructions to Claude, not explanations to the user.\n    - There is explicit logic that halts when $1 is missing and provides the required usage guidance.\n    - When $1 exists, instructions include: referencing @$1, running git status via Bash, handling $2 with default “medium” when absent, and producing the requested analysis (changes, fixes, next steps tied to severity).\n\n    Score 0.5 if the response covers at least half the requirements but misses key elements (e.g., lacks default severity handling or omits git status) while still showing understanding of command structure.\n\n    Score 0.0 if the response is not a valid command file or ignores most requirements.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong understanding of command development concepts (frontmatter fields, argument validation, tool scoping, file references, bash execution) and tests transferable knowledge about instructing agents vs. describing to users. However, it has moderate clarity issues: the grader accepts solutions that 'halt when $1 is missing' but doesn't specify whether this should be done via conditional logic in the prompt itself (which isn't standard Markdown/YAML) or through Claude's interpretation of the instructions. The test also requires specific implementation details (exact git command syntax, specific severity default handling) that could reject valid alternative approaches. The realism is high (ops engineers auditing services is genuine), and non-obviousness is good (tests gotchas like agent-facing instructions, argument validation patterns, tool scoping), but the grader's brittleness around implementation specifics and ambiguity about conditional logic mechanisms prevents a higher score."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "The Command Development skill teaches how to author reusable slash commands for Claude, covering the Markdown structure, YAML frontmatter fields (like description, allowed-tools, argument-hint), dynamic argument handling, file and plugin references, inline Bash execution, validation, and organization best practices. It enables workflow decisions about when to add frontmatter controls (e.g., restricting allowed tools, choosing models), when to use dynamic arguments vs. defaults, how and when to pull live context via Bash, how to reference files and plugin resources, and how to validate inputs before executing work. Without this skill, practitioners often write commands as user-facing descriptions instead of instructions for Claude, omit necessary tool restrictions or argument hints, forget to validate inputs, fail to request dynamic data (like git diffs) safely, and miss opportunities to reference files or plugin roots, resulting in fragile, inconsistent slash commands.\n\n```yaml\nname: review-pr-workflow-command\nskills:\n- Command Development\nprompt: |\n  You need to define a reusable slash command named /review-pr-workflow that Claude can execute. Provide the complete Markdown content of the command (frontmatter plus body). The command should:\n  * Use YAML frontmatter to describe the command (clear description under 60 characters), restrict tools to Read and Bash(git:*), and include an argument hint for `[pr-number] [priority]`.\n  * Accept a PR number as the first argument (`$1`) and optionally a priority label (`$2`), validating that the PR number is provided with an `$IF` block that prompts for usage when missing.\n  * Run a Bash command inline to capture the list of files changed in the target PR (e.g., `git diff --name-only origin/main...pr/$1`) and surface that list before reviewing.\n  * Instruct Claude to review each changed file for correctness, tests, and documentation impacts, referencing a repository checklist file (use `@docs/pr-review-template.md`) and requiring severity ratings with line numbers.\n  * Treat the priority argument as “standard” when not supplied and include that context in the review instructions.\n  * Emphasize that the workflow should produce actionable findings (e.g., issue list, follow-up actions, test recommendations).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response provides a complete slash command definition that demonstrates Command Development expertise.\n\n    Score 1.0 if: The response is a Markdown command for /review-pr-workflow that includes YAML frontmatter with description, allowed-tools listing Read and Bash(git:*), and argument-hint for `[pr-number] [priority]`, uses `$IF` to validate the PR number, runs an inline Bash command to list PR files, references `@docs/pr-review-template.md`, treats priority as “standard” when missing, and instructs Claude to perform a file-by-file review with severity ratings, line numbers, tests, and follow-up actions.\n\n    Score 0.5 if: The response mostly satisfies the above but is missing one key element (e.g., no inline Bash command, missing argument validation, or missing file reference), yet still shows awareness of command structure and intent.\n\n    Score 0.0 if: The response fails to present a Markdown slash command with appropriate frontmatter/instructions, lacks validation or dynamic behavior, or is framed as a user-facing description rather than instructions for Claude.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong understanding of Command Development by requiring frontmatter configuration, dynamic arguments, inline Bash execution, file references, and validation—all transferable patterns. However, it has moderate clarity issues: the $IF validation syntax is not documented in the skill (making it potentially ungrounded), and the grader's strict requirement for exact elements (e.g., 'severity ratings with line numbers') may reject valid alternative approaches to PR review workflows. The task is realistic and tests genuine workflow design knowledge rather than trivia."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: create-plugin-command\nskills:\n  - Command Development\nprompt: |\n  I am developing a plugin for Claude Code and need to create a new slash command file named `security-scan.md`.\n  \n  This command needs to do the following:\n  1. Execute a bash script located at `bin/scan.sh` inside the plugin's root directory.\n  2. The script takes one argument: the target directory to scan (which the user will provide when running the command).\n  3. After running the script, Claude should analyze the text output and summarize high-severity vulnerabilities.\n  \n  Please write the complete content for this markdown file, including the necessary YAML frontmatter to ensure it has permission to run the script.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the generated slash command file content.\n\n    The response must contain a YAML frontmatter block and a Markdown body.\n\n    Score 1.0 if all criteria are met:\n    1.  **Frontmatter**: Includes `allowed-tools` specifying `Bash` (or `Bash(*)`, `Bash(git:*)` etc) or `*`.\n    2.  **Plugin Variable**: Uses `${CLAUDE_PLUGIN_ROOT}` (or equivalent variable mentioned in the skill) to reference the script location `bin/scan.sh`.\n    3.  **Arguments**: Uses `$1` (or `$ARGUMENTS`) to pass the user's input to the bash command.\n    4.  **Execution Syntax**: Uses the correct syntax for bash execution within the prompt (e.g., `!`, backticks, or `Bash` tool invocation instruction).\n    5.  **Instruction Style**: The prompt text is an imperative instruction TO Claude (e.g., \"Analyze the output...\", \"Run the script...\"), NOT a description to the user (e.g., \"This command will run...\").\n\n    Score 0.5 if:\n    - The command structure is generally correct (frontmatter + body), but misses the specific `${CLAUDE_PLUGIN_ROOT}` variable (uses a hardcoded path or placeholder).\n    - OR if it misses the `allowed-tools` configuration.\n\n    Score 0.0 if:\n    - It does not resemble a Claude Code slash command (e.g., standard python script or just natural language description).\n    - It writes the prompt as a message to the user rather than instructions to the agent.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test measures transferable knowledge about plugin command structure (CLAUDE_PLUGIN_ROOT, allowed-tools, argument passing, bash execution syntax) rather than trivia. The grader is mostly robust with clear criteria, though it could be slightly more flexible (e.g., accepting alternative valid bash execution patterns). The task is realistic - creating a plugin command that executes a script is a common real-world need. However, the test loses points for non-obviousness because it explicitly asks for 'YAML frontmatter' and 'markdown file', making the format somewhat obvious rather than testing whether the model knows commands require this structure. The grader's 0.5 partial credit for missing CLAUDE_PLUGIN_ROOT is appropriate, though it could better handle edge cases like different valid bash execution syntaxes."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "```yaml\nname: slash-command-development\nskills:\n  - Command Development\nprompt: |\n  You need to create a slash command for reviewing pull requests. The command should:\n\n  1. Accept a PR number as the first argument\n  2. Accept a priority level (low/medium/high) as the second argument  \n  3. Reference specific files for focused review\n  4. Use appropriate YAML frontmatter\n  5. Be written as instructions FOR Claude (not messages TO the user)\n  \n  Create the complete command file content that would be saved as `.claude/commands/review-pr.md`.\n  \n  Show me the exact command content that should be in the file.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of slash command development principles.\n\n    Score 1.0 if:\n    - Command is written as instructions FOR Claude (using imperative language like \"Review\", \"Analyze\", \"Check\")\n    - Contains valid YAML frontmatter with appropriate fields (at minimum description, should include argument-hint)\n    - Uses correct dynamic argument syntax ($1 for PR number, $2 for priority)\n    - Includes file reference syntax (@) for reviewing specific files\n    - Has proper Markdown formatting\n    - Demonstrates understanding that commands are agent instructions, not user-facing messages\n\n    Score 0.5 if:\n    - Shows partial understanding but makes minor errors in frontmatter, argument syntax, or instruction style\n    - Uses correct basic structure but misses some key elements\n\n    Score 0.0 if:\n    - Writes commands as messages TO users instead of instructions FOR Claude\n    - Has invalid YAML frontmatter or missing required fields\n    - Uses incorrect argument syntax or file reference syntax\n    - Does not follow the command development patterns from the skill\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of command development principles and tests transferable knowledge (non-obviousness: 7/10). The grader is mostly robust and accepts multiple valid approaches, though it could be more specific about what constitutes 'appropriate fields' in frontmatter (clarity: 7/10). The task is realistic - creating a PR review command is a common real-world use case (realism: 8/10). However, the test could be improved by being more explicit about acceptable variations (e.g., whether allowed-tools is required, whether $ARGUMENTS could be used instead of positional arguments) and by avoiding potential brittleness around 'appropriate fields' which is somewhat subjective."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-dynamic-command-with-validation\nskills:\n  - Command Development\nprompt: |\n  You need to create a slash command named `/deploy` that deploys an application to a specified environment. The command should accept two arguments: the environment name (e.g., `staging`, `production`) and the version number (e.g., `1.2.3`). The command should include the following features:\n\n  1. Validate the environment name to ensure it is one of the following: `staging`, `production`. If the environment is invalid, the command should provide a helpful error message.\n  2. Validate the version number to ensure it follows the semantic versioning format (e.g., `1.2.3`). If the version is invalid, the command should provide a helpful error message.\n  3. If both arguments are valid, the command should execute a deployment script located at `${CLAUDE_PLUGIN_ROOT}/scripts/deploy.sh` with the environment and version as arguments.\n  4. The command should include a description and argument hint for better usability.\n  5. Use the `Bash` tool to execute the deployment script and handle any errors that may occur during the deployment process.\n\n  Please write the complete Markdown file for this command, including the YAML frontmatter.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the provided command to ensure it meets the specified requirements.\n\n    Score 1.0 if:\n    - The command includes a valid YAML frontmatter with a description and argument hint.\n    - The command validates the environment name and version number as specified.\n    - The command executes the deployment script with the correct arguments and handles errors.\n    - The command follows best practices for argument handling and error reporting.\n\n    Score 0.5 if:\n    - The command includes most of the required features but is missing one or two minor aspects.\n    - The command is mostly correct but has minor issues with validation or error handling.\n\n    Score 0.0 if:\n    - The command is completely incorrect or missing key features.\n    - The command does not validate the environment name or version number.\n    - The command does not execute the deployment script or handle errors.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates reasonable domain knowledge about command validation patterns and dynamic arguments, which are transferable concepts (non-obviousness: 7/10). However, the grader has significant brittleness issues - it uses vague criteria like 'mostly correct' and 'minor issues' without defining what constitutes valid solutions, and doesn't specify whether alternative validation approaches (regex vs bash test, different error message formats) would be accepted (clarity: 4/10). The task is realistic as deployment commands with validation are common in practice (realism: 8/10). Overall: (7+4+8)/30 = 0.63, rounded to 0.65 for demonstrating solid transferable knowledge despite grader weaknesses."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: command-development-skill-test\nskills:\n  - Command Development\nprompt: |\n  Create a slash command that deploys an application to a specified environment. The command should take two arguments: the environment (e.g., staging, production) and the version number. Use the `allowed-tools` field to ensure that only the necessary tools are used. Also, include a description and an `argument-hint` in the YAML frontmatter.\n\n  The deployment process involves running a Bash script that takes the environment and version as arguments. Assume the script is located in the `${CLAUDE_PLUGIN_ROOT}/scripts` directory and is named `deploy.sh`.\n\n  Provide the complete command, including the Markdown content and the YAML frontmatter.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes:\n    - A valid YAML frontmatter with `description`, `allowed-tools`, and `argument-hint` fields.\n    - A correct `deploy.sh` script invocation using Bash, passing the environment and version as arguments.\n    - The command is written in a clear and concise manner, following best practices for command development.\n\n    Score 0.5 if the response includes most of the required elements but lacks clarity or has minor errors in the frontmatter or script invocation.\n\n    Score 0.0 if the response does not include the necessary YAML frontmatter fields, the script invocation is incorrect, or the command does not follow best practices.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues across all dimensions. Non-obviousness (3/10): It tests specific trivia like the exact script name 'deploy.sh' and the ${CLAUDE_PLUGIN_ROOT} variable rather than transferable concepts about command structure. Clarity (5/10): The grader is somewhat vague ('clear and concise manner', 'best practices') and doesn't specify what constitutes valid alternatives (e.g., different bash syntax, inline commands vs script references). Realism (5/10): While deployment commands are realistic, requiring the exact plugin-specific pattern with ${CLAUDE_PLUGIN_ROOT} makes it contrived - a real practitioner might use various approaches. The test focuses too much on memorizing specific syntax from the skill document rather than understanding command development principles."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n",
          "score": 0.0,
          "rationale": "The agent's submission is completely empty - no test.yaml file was provided. This represents a complete failure to complete the assigned task of creating a test case for the Command Development skill."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Command Development",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 3,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Command Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Command Development\ndescription: This skill should be used when the user asks to \"create a slash command\", \"add a command\", \"write a custom command\", \"define command arguments\", \"use command frontmatter\", \"organize commands\", \"create command with file references\", \"interactive command\", \"use AskUserQuestion in command\", or needs guidance on slash command structure, YAML frontmatter fields, dynamic arguments, bash execution in commands, user interaction patterns, or command development best practices for Claude Code.\nversion: 0.2.0\n---\n\n# Command Development for Claude Code\n\n## Overview\n\nSlash commands are frequently-used prompts defined as Markdown files that Claude executes during interactive sessions. Understanding command structure, frontmatter options, and dynamic features enables creating powerful, reusable workflows.\n\n**Key concepts:**\n- Markdown file format for commands\n- YAML frontmatter for configuration\n- Dynamic arguments and file references\n- Bash execution for context\n- Command organization and namespacing\n\n## Command Basics\n\n### What is a Slash Command?\n\nA slash command is a Markdown file containing a prompt that Claude executes when invoked. Commands provide:\n- **Reusability**: Define once, use repeatedly\n- **Consistency**: Standardize common workflows\n- **Sharing**: Distribute across team or projects\n- **Efficiency**: Quick access to complex prompts\n\n### Critical: Commands are Instructions FOR Claude\n\n**Commands are written for agent consumption, not human consumption.**\n\nWhen a user invokes `/command-name`, the command content becomes Claude's instructions. Write commands as directives TO Claude about what to do, not as messages TO the user.\n\n**Correct approach (instructions for Claude):**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication issues\n\nProvide specific line numbers and severity ratings.\n```\n\n**Incorrect approach (messages to user):**\n```markdown\nThis command will review your code for security issues.\nYou'll receive a report with vulnerability details.\n```\n\nThe first example tells Claude what to do. The second tells the user what will happen but doesn't instruct Claude. Always use the first approach.\n\n### Command Locations\n\n**Project commands** (shared with team):\n- Location: `.claude/commands/`\n- Scope: Available in specific project\n- Label: Shown as \"(project)\" in `/help`\n- Use for: Team workflows, project-specific tasks\n\n**Personal commands** (available everywhere):\n- Location: `~/.claude/commands/`\n- Scope: Available in all projects\n- Label: Shown as \"(user)\" in `/help`\n- Use for: Personal workflows, cross-project utilities\n\n**Plugin commands** (bundled with plugins):\n- Location: `plugin-name/commands/`\n- Scope: Available when plugin installed\n- Label: Shown as \"(plugin-name)\" in `/help`\n- Use for: Plugin-specific functionality\n\n## File Format\n\n### Basic Structure\n\nCommands are Markdown files with `.md` extension:\n\n```\n.claude/commands/\n├── review.md           # /review command\n├── test.md             # /test command\n└── deploy.md           # /deploy command\n```\n\n**Simple command:**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication bypass\n- Insecure data handling\n```\n\nNo frontmatter needed for basic commands.\n\n### With YAML Frontmatter\n\nAdd configuration using YAML frontmatter:\n\n```markdown\n---\ndescription: Review code for security issues\nallowed-tools: Read, Grep, Bash(git:*)\nmodel: sonnet\n---\n\nReview this code for security vulnerabilities...\n```\n\n## YAML Frontmatter Fields\n\n### description\n\n**Purpose:** Brief description shown in `/help`\n**Type:** String\n**Default:** First line of command prompt\n\n```yaml\n---\ndescription: Review pull request for code quality\n---\n```\n\n**Best practice:** Clear, actionable description (under 60 characters)\n\n### allowed-tools\n\n**Purpose:** Specify which tools command can use\n**Type:** String or Array\n**Default:** Inherits from conversation\n\n```yaml\n---\nallowed-tools: Read, Write, Edit, Bash(git:*)\n---\n```\n\n**Patterns:**\n- `Read, Write, Edit` - Specific tools\n- `Bash(git:*)` - Bash with git commands only\n- `*` - All tools (rarely needed)\n\n**Use when:** Command requires specific tool access\n\n### model\n\n**Purpose:** Specify model for command execution\n**Type:** String (sonnet, opus, haiku)\n**Default:** Inherits from conversation\n\n```yaml\n---\nmodel: haiku\n---\n```\n\n**Use cases:**\n- `haiku` - Fast, simple commands\n- `sonnet` - Standard workflows\n- `opus` - Complex analysis\n\n### argument-hint\n\n**Purpose:** Document expected arguments for autocomplete\n**Type:** String\n**Default:** None\n\n```yaml\n---\nargument-hint: [pr-number] [priority] [assignee]\n---\n```\n\n**Benefits:**\n- Helps users understand command arguments\n- Improves command discovery\n- Documents command interface\n\n### disable-model-invocation\n\n**Purpose:** Prevent SlashCommand tool from programmatically calling command\n**Type:** Boolean\n**Default:** false\n\n```yaml\n---\ndisable-model-invocation: true\n---\n```\n\n**Use when:** Command should only be manually invoked\n\n## Dynamic Arguments\n\n### Using $ARGUMENTS\n\nCapture all arguments as single string:\n\n```markdown\n---\ndescription: Fix issue by number\nargument-hint: [issue-number]\n---\n\nFix issue #$ARGUMENTS following our coding standards and best practices.\n```\n\n**Usage:**\n```\n> /fix-issue 123\n> /fix-issue 456\n```\n\n**Expands to:**\n```\nFix issue #123 following our coding standards...\nFix issue #456 following our coding standards...\n```\n\n### Using Positional Arguments\n\nCapture individual arguments with `$1`, `$2`, `$3`, etc.:\n\n```markdown\n---\ndescription: Review PR with priority and assignee\nargument-hint: [pr-number] [priority] [assignee]\n---\n\nReview pull request #$1 with priority level $2.\nAfter review, assign to $3 for follow-up.\n```\n\n**Usage:**\n```\n> /review-pr 123 high alice\n```\n\n**Expands to:**\n```\nReview pull request #123 with priority level high.\nAfter review, assign to alice for follow-up.\n```\n\n### Combining Arguments\n\nMix positional and remaining arguments:\n\n```markdown\nDeploy $1 to $2 environment with options: $3\n```\n\n**Usage:**\n```\n> /deploy api staging --force --skip-tests\n```\n\n**Expands to:**\n```\nDeploy api to staging environment with options: --force --skip-tests\n```\n\n## File References\n\n### Using @ Syntax\n\nInclude file contents in command:\n\n```markdown\n---\ndescription: Review specific file\nargument-hint: [file-path]\n---\n\nReview @$1 for:\n- Code quality\n- Best practices\n- Potential bugs\n```\n\n**Usage:**\n```\n> /review-file src/api/users.ts\n```\n\n**Effect:** Claude reads `src/api/users.ts` before processing command\n\n### Multiple File References\n\nReference multiple files:\n\n```markdown\nCompare @src/old-version.js with @src/new-version.js\n\nIdentify:\n- Breaking changes\n- New features\n- Bug fixes\n```\n\n### Static File References\n\nReference known files without arguments:\n\n```markdown\nReview @package.json and @tsconfig.json for consistency\n\nEnsure:\n- TypeScript version matches\n- Dependencies are aligned\n- Build configuration is correct\n```\n\n## Bash Execution in Commands\n\nCommands can execute bash commands inline to dynamically gather context before Claude processes the command. This is useful for including repository state, environment information, or project-specific context.\n\n**When to use:**\n- Include dynamic context (git status, environment vars, etc.)\n- Gather project/repository state\n- Build context-aware workflows\n\n**Implementation details:**\nFor complete syntax, examples, and best practices, see `references/plugin-features-reference.md` section on bash execution. The reference includes the exact syntax and multiple working examples to avoid execution issues\n\n## Command Organization\n\n### Flat Structure\n\nSimple organization for small command sets:\n\n```\n.claude/commands/\n├── build.md\n├── test.md\n├── deploy.md\n├── review.md\n└── docs.md\n```\n\n**Use when:** 5-15 commands, no clear categories\n\n### Namespaced Structure\n\nOrganize commands in subdirectories:\n\n```\n.claude/commands/\n├── ci/\n│   ├── build.md        # /build (project:ci)\n│   ├── test.md         # /test (project:ci)\n│   └── lint.md         # /lint (project:ci)\n├── git/\n│   ├── commit.md       # /commit (project:git)\n│   └── pr.md           # /pr (project:git)\n└── docs/\n    ├── generate.md     # /generate (project:docs)\n    └── publish.md      # /publish (project:docs)\n```\n\n**Benefits:**\n- Logical grouping by category\n- Namespace shown in `/help`\n- Easier to find related commands\n\n**Use when:** 15+ commands, clear categories\n\n## Best Practices\n\n### Command Design\n\n1. **Single responsibility:** One command, one task\n2. **Clear descriptions:** Self-explanatory in `/help`\n3. **Explicit dependencies:** Use `allowed-tools` when needed\n4. **Document arguments:** Always provide `argument-hint`\n5. **Consistent naming:** Use verb-noun pattern (review-pr, fix-issue)\n\n### Argument Handling\n\n1. **Validate arguments:** Check for required arguments in prompt\n2. **Provide defaults:** Suggest defaults when arguments missing\n3. **Document format:** Explain expected argument format\n4. **Handle edge cases:** Consider missing or invalid arguments\n\n```markdown\n---\nargument-hint: [pr-number]\n---\n\n$IF($1,\n  Review PR #$1,\n  Please provide a PR number. Usage: /review-pr [number]\n)\n```\n\n### File References\n\n1. **Explicit paths:** Use clear file paths\n2. **Check existence:** Handle missing files gracefully\n3. **Relative paths:** Use project-relative paths\n4. **Glob support:** Consider using Glob tool for patterns\n\n### Bash Commands\n\n1. **Limit scope:** Use `Bash(git:*)` not `Bash(*)`\n2. **Safe commands:** Avoid destructive operations\n3. **Handle errors:** Consider command failures\n4. **Keep fast:** Long-running commands slow invocation\n\n### Documentation\n\n1. **Add comments:** Explain complex logic\n2. **Provide examples:** Show usage in comments\n3. **List requirements:** Document dependencies\n4. **Version commands:** Note breaking changes\n\n```markdown\n---\ndescription: Deploy application to environment\nargument-hint: [environment] [version]\n---\n\n<!--\nUsage: /deploy [staging|production] [version]\nRequires: AWS credentials configured\nExample: /deploy staging v1.2.3\n-->\n\nDeploy application to $1 environment using version $2...\n```\n\n## Common Patterns\n\n### Review Pattern\n\n```markdown\n---\ndescription: Review code changes\nallowed-tools: Read, Bash(git:*)\n---\n\nFiles changed: !`git diff --name-only`\n\nReview each file for:\n1. Code quality and style\n2. Potential bugs or issues\n3. Test coverage\n4. Documentation needs\n\nProvide specific feedback for each file.\n```\n\n### Testing Pattern\n\n```markdown\n---\ndescription: Run tests for specific file\nargument-hint: [test-file]\nallowed-tools: Bash(npm:*)\n---\n\nRun tests: !`npm test $1`\n\nAnalyze results and suggest fixes for failures.\n```\n\n### Documentation Pattern\n\n```markdown\n---\ndescription: Generate documentation for file\nargument-hint: [source-file]\n---\n\nGenerate comprehensive documentation for @$1 including:\n- Function/class descriptions\n- Parameter documentation\n- Return value descriptions\n- Usage examples\n- Edge cases and errors\n```\n\n### Workflow Pattern\n\n```markdown\n---\ndescription: Complete PR workflow\nargument-hint: [pr-number]\nallowed-tools: Bash(gh:*), Read\n---\n\nPR #$1 Workflow:\n\n1. Fetch PR: !`gh pr view $1`\n2. Review changes\n3. Run checks\n4. Approve or request changes\n```\n\n## Troubleshooting\n\n**Command not appearing:**\n- Check file is in correct directory\n- Verify `.md` extension present\n- Ensure valid Markdown format\n- Restart Claude Code\n\n**Arguments not working:**\n- Verify `$1`, `$2` syntax correct\n- Check `argument-hint` matches usage\n- Ensure no extra spaces\n\n**Bash execution failing:**\n- Check `allowed-tools` includes Bash\n- Verify command syntax in backticks\n- Test command in terminal first\n- Check for required permissions\n\n**File references not working:**\n- Verify `@` syntax correct\n- Check file path is valid\n- Ensure Read tool allowed\n- Use absolute or project-relative paths\n\n## Plugin-Specific Features\n\n### CLAUDE_PLUGIN_ROOT Variable\n\nPlugin commands have access to `${CLAUDE_PLUGIN_ROOT}`, an environment variable that resolves to the plugin's absolute path.\n\n**Purpose:**\n- Reference plugin files portably\n- Execute plugin scripts\n- Load plugin configuration\n- Access plugin templates\n\n**Basic usage:**\n\n```markdown\n---\ndescription: Analyze using plugin script\nallowed-tools: Bash(node:*)\n---\n\nRun analysis: !`node ${CLAUDE_PLUGIN_ROOT}/scripts/analyze.js $1`\n\nReview results and report findings.\n```\n\n**Common patterns:**\n\n```markdown\n# Execute plugin script\n!`bash ${CLAUDE_PLUGIN_ROOT}/scripts/script.sh`\n\n# Load plugin configuration\n@${CLAUDE_PLUGIN_ROOT}/config/settings.json\n\n# Use plugin template\n@${CLAUDE_PLUGIN_ROOT}/templates/report.md\n\n# Access plugin resources\n@${CLAUDE_PLUGIN_ROOT}/docs/reference.md\n```\n\n**Why use it:**\n- Works across all installations\n- Portable between systems\n- No hardcoded paths needed\n- Essential for multi-file plugins\n\n### Plugin Command Organization\n\nPlugin commands discovered automatically from `commands/` directory:\n\n```\nplugin-name/\n├── commands/\n│   ├── foo.md              # /foo (plugin:plugin-name)\n│   ├── bar.md              # /bar (plugin:plugin-name)\n│   └── utils/\n│       └── helper.md       # /helper (plugin:plugin-name:utils)\n└── plugin.json\n```\n\n**Namespace benefits:**\n- Logical command grouping\n- Shown in `/help` output\n- Avoid name conflicts\n- Organize related commands\n\n**Naming conventions:**\n- Use descriptive action names\n- Avoid generic names (test, run)\n- Consider plugin-specific prefix\n- Use hyphens for multi-word names\n\n### Plugin Command Patterns\n\n**Configuration-based pattern:**\n\n```markdown\n---\ndescription: Deploy using plugin configuration\nargument-hint: [environment]\nallowed-tools: Read, Bash(*)\n---\n\nLoad configuration: @${CLAUDE_PLUGIN_ROOT}/config/$1-deploy.json\n\nDeploy to $1 using configuration settings.\nMonitor deployment and report status.\n```\n\n**Template-based pattern:**\n\n```markdown\n---\ndescription: Generate docs from template\nargument-hint: [component]\n---\n\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/docs.md\n\nGenerate documentation for $1 following template structure.\n```\n\n**Multi-script pattern:**\n\n```markdown\n---\ndescription: Complete build workflow\nallowed-tools: Bash(*)\n---\n\nBuild: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh`\nTest: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/test.sh`\nPackage: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/package.sh`\n\nReview outputs and report workflow status.\n```\n\n**See `references/plugin-features-reference.md` for detailed patterns.**\n\n## Integration with Plugin Components\n\nCommands can integrate with other plugin components for powerful workflows.\n\n### Agent Integration\n\nLaunch plugin agents for complex tasks:\n\n```markdown\n---\ndescription: Deep code review\nargument-hint: [file-path]\n---\n\nInitiate comprehensive review of @$1 using the code-reviewer agent.\n\nThe agent will analyze:\n- Code structure\n- Security issues\n- Performance\n- Best practices\n\nAgent uses plugin resources:\n- ${CLAUDE_PLUGIN_ROOT}/config/rules.json\n- ${CLAUDE_PLUGIN_ROOT}/checklists/review.md\n```\n\n**Key points:**\n- Agent must exist in `plugin/agents/` directory\n- Claude uses Task tool to launch agent\n- Document agent capabilities\n- Reference plugin resources agent uses\n\n### Skill Integration\n\nLeverage plugin skills for specialized knowledge:\n\n```markdown\n---\ndescription: Document API with standards\nargument-hint: [api-file]\n---\n\nDocument API in @$1 following plugin standards.\n\nUse the api-docs-standards skill to ensure:\n- Complete endpoint documentation\n- Consistent formatting\n- Example quality\n- Error documentation\n\nGenerate production-ready API docs.\n```\n\n**Key points:**\n- Skill must exist in `plugin/skills/` directory\n- Mention skill name to trigger invocation\n- Document skill purpose\n- Explain what skill provides\n\n### Hook Coordination\n\nDesign commands that work with plugin hooks:\n- Commands can prepare state for hooks to process\n- Hooks execute automatically on tool events\n- Commands should document expected hook behavior\n- Guide Claude on interpreting hook output\n\nSee `references/plugin-features-reference.md` for examples of commands that coordinate with hooks\n\n### Multi-Component Workflows\n\nCombine agents, skills, and scripts:\n\n```markdown\n---\ndescription: Comprehensive review workflow\nargument-hint: [file]\nallowed-tools: Bash(node:*), Read\n---\n\nTarget: @$1\n\nPhase 1 - Static Analysis:\n!`node ${CLAUDE_PLUGIN_ROOT}/scripts/lint.js $1`\n\nPhase 2 - Deep Review:\nLaunch code-reviewer agent for detailed analysis.\n\nPhase 3 - Standards Check:\nUse coding-standards skill for validation.\n\nPhase 4 - Report:\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/review.md\n\nCompile findings into report following template.\n```\n\n**When to use:**\n- Complex multi-step workflows\n- Leverage multiple plugin capabilities\n- Require specialized analysis\n- Need structured outputs\n\n## Validation Patterns\n\nCommands should validate inputs and resources before processing.\n\n### Argument Validation\n\n```markdown\n---\ndescription: Deploy with validation\nargument-hint: [environment]\n---\n\nValidate environment: !`echo \"$1\" | grep -E \"^(dev|staging|prod)$\" || echo \"INVALID\"`\n\nIf $1 is valid environment:\n  Deploy to $1\nOtherwise:\n  Explain valid environments: dev, staging, prod\n  Show usage: /deploy [environment]\n```\n\n### File Existence Checks\n\n```markdown\n---\ndescription: Process configuration\nargument-hint: [config-file]\n---\n\nCheck file exists: !`test -f $1 && echo \"EXISTS\" || echo \"MISSING\"`\n\nIf file exists:\n  Process configuration: @$1\nOtherwise:\n  Explain where to place config file\n  Show expected format\n  Provide example configuration\n```\n\n### Plugin Resource Validation\n\n```markdown\n---\ndescription: Run plugin analyzer\nallowed-tools: Bash(test:*)\n---\n\nValidate plugin setup:\n- Script: !`test -x ${CLAUDE_PLUGIN_ROOT}/bin/analyze && echo \"✓\" || echo \"✗\"`\n- Config: !`test -f ${CLAUDE_PLUGIN_ROOT}/config.json && echo \"✓\" || echo \"✗\"`\n\nIf all checks pass, run analysis.\nOtherwise, report missing components.\n```\n\n### Error Handling\n\n```markdown\n---\ndescription: Build with error handling\nallowed-tools: Bash(*)\n---\n\nExecute build: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh 2>&1 || echo \"BUILD_FAILED\"`\n\nIf build succeeded:\n  Report success and output location\nIf build failed:\n  Analyze error output\n  Suggest likely causes\n  Provide troubleshooting steps\n```\n\n**Best practices:**\n- Validate early in command\n- Provide helpful error messages\n- Suggest corrective actions\n- Handle edge cases gracefully\n\n---\n\nFor detailed frontmatter field specifications, see `references/frontmatter-reference.md`.\nFor plugin-specific features and patterns, see `references/plugin-features-reference.md`.\nFor command pattern examples, see `examples/` directory.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Command Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Command Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Command Development\ndescription: This skill should be used when the user asks to \"create a slash command\", \"add a command\", \"write a custom command\", \"define command arguments\", \"use command frontmatter\", \"organize commands\", \"create command with file references\", \"interactive command\", \"use AskUserQuestion in command\", or needs guidance on slash command structure, YAML frontmatter fields, dynamic arguments, bash execution in commands, user interaction patterns, or command development best practices for Claude Code.\nversion: 0.2.0\n---\n\n# Command Development for Claude Code\n\n## Overview\n\nSlash commands are frequently-used prompts defined as Markdown files that Claude executes during interactive sessions. Understanding command structure, frontmatter options, and dynamic features enables creating powerful, reusable workflows.\n\n**Key concepts:**\n- Markdown file format for commands\n- YAML frontmatter for configuration\n- Dynamic arguments and file references\n- Bash execution for context\n- Command organization and namespacing\n\n## Command Basics\n\n### What is a Slash Command?\n\nA slash command is a Markdown file containing a prompt that Claude executes when invoked. Commands provide:\n- **Reusability**: Define once, use repeatedly\n- **Consistency**: Standardize common workflows\n- **Sharing**: Distribute across team or projects\n- **Efficiency**: Quick access to complex prompts\n\n### Critical: Commands are Instructions FOR Claude\n\n**Commands are written for agent consumption, not human consumption.**\n\nWhen a user invokes `/command-name`, the command content becomes Claude's instructions. Write commands as directives TO Claude about what to do, not as messages TO the user.\n\n**Correct approach (instructions for Claude):**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication issues\n\nProvide specific line numbers and severity ratings.\n```\n\n**Incorrect approach (messages to user):**\n```markdown\nThis command will review your code for security issues.\nYou'll receive a report with vulnerability details.\n```\n\nThe first example tells Claude what to do. The second tells the user what will happen but doesn't instruct Claude. Always use the first approach.\n\n### Command Locations\n\n**Project commands** (shared with team):\n- Location: `.claude/commands/`\n- Scope: Available in specific project\n- Label: Shown as \"(project)\" in `/help`\n- Use for: Team workflows, project-specific tasks\n\n**Personal commands** (available everywhere):\n- Location: `~/.claude/commands/`\n- Scope: Available in all projects\n- Label: Shown as \"(user)\" in `/help`\n- Use for: Personal workflows, cross-project utilities\n\n**Plugin commands** (bundled with plugins):\n- Location: `plugin-name/commands/`\n- Scope: Available when plugin installed\n- Label: Shown as \"(plugin-name)\" in `/help`\n- Use for: Plugin-specific functionality\n\n## File Format\n\n### Basic Structure\n\nCommands are Markdown files with `.md` extension:\n\n```\n.claude/commands/\n├── review.md           # /review command\n├── test.md             # /test command\n└── deploy.md           # /deploy command\n```\n\n**Simple command:**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication bypass\n- Insecure data handling\n```\n\nNo frontmatter needed for basic commands.\n\n### With YAML Frontmatter\n\nAdd configuration using YAML frontmatter:\n\n```markdown\n---\ndescription: Review code for security issues\nallowed-tools: Read, Grep, Bash(git:*)\nmodel: sonnet\n---\n\nReview this code for security vulnerabilities...\n```\n\n## YAML Frontmatter Fields\n\n### description\n\n**Purpose:** Brief description shown in `/help`\n**Type:** String\n**Default:** First line of command prompt\n\n```yaml\n---\ndescription: Review pull request for code quality\n---\n```\n\n**Best practice:** Clear, actionable description (under 60 characters)\n\n### allowed-tools\n\n**Purpose:** Specify which tools command can use\n**Type:** String or Array\n**Default:** Inherits from conversation\n\n```yaml\n---\nallowed-tools: Read, Write, Edit, Bash(git:*)\n---\n```\n\n**Patterns:**\n- `Read, Write, Edit` - Specific tools\n- `Bash(git:*)` - Bash with git commands only\n- `*` - All tools (rarely needed)\n\n**Use when:** Command requires specific tool access\n\n### model\n\n**Purpose:** Specify model for command execution\n**Type:** String (sonnet, opus, haiku)\n**Default:** Inherits from conversation\n\n```yaml\n---\nmodel: haiku\n---\n```\n\n**Use cases:**\n- `haiku` - Fast, simple commands\n- `sonnet` - Standard workflows\n- `opus` - Complex analysis\n\n### argument-hint\n\n**Purpose:** Document expected arguments for autocomplete\n**Type:** String\n**Default:** None\n\n```yaml\n---\nargument-hint: [pr-number] [priority] [assignee]\n---\n```\n\n**Benefits:**\n- Helps users understand command arguments\n- Improves command discovery\n- Documents command interface\n\n### disable-model-invocation\n\n**Purpose:** Prevent SlashCommand tool from programmatically calling command\n**Type:** Boolean\n**Default:** false\n\n```yaml\n---\ndisable-model-invocation: true\n---\n```\n\n**Use when:** Command should only be manually invoked\n\n## Dynamic Arguments\n\n### Using $ARGUMENTS\n\nCapture all arguments as single string:\n\n```markdown\n---\ndescription: Fix issue by number\nargument-hint: [issue-number]\n---\n\nFix issue #$ARGUMENTS following our coding standards and best practices.\n```\n\n**Usage:**\n```\n> /fix-issue 123\n> /fix-issue 456\n```\n\n**Expands to:**\n```\nFix issue #123 following our coding standards...\nFix issue #456 following our coding standards...\n```\n\n### Using Positional Arguments\n\nCapture individual arguments with `$1`, `$2`, `$3`, etc.:\n\n```markdown\n---\ndescription: Review PR with priority and assignee\nargument-hint: [pr-number] [priority] [assignee]\n---\n\nReview pull request #$1 with priority level $2.\nAfter review, assign to $3 for follow-up.\n```\n\n**Usage:**\n```\n> /review-pr 123 high alice\n```\n\n**Expands to:**\n```\nReview pull request #123 with priority level high.\nAfter review, assign to alice for follow-up.\n```\n\n### Combining Arguments\n\nMix positional and remaining arguments:\n\n```markdown\nDeploy $1 to $2 environment with options: $3\n```\n\n**Usage:**\n```\n> /deploy api staging --force --skip-tests\n```\n\n**Expands to:**\n```\nDeploy api to staging environment with options: --force --skip-tests\n```\n\n## File References\n\n### Using @ Syntax\n\nInclude file contents in command:\n\n```markdown\n---\ndescription: Review specific file\nargument-hint: [file-path]\n---\n\nReview @$1 for:\n- Code quality\n- Best practices\n- Potential bugs\n```\n\n**Usage:**\n```\n> /review-file src/api/users.ts\n```\n\n**Effect:** Claude reads `src/api/users.ts` before processing command\n\n### Multiple File References\n\nReference multiple files:\n\n```markdown\nCompare @src/old-version.js with @src/new-version.js\n\nIdentify:\n- Breaking changes\n- New features\n- Bug fixes\n```\n\n### Static File References\n\nReference known files without arguments:\n\n```markdown\nReview @package.json and @tsconfig.json for consistency\n\nEnsure:\n- TypeScript version matches\n- Dependencies are aligned\n- Build configuration is correct\n```\n\n## Bash Execution in Commands\n\nCommands can execute bash commands inline to dynamically gather context before Claude processes the command. This is useful for including repository state, environment information, or project-specific context.\n\n**When to use:**\n- Include dynamic context (git status, environment vars, etc.)\n- Gather project/repository state\n- Build context-aware workflows\n\n**Implementation details:**\nFor complete syntax, examples, and best practices, see `references/plugin-features-reference.md` section on bash execution. The reference includes the exact syntax and multiple working examples to avoid execution issues\n\n## Command Organization\n\n### Flat Structure\n\nSimple organization for small command sets:\n\n```\n.claude/commands/\n├── build.md\n├── test.md\n├── deploy.md\n├── review.md\n└── docs.md\n```\n\n**Use when:** 5-15 commands, no clear categories\n\n### Namespaced Structure\n\nOrganize commands in subdirectories:\n\n```\n.claude/commands/\n├── ci/\n│   ├── build.md        # /build (project:ci)\n│   ├── test.md         # /test (project:ci)\n│   └── lint.md         # /lint (project:ci)\n├── git/\n│   ├── commit.md       # /commit (project:git)\n│   └── pr.md           # /pr (project:git)\n└── docs/\n    ├── generate.md     # /generate (project:docs)\n    └── publish.md      # /publish (project:docs)\n```\n\n**Benefits:**\n- Logical grouping by category\n- Namespace shown in `/help`\n- Easier to find related commands\n\n**Use when:** 15+ commands, clear categories\n\n## Best Practices\n\n### Command Design\n\n1. **Single responsibility:** One command, one task\n2. **Clear descriptions:** Self-explanatory in `/help`\n3. **Explicit dependencies:** Use `allowed-tools` when needed\n4. **Document arguments:** Always provide `argument-hint`\n5. **Consistent naming:** Use verb-noun pattern (review-pr, fix-issue)\n\n### Argument Handling\n\n1. **Validate arguments:** Check for required arguments in prompt\n2. **Provide defaults:** Suggest defaults when arguments missing\n3. **Document format:** Explain expected argument format\n4. **Handle edge cases:** Consider missing or invalid arguments\n\n```markdown\n---\nargument-hint: [pr-number]\n---\n\n$IF($1,\n  Review PR #$1,\n  Please provide a PR number. Usage: /review-pr [number]\n)\n```\n\n### File References\n\n1. **Explicit paths:** Use clear file paths\n2. **Check existence:** Handle missing files gracefully\n3. **Relative paths:** Use project-relative paths\n4. **Glob support:** Consider using Glob tool for patterns\n\n### Bash Commands\n\n1. **Limit scope:** Use `Bash(git:*)` not `Bash(*)`\n2. **Safe commands:** Avoid destructive operations\n3. **Handle errors:** Consider command failures\n4. **Keep fast:** Long-running commands slow invocation\n\n### Documentation\n\n1. **Add comments:** Explain complex logic\n2. **Provide examples:** Show usage in comments\n3. **List requirements:** Document dependencies\n4. **Version commands:** Note breaking changes\n\n```markdown\n---\ndescription: Deploy application to environment\nargument-hint: [environment] [version]\n---\n\n<!--\nUsage: /deploy [staging|production] [version]\nRequires: AWS credentials configured\nExample: /deploy staging v1.2.3\n-->\n\nDeploy application to $1 environment using version $2...\n```\n\n## Common Patterns\n\n### Review Pattern\n\n```markdown\n---\ndescription: Review code changes\nallowed-tools: Read, Bash(git:*)\n---\n\nFiles changed: !`git diff --name-only`\n\nReview each file for:\n1. Code quality and style\n2. Potential bugs or issues\n3. Test coverage\n4. Documentation needs\n\nProvide specific feedback for each file.\n```\n\n### Testing Pattern\n\n```markdown\n---\ndescription: Run tests for specific file\nargument-hint: [test-file]\nallowed-tools: Bash(npm:*)\n---\n\nRun tests: !`npm test $1`\n\nAnalyze results and suggest fixes for failures.\n```\n\n### Documentation Pattern\n\n```markdown\n---\ndescription: Generate documentation for file\nargument-hint: [source-file]\n---\n\nGenerate comprehensive documentation for @$1 including:\n- Function/class descriptions\n- Parameter documentation\n- Return value descriptions\n- Usage examples\n- Edge cases and errors\n```\n\n### Workflow Pattern\n\n```markdown\n---\ndescription: Complete PR workflow\nargument-hint: [pr-number]\nallowed-tools: Bash(gh:*), Read\n---\n\nPR #$1 Workflow:\n\n1. Fetch PR: !`gh pr view $1`\n2. Review changes\n3. Run checks\n4. Approve or request changes\n```\n\n## Troubleshooting\n\n**Command not appearing:**\n- Check file is in correct directory\n- Verify `.md` extension present\n- Ensure valid Markdown format\n- Restart Claude Code\n\n**Arguments not working:**\n- Verify `$1`, `$2` syntax correct\n- Check `argument-hint` matches usage\n- Ensure no extra spaces\n\n**Bash execution failing:**\n- Check `allowed-tools` includes Bash\n- Verify command syntax in backticks\n- Test command in terminal first\n- Check for required permissions\n\n**File references not working:**\n- Verify `@` syntax correct\n- Check file path is valid\n- Ensure Read tool allowed\n- Use absolute or project-relative paths\n\n## Plugin-Specific Features\n\n### CLAUDE_PLUGIN_ROOT Variable\n\nPlugin commands have access to `${CLAUDE_PLUGIN_ROOT}`, an environment variable that resolves to the plugin's absolute path.\n\n**Purpose:**\n- Reference plugin files portably\n- Execute plugin scripts\n- Load plugin configuration\n- Access plugin templates\n\n**Basic usage:**\n\n```markdown\n---\ndescription: Analyze using plugin script\nallowed-tools: Bash(node:*)\n---\n\nRun analysis: !`node ${CLAUDE_PLUGIN_ROOT}/scripts/analyze.js $1`\n\nReview results and report findings.\n```\n\n**Common patterns:**\n\n```markdown\n# Execute plugin script\n!`bash ${CLAUDE_PLUGIN_ROOT}/scripts/script.sh`\n\n# Load plugin configuration\n@${CLAUDE_PLUGIN_ROOT}/config/settings.json\n\n# Use plugin template\n@${CLAUDE_PLUGIN_ROOT}/templates/report.md\n\n# Access plugin resources\n@${CLAUDE_PLUGIN_ROOT}/docs/reference.md\n```\n\n**Why use it:**\n- Works across all installations\n- Portable between systems\n- No hardcoded paths needed\n- Essential for multi-file plugins\n\n### Plugin Command Organization\n\nPlugin commands discovered automatically from `commands/` directory:\n\n```\nplugin-name/\n├── commands/\n│   ├── foo.md              # /foo (plugin:plugin-name)\n│   ├── bar.md              # /bar (plugin:plugin-name)\n│   └── utils/\n│       └── helper.md       # /helper (plugin:plugin-name:utils)\n└── plugin.json\n```\n\n**Namespace benefits:**\n- Logical command grouping\n- Shown in `/help` output\n- Avoid name conflicts\n- Organize related commands\n\n**Naming conventions:**\n- Use descriptive action names\n- Avoid generic names (test, run)\n- Consider plugin-specific prefix\n- Use hyphens for multi-word names\n\n### Plugin Command Patterns\n\n**Configuration-based pattern:**\n\n```markdown\n---\ndescription: Deploy using plugin configuration\nargument-hint: [environment]\nallowed-tools: Read, Bash(*)\n---\n\nLoad configuration: @${CLAUDE_PLUGIN_ROOT}/config/$1-deploy.json\n\nDeploy to $1 using configuration settings.\nMonitor deployment and report status.\n```\n\n**Template-based pattern:**\n\n```markdown\n---\ndescription: Generate docs from template\nargument-hint: [component]\n---\n\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/docs.md\n\nGenerate documentation for $1 following template structure.\n```\n\n**Multi-script pattern:**\n\n```markdown\n---\ndescription: Complete build workflow\nallowed-tools: Bash(*)\n---\n\nBuild: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh`\nTest: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/test.sh`\nPackage: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/package.sh`\n\nReview outputs and report workflow status.\n```\n\n**See `references/plugin-features-reference.md` for detailed patterns.**\n\n## Integration with Plugin Components\n\nCommands can integrate with other plugin components for powerful workflows.\n\n### Agent Integration\n\nLaunch plugin agents for complex tasks:\n\n```markdown\n---\ndescription: Deep code review\nargument-hint: [file-path]\n---\n\nInitiate comprehensive review of @$1 using the code-reviewer agent.\n\nThe agent will analyze:\n- Code structure\n- Security issues\n- Performance\n- Best practices\n\nAgent uses plugin resources:\n- ${CLAUDE_PLUGIN_ROOT}/config/rules.json\n- ${CLAUDE_PLUGIN_ROOT}/checklists/review.md\n```\n\n**Key points:**\n- Agent must exist in `plugin/agents/` directory\n- Claude uses Task tool to launch agent\n- Document agent capabilities\n- Reference plugin resources agent uses\n\n### Skill Integration\n\nLeverage plugin skills for specialized knowledge:\n\n```markdown\n---\ndescription: Document API with standards\nargument-hint: [api-file]\n---\n\nDocument API in @$1 following plugin standards.\n\nUse the api-docs-standards skill to ensure:\n- Complete endpoint documentation\n- Consistent formatting\n- Example quality\n- Error documentation\n\nGenerate production-ready API docs.\n```\n\n**Key points:**\n- Skill must exist in `plugin/skills/` directory\n- Mention skill name to trigger invocation\n- Document skill purpose\n- Explain what skill provides\n\n### Hook Coordination\n\nDesign commands that work with plugin hooks:\n- Commands can prepare state for hooks to process\n- Hooks execute automatically on tool events\n- Commands should document expected hook behavior\n- Guide Claude on interpreting hook output\n\nSee `references/plugin-features-reference.md` for examples of commands that coordinate with hooks\n\n### Multi-Component Workflows\n\nCombine agents, skills, and scripts:\n\n```markdown\n---\ndescription: Comprehensive review workflow\nargument-hint: [file]\nallowed-tools: Bash(node:*), Read\n---\n\nTarget: @$1\n\nPhase 1 - Static Analysis:\n!`node ${CLAUDE_PLUGIN_ROOT}/scripts/lint.js $1`\n\nPhase 2 - Deep Review:\nLaunch code-reviewer agent for detailed analysis.\n\nPhase 3 - Standards Check:\nUse coding-standards skill for validation.\n\nPhase 4 - Report:\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/review.md\n\nCompile findings into report following template.\n```\n\n**When to use:**\n- Complex multi-step workflows\n- Leverage multiple plugin capabilities\n- Require specialized analysis\n- Need structured outputs\n\n## Validation Patterns\n\nCommands should validate inputs and resources before processing.\n\n### Argument Validation\n\n```markdown\n---\ndescription: Deploy with validation\nargument-hint: [environment]\n---\n\nValidate environment: !`echo \"$1\" | grep -E \"^(dev|staging|prod)$\" || echo \"INVALID\"`\n\nIf $1 is valid environment:\n  Deploy to $1\nOtherwise:\n  Explain valid environments: dev, staging, prod\n  Show usage: /deploy [environment]\n```\n\n### File Existence Checks\n\n```markdown\n---\ndescription: Process configuration\nargument-hint: [config-file]\n---\n\nCheck file exists: !`test -f $1 && echo \"EXISTS\" || echo \"MISSING\"`\n\nIf file exists:\n  Process configuration: @$1\nOtherwise:\n  Explain where to place config file\n  Show expected format\n  Provide example configuration\n```\n\n### Plugin Resource Validation\n\n```markdown\n---\ndescription: Run plugin analyzer\nallowed-tools: Bash(test:*)\n---\n\nValidate plugin setup:\n- Script: !`test -x ${CLAUDE_PLUGIN_ROOT}/bin/analyze && echo \"✓\" || echo \"✗\"`\n- Config: !`test -f ${CLAUDE_PLUGIN_ROOT}/config.json && echo \"✓\" || echo \"✗\"`\n\nIf all checks pass, run analysis.\nOtherwise, report missing components.\n```\n\n### Error Handling\n\n```markdown\n---\ndescription: Build with error handling\nallowed-tools: Bash(*)\n---\n\nExecute build: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh 2>&1 || echo \"BUILD_FAILED\"`\n\nIf build succeeded:\n  Report success and output location\nIf build failed:\n  Analyze error output\n  Suggest likely causes\n  Provide troubleshooting steps\n```\n\n**Best practices:**\n- Validate early in command\n- Provide helpful error messages\n- Suggest corrective actions\n- Handle edge cases gracefully\n\n---\n\nFor detailed frontmatter field specifications, see `references/frontmatter-reference.md`.\nFor plugin-specific features and patterns, see `references/plugin-features-reference.md`.\nFor command pattern examples, see `examples/` directory.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Command Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Command Development",
          "skill_dir": "command-development",
          "skill_file_tree": "SKILL.md\nexamples/plugin-commands.md\nexamples/simple-commands.md\nreferences/advanced-workflows.md\nreferences/documentation-patterns.md\nreferences/frontmatter-reference.md\nreferences/interactive-commands.md\nreferences/marketplace-considerations.md\nreferences/plugin-features-reference.md\nreferences/testing-strategies.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n### 1. What domain knowledge does this skill teach?\nThe skill teaches the workflow decision tree for working with .docx files: when to use tracked changes (redlining) vs direct editing, when to use pandoc vs raw XML access, and when to use docx-js (creation) vs the Document library (editing). It also teaches the principle of \"minimal, precise edits\" for tracked changes.\n\n### 2. What workflow decisions does it enable?\n- **Reading content**: Use pandoc for text extraction; use raw XML for comments, metadata, or complex formatting\n- **Creating new documents**: Use docx-js (JavaScript)\n- **Editing existing documents**: Use redlining workflow for documents from others or professional contexts (legal, academic, business); use basic OOXML editing only for your own documents with simple changes\n\n### 3. What would a practitioner struggle with without this skill?\nA practitioner would struggle to know when tracked changes (redlining) are required vs optional. The skill explicitly states that for legal, academic, business, or government documents, the redlining workflow is **required**, not optional. This is critical professional knowledge - directly editing a legal contract without tracked changes could have serious consequences.\n\n```yaml\nname: docx-editing-workflow-selection\nskills:\n  - docx\nprompt: |\n  I'm a paralegal at a law firm. My supervising attorney just sent me a contract draft from opposing counsel (the other party's lawyers). She wants me to propose several changes to the terms - things like adjusting payment deadlines and modifying the termination clause.\n  \n  What approach should I use to make these edits to the .docx file? Should I just directly edit the text, or is there a specific workflow I should follow? Please explain why.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should recommend using tracked changes (also known as \"redlining\") for this scenario. This is required because:\n    1. The document comes from someone else (opposing counsel)\n    2. It's a legal document (contract)\n    \n    Score 1.0 if: The response clearly recommends using tracked changes/redlining AND explains that this is appropriate because it's either (a) a document from someone else, or (b) a legal/professional document, or (c) both. The response should convey that direct editing without tracking would be inappropriate.\n    \n    Score 0.5 if: The response mentions tracked changes as an option but doesn't clearly recommend it as the required/preferred approach, OR recommends tracked changes but doesn't explain why it's important for this scenario.\n    \n    Score 0.0 if: The response recommends directly editing the document without tracked changes, OR doesn't mention tracked changes at all, OR gives generic advice without addressing the specific professional context.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about when to use tracked changes in professional document editing contexts, which is a genuine workflow decision practitioners face. The grader is robust and accepts multiple valid explanations. However, the test has minor realism issues: while the scenario is plausible, a paralegal would typically already know to use tracked changes for legal documents from opposing counsel, making this slightly more of a 'teaching moment' than a realistic task they'd ask an AI assistant about. The test would be stronger if it presented a more ambiguous scenario where the workflow decision is less obvious."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what it teaches:\n\n## Analysis\n\n**Domain Knowledge:** The skill teaches OOXML (XML-based) document structure and when to use different workflows (Pandoc for text extraction, docx-js for creation, Document library for editing, and redlining for reviews).\n\n**Workflow Decisions:** The critical decisions practitioners face:\n- When to use Pandoc vs. raw XML access vs. Document library\n- When redlining is mandatory (legal/business/academic docs) vs. direct editing\n- How to implement minimal, precise tracked changes vs. careless whole-section replacements\n- Batching strategy for managing complex edits\n- RSID preservation for maintaining professional appearance\n\n**Without this skill, practitioners struggle with:**\n- Choosing wrong tools for tasks (using Pandoc for comments, not knowing about Document library)\n- Not recognizing when tracked changes are mandatory\n- Creating unprofessional redlines (replacing entire sentences instead of only changed words)\n- Not understanding why minimal edits matter or how to preserve RSIDs\n- Failing to batch changes effectively, making debugging impossible\n\nThe strongest test targets the **redlining workflow** decision and implementation principles, as it requires understanding both the \"when/why\" and the \"how\" of professional document editing.\n\n```yaml\nname: document-redline-workflow-decisions\nskills:\n  - docx\nprompt: |\n  You receive a contract from an external law firm that needs several edits. The contract has 12 proposed modifications across 5 different sections. You need to implement tracked changes (redlines) so the firm can review your edits.\n\n  One proposed change is in Section 3, which currently reads:\n  \"The total contract value is $500,000 payable over 24 months.\"\n  \n  This needs to become:\n  \"The total contract value is $750,000 payable over 36 months.\"\n\n  Explain your approach to implementing this single change and similar edits in the document. Specifically address:\n  \n  1. Why is redlining (tracked changes) the right workflow for this scenario?\n  2. For the sentence above, describe how you would implement the changes - show the conceptual approach (not code) for marking only what changed vs. what stayed the same\n  3. How would you organize the 12 modifications across 5 sections for implementation?\n  4. Why is the \"minimal edits\" approach (only marking changed words) better than replacing entire sentences?\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of the docx redlining workflow and professional tracked changes principles.\n\n    Score 1.0 if the response:\n    - Correctly identifies that redlining is mandatory for external document review (legal/business context)\n    - Explains the minimal edits principle: only \"500,000\" → \"750,000\" and \"24\" → \"36\" should be marked as changes, NOT the entire sentence\n    - Describes preserving unchanged text/RSIDs around the deletions and insertions\n    - Proposes a batching strategy for the 12 changes (e.g., by section, by type, or by complexity)\n    - Explains WHY minimal edits are professional and improve document reviewability\n    - Demonstrates understanding that this approach requires unpacking the document, finding exact XML locations, and implementing changes systematically\n\n    Score 0.5 if the response:\n    - Correctly identifies redlining as important but doesn't explain why it's mandatory for this scenario\n    - Shows partial understanding of minimal edits (mentions not replacing entire sentences) but doesn't explain RSID preservation\n    - Mentions batching but doesn't provide concrete organization strategy\n    - Explains the principles but with some gaps in understanding preserved text or workflow steps\n\n    Score 0.0 if the response:\n    - Doesn't recognize redlining as the appropriate workflow\n    - Suggests replacing entire sentences with tracked changes (showing ignorance of minimal edits principle)\n    - Shows no understanding of batching strategy or document structure\n    - Provides generic advice without demonstrating knowledge of OOXML or professional tracked changes implementation\n    - Suggests using a tool incorrectly (e.g., using Pandoc for editing instead of text extraction)\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It effectively tests the redlining workflow decision-making (when/why to use tracked changes for external documents), the minimal edits principle (only marking changed text vs. entire sentences), RSID preservation, and batching strategy - all transferable domain knowledge rather than trivia. The grading rubric is robust and accepts multiple valid approaches while clearly distinguishing between full, partial, and no understanding. The scenario is highly realistic (contract review with external parties). Minor deduction: the test is somewhat verbose and could be slightly more concise, and the grader could more explicitly accept alternative batching strategies (by proximity, by change type, etc.) as equally valid, though it does mention 'by section, by type, or by complexity' which covers most approaches."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: redline-legal-docx-minimal-tracked-change-workflow\nskills:\n- docx\nprompt: |\n  You are asked to edit a client's 12-page contract.docx (a legal document you did not author). The client requires:\n  1) The change must appear as tracked changes (redlines), not as “clean” edits.\n  2) The edit must be minimal and professional: only the specific changed text should be inserted/deleted; unchanged text should remain unchanged so the redline is easy to review.\n  3) You must preserve formatting.\n\n  Specific requested edits:\n  - In the sentence: \"Payment is due within 30 days of receipt of invoice.\", change \"30\" to \"60\" using tracked changes.\n  - Add a reviewer comment on the paragraph that starts with: \"Late payments will accrue interest...\" saying: \"Confirm the interest rate matches Exhibit B.\"\n\n  Task: Describe the end-to-end workflow you would follow (tools/commands and key files), including how you would:\n  - extract the document to review it while preserving tracked changes\n  - decide what workflow to use given this is a legal document from someone else\n  - locate the right XML to edit\n  - implement the “30” -> “60” edit as minimal tracked changes in OOXML (include a representative OOXML snippet showing the minimal <w:del> + <w:ins> approach with surrounding unchanged runs)\n  - implement the comment at a high level (which XML parts are involved and how the comment is linked to the text)\n  - verify all changes and repackage the .docx\n\n  Do not write a full program; focus on the correct workflow decisions and the critical technical details that prevent broken/corrupted docx output.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates the specialized DOCX/OOXML + redlining workflow knowledge needed for professional tracked-change editing.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Correct workflow decision: explicitly recommends/uses a redlining / tracked-changes workflow because it is a legal doc and/or someone else’s document (not “just edit it normally”).\n    - Uses pandoc (or equivalent) to extract/review content AND mentions preserving tracked changes via an option like --track-changes=all (accept/reject/all acceptable as long as it’s purposeful and explained).\n    - Explains unpack/edit/pack loop: identifies that .docx is a zip of XML; names word/document.xml as the main content file; mentions unpacking then repacking the docx.\n    - Describes how to map the target text to XML (e.g., grep/search for unique surrounding text; notes that text may be split across runs).\n    - Provides a representative OOXML snippet for the “30” -> “60” change that is MINIMAL and shows:\n      * unchanged surrounding text kept as normal <w:r>/<w:t>\n      * a deletion element <w:del> wrapping the deleted text (using <w:delText> or equivalent)\n      * an insertion element <w:ins> wrapping the inserted text (using <w:t>)\n      * does NOT replace the whole sentence as one big deletion/insertion\n    - Mentions at least one tracked-change correctness constraint (any of):\n      * <w:ins> and <w:del> tag pairing correctness (don’t mix close tags)\n      * using author attribution (e.g., w:author)\n      * RSID constraints (e.g., 8-digit hex) or preserving RSIDs for unchanged runs\n      * enabling track revisions in settings.xml (trackRevisions) as part of making tracked changes show up\n    - Comment implementation described at a high level with correct file/structure awareness:\n      * references word/comments.xml AND that document.xml contains references/anchors linking a range to a comment (exact tag names not required, but must indicate both the comment store and in-document linkage).\n    - Verification step: re-convert to markdown with tracked changes and/or grep old/new phrases to confirm; checks no unintended changes.\n\n    Score 0.5 if the answer shows partial but significant understanding, for example:\n    - Chooses tracked changes and mentions unpack/edit/pack, but the OOXML snippet is too coarse (replaces whole sentence) OR omits key tracked-change constraints (author/RSID/tag correctness) OR comment handling is too vague (e.g., “add a comment” with no mention of comments.xml/references).\n    - Uses pandoc without mentioning tracked-change preservation, or vice versa.\n\n    Score 0.0 if the answer is generic and misses the core domain workflow, e.g.:\n    - Suggests editing in Word UI only with no OOXML/redlining details, OR\n    - Does not use tracked changes / does not understand <w:ins>/<w:del> minimal edits, OR\n    - Provides no plausible end-to-end approach for preserving formatting and producing a valid updated .docx.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case demonstrates strong understanding of the skill domain by testing the redlining workflow for tracked changes in legal documents - a genuinely transferable pattern that applies across professional document editing contexts. The grader is comprehensive and robust, accepting multiple valid approaches while requiring demonstration of core concepts (minimal edits, OOXML structure, comment implementation). The task is highly realistic, representing actual legal/professional document review work. Minor deductions: (1) The grader is somewhat verbose and could be more concise while maintaining rigor, and (2) the test requires fairly detailed technical knowledge that borders on implementation specifics rather than pure conceptual understanding, though this is justified given the complexity of OOXML manipulation."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis:\n# 1) Domain knowledge taught:\n#    - DOCX files are ZIP archives containing OOXML parts; meaningful edits require understanding of\n#      `word/document.xml`, `word/comments.xml`, and (often) `word/settings.xml`.\n#    - Tracked changes in Word are represented with <w:ins>/<w:del> wrappers around runs, using\n#      <w:delText> for deletions; correct nesting/closure matters for Word compatibility.\n#    - Comments are implemented by linking range markers in document.xml (commentRangeStart/End +\n#      commentReference) to a <w:comment> entry in comments.xml via a shared id.\n#    - Whitespace and run boundaries (e.g., xml:space=\"preserve\") must be preserved to avoid\n#      subtle text corruption.\n# 2) Workflow decisions enabled:\n#    - For client/legal/professional docs, prefer a redlining workflow (plan in an extracted view,\n#      then implement tracked changes) instead of direct rewrite.\n#    - Use text extraction for review/planning; use raw XML for comments/tracked changes.\n#    - Batch changes and verify after repacking to avoid hard-to-debug corruption.\n# 3) What practitioners struggle with without this skill (test target):\n#    - Models often rewrite whole paragraphs (bad redlines), omit proper tracked-change markup,\n#      break whitespace, or fail to correctly link comments across XML parts.\n#    - They also often omit a safe unpack/edit/repack/verify loop and batching strategy.\nname: docx-tracked-changes-and-comments-ooxml\nskills:\n  - docx\nprompt: |\n  You are reviewing a client-provided contract.docx. You cannot use Microsoft Word UI; you must implement the edits in OOXML so the client can review them as *tracked changes* and *comments*.\n\n  Below is the current `word/document.xml` fragment for clause 2.1 (namespaces omitted for readability). The sentence currently reads: “The term is 30 days after notice.”\n\n  ```xml\n  <w:p>\n    <w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r>\n    <w:r w:rsidR=\"00AB12CD\"><w:t>30</w:t></w:r>\n    <w:r w:rsidR=\"00AB12CD\"><w:t xml:space=\"preserve\"> days after notice.</w:t></w:r>\n  </w:p>\n  ```\n\n  Tasks:\n\n  1) Tracked change: Change **only** “30” to “60” using tracked changes, with *minimal, precise edits*. Provide the updated XML for this paragraph.\n     - Keep the unchanged text runs intact (including their existing `w:rsidR` values).\n     - Use `<w:del>` + `<w:ins>` in a way Word will recognize.\n     - Include `w:author=\"ReviewBot\"` and a `w:date=\"...\"` attribute on both the deletion and insertion (any valid ISO-like datetime is fine).\n     - Preserve whitespace correctly (do not accidentally remove/add spaces).\n\n  2) Comment: Add a reviewer comment on the phrase **“after notice”** saying: “Define what constitutes notice (email vs certified mail).”\n     - Describe what you need to add to `document.xml` (which markers/runs and where they go) and what new entry you need in `word/comments.xml`, including how the IDs link up.\n     - You do not need to write the entire files—just the relevant snippets/pattern.\n\n  3) Workflow: Briefly outline a safe end-to-end workflow you would use to apply these edits across a whole legal document while minimizing risk (how you’d inspect text, locate the right XML, batch changes, and verify the final .docx).\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical OOXML/Docx editing competence for tracked changes + comments, and a safe workflow.\n\n    Score 1.0 if ALL of the following are true:\n    - Tracked-change XML correctness & minimality:\n      - Produces an updated paragraph snippet where only the “30” token is marked as deleted and only “60” is inserted (does NOT wrap/delete the whole sentence/paragraph).\n      - Uses <w:del> containing <w:r><w:delText>30</w:delText></w:r> (or equivalent valid delText usage) and a sibling <w:ins> containing <w:r><w:t>60</w:t></w:r>.\n      - <w:del> and <w:ins> are not incorrectly nested inside <w:r>; tags are properly closed (no </w:ins> for a <w:del>, etc.).\n      - Includes w:author=\"ReviewBot\" and some w:date=\"...\" on BOTH the <w:del> and <w:ins>.\n      - Preserves the unchanged runs exactly (keeps the original <w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r> and the trailing run with xml:space=\"preserve\"), maintaining correct spacing.\n    - Comment knowledge:\n      - Explains/illustrates the need for w:commentRangeStart and w:commentRangeEnd around the target text, plus a w:commentReference run, all sharing a consistent w:id.\n      - Describes adding a corresponding <w:comment ... w:id=\"...\"> entry in word/comments.xml containing the comment text and author/date metadata (exact schema details can vary).\n      - Mentions (explicitly or implicitly) that the text may need to be split into separate runs to bracket “after notice” cleanly.\n    - Workflow:\n      - Proposes an end-to-end workflow that includes: getting a human-reviewable view (e.g., docx->markdown/text extraction), locating the change in document.xml (e.g., search/grep by surrounding text), applying edits in small batches, repacking the docx, and verifying results (e.g., reconvert/check that old phrase is gone/new phrase present).\n      - Indicates that raw XML access is required for comments/tracked changes.\n\n    Score 0.5 if:\n    - The answer generally uses tracked changes and/or comments but misses one important requirement (e.g., over-broad deletion, missing author/date, unclear comment linking), OR the workflow is present but lacks batching/verification.\n    - Minor OOXML inaccuracies that wouldn't fundamentally negate the approach are present, but the intent and main structures (<w:ins>/<w:del>, commentRangeStart/End + comments.xml entry) are correct.\n\n    Score 0.0 if:\n    - The answer does not use tracked-change constructs (<w:ins>/<w:del>) and instead suggests plain text editing, OR provides clearly invalid/mismatched OOXML structures.\n    - Fails to explain how comments are represented/linked in OOXML (no range markers and no comments.xml entry), AND provides no credible safe workflow.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of DOCX/OOXML domain knowledge by testing tracked changes, comments, and safe workflows - all transferable skills practitioners need. The grader is robust and accepts valid OOXML approaches while checking for key competencies (minimal edits, proper nesting, comment linking). However, it loses points on realism: while the individual components (tracked changes, comments) are realistic, the specific task of manually editing raw OOXML is somewhat contrived - practitioners would typically use Word's UI or higher-level libraries rather than hand-crafting XML snippets. The test is still valuable for assessing deep OOXML understanding, but the scenario of providing exact XML fragments is more academic than practical."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "Analysis:\n1. Core domain knowledge: Managing professional .docx documents via OOXML—extracting text via pandoc, unpacking/packing docx archives, manipulating XML with the Document library, handling tracked changes/comment structures, and creating documents via docx-js. Emphasis on preserving formatting, RSIDs, and minimal redlines.\n2. Workflow decisions: Selecting text extraction vs raw XML inspection; choosing docx-js for new docs vs Document library for edits; defaulting to redlining workflow for third-party/legal docs; batching related tracked changes and verifying via pandoc.\n3. Practitioner struggles: Without the skill, they wouldn’t know to use pandoc for review, how to batch edits, when to read ooxml.md, how to manage RSIDs or minimal-change tracked edits, or the pack/unpack process—leading to improper redlines and broken documents.\n\n```yaml\nname: redline-legal-docx-workflow-plan\nskills:\n  - docx\nprompt: |\n  A client sent you a vendor-supplied services contract in .docx format. You must add tracked changes and one inline comment to adjust Section 4’s liability cap, clarify the data retention clause in Section 7, and replace the outdated signature block. You are not to make the edits yet—your task is to outline the exact end-to-end workflow you will follow so the client can review your plan before you touch the file.\n\n  In your workflow description, explain:\n  - How you will review the current document text (while preserving tracked changes) and organize the needed edits.\n  - Which documentation you must read before editing and why.\n  - How you will unpack, edit, and repack the file, including the tools/libraries involved.\n  - How you will structure batches of changes and ensure minimal, precise tracked edits with correct RSIDs.\n  - How you will add the inline comment.\n  - How you will verify that all requested changes (and only those changes) are present at the end.\n\n  Provide a detailed, step-by-step plan referencing specific commands, files, or library actions where relevant. Do not invent final redline text; focus solely on the workflow.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response:\n    - Explicitly chooses the redlining workflow for this third-party legal document.\n    - Mentions converting the docx to markdown with `pandoc --track-changes=all` (or equivalent) to review existing text/tracked changes.\n    - Notes the requirement to read the entire ooxml.md (Document library) file before editing.\n    - Describes unpacking with the provided script, editing via the Python Document library (including batching 3-10 related edits, mapping text to XML, preserving RSIDs, and making minimal precise tracked edits), and repacking afterward.\n    - Explains how an inline comment will be inserted (e.g., via comments.xml or Document library support).\n    - Details final verification steps (pandoc conversion, grepping/inspection to confirm only intended changes).\n    \n    Score 0.5 if the plan references some but not all critical steps (e.g., mentions redlining and pandoc but omits RSID handling or final verification), or conflates workflows yet still demonstrates partial understanding.\n    \n    Score 0.0 if the response fails to outline the redlining workflow, omits key tools (pandoc, Document library, unpack/pack), or gives only generic editing advice unrelated to the skill.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the docx skill's core workflows and tests genuine practitioner knowledge. It requires understanding the redlining workflow, pandoc conversion, Document library usage, RSID handling, and verification steps—all transferable domain knowledge rather than trivia. The grader is mostly robust, accepting responses that demonstrate the workflow even if they don't use exact terminology. However, there are minor clarity concerns: the grader could be slightly brittle around exact tool names (e.g., requiring 'pandoc --track-changes=all' specifically rather than accepting equivalent approaches), and the task is somewhat contrived (asking for a workflow plan rather than execution). The realism is good but not perfect—practitioners would typically just do the edits rather than write a detailed workflow plan first. Overall, this is a high-quality test that effectively measures skill knowledge with only minor weaknesses."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "Analysis:\n\n1.  **Domain Knowledge**: The skill teaches the internal structure of Office Open XML (OOXML), specifically how to handle tracked changes (`<w:ins>` and `<w:del>`). It emphasizes that a \"professional\" edit involves splitting text runs (`<w:r>`) so that unchanged text remains unmarked, while only the specific changed characters are wrapped in deletion/insertion tags.\n2.  **Workflow Decisions**: The skill explicitly differentiates between \"Basic OOXML editing\" (for simple changes) and the \"Redlining workflow\" (for legal/formal docs). The Redlining workflow requires manual or scripted manipulation of the XML tree to ensure precise change tracking, rather than just replacing text which would look like a bulk delete/insert.\n3.  **Practitioner Struggle**: A common failure mode for LLMs editing XML/HTML is to replace the entire parent node (e.g., the whole sentence) rather than surgically editing the child nodes. In a legal document context, deleting a whole sentence just to change a date is unacceptable because it makes the redline hard to read. This test captures that specific nuance.\n\n```yaml\nname: docx-redlining-xml-structure\nskills:\n- docx\nprompt: |\n  I am writing a script to programmatically edit a Word document's `word/document.xml` file to implement tracked changes.\n\n  Current XML fragment:\n  ```xml\n  <w:p>\n    <w:r>\n      <w:t>Term: 12 months.</w:t>\n    </w:r>\n  </w:p>\n  ```\n\n  I need to change \"12\" to \"24\" so it appears as a tracked change (redline) to the user.\n  Provide the correct OOXML string for this paragraph. Follow the \"Minimal, Precise Edits\" principle to ensure professional output.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Analyze the XML output.\n\n    The user asked to change \"12\" to \"24\" using tracked changes in OOXML.\n    \n    Correct behavior defined in the skill:\n    1.  **Split the run**: The text \"Term: \" and \" months.\" should be separated from the changed number.\n    2.  **Deletion**: \"12\" should be inside `<w:del>...<w:delText>12</w:delText>...</w:del>`.\n    3.  **Insertion**: \"24\" should be inside `<w:ins>...<w:t>24</w:t>...</w:ins>`.\n    4.  **Minimalism**: The surrounding text (\"Term: \" and \" months.\") must NOT be marked as deleted/inserted. It should exist in `<w:r>` tags outside the del/ins tags.\n\n    Score 1.0 if:\n    - The XML structure correctly isolates the change (does not delete the whole sentence).\n    - Uses `<w:del>` and `<w:ins>` correctly.\n    - Follows the structure: [Unchanged Run] [Del] [Ins] [Unchanged Run].\n\n    Score 0.5 if:\n    - The specific XML tags might be slightly malformed (e.g. missing inner `<w:r>` inside the del/ins), but the *concept* of splitting the run and only marking \"12\" and \"24\" is demonstrated.\n    - OR if it marks \"12 months\" and inserts \"24 months\" (slightly less minimal but valid).\n\n    Score 0.0 if:\n    - It deletes the entire sentence \"Term: 12 months.\" and inserts \"Term: 24 months.\".\n    - It just changes the text to \"24\" without tracked change tags.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This is a strong test case that effectively measures understanding of OOXML tracked changes. It tests genuine domain knowledge (minimal edits principle, proper XML structure for redlining) rather than trivia, and represents a realistic task practitioners face when programmatically editing legal documents. The grader is mostly robust, accepting variations while penalizing incorrect approaches. Minor deduction: the grader could be slightly more explicit about accepting valid alternative XML structures (e.g., different RSID handling, attribute ordering), and the 0.5 case for 'marking 12 months and inserting 24 months' is somewhat lenient given the skill's emphasis on minimalism. Overall, this effectively distinguishes between models that understand professional document editing practices versus those that would naively replace entire text blocks."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "1. **What domain knowledge does this skill teach?**\nThe skill focuses on the internal structure of Office Open XML (OOXML), specifically for Word (.docx) files. It teaches the hierarchical relationship between paragraphs (`<w:p>`), runs (`<w:r>`), and text (`<w:t>`). Crucially, it covers the specialized XML syntax for \"redlining\" (tracked changes) using `<w:ins>` and `<w:del>` tags, and the importance of RSIDs (Revision Save IDs) for maintaining document history.\n\n2. **What workflow decisions does it enable?**\nIt guides the model to choose between high-level JS libraries (`docx-js`) for new documents and low-level Python/XML manipulation for editing existing ones. It emphasizes a \"Redlining workflow\" for professional contexts (legal/business), which involves converting to markdown for analysis but performing XML-level edits for the final product. It also defines a \"Minimal, Precise Edits\" principle to ensure professional-looking tracked changes.\n\n3. **What would a practitioner actually struggle with without this skill?**\nA practitioner (or an LLM without this skill) would likely replace entire paragraphs or sentences when making a small change, which \"blows away\" the original formatting history and makes the redlining view in Word incredibly messy for human reviewers. They might also place `<w:ins>` tags inside `<w:r>` tags (which is a common mistake that can lead to schema errors or rendering issues) or ignore RSIDs, leading to fragmented document metadata.\n\n```yaml\nname: docx-redlining-precision\nskills:\n- docx\nprompt: |\n  I am using the `docx` skill tools to edit a legal agreement. I have unpacked the document and am looking at the following XML snippet in `word/document.xml`:\n\n  ```xml\n  <w:p w:rsidR=\"00A1B2C3\">\n    <w:r w:rsidR=\"00A1B2C3\">\n      <w:t xml:space=\"preserve\">The termination notice period shall be </w:t>\n    </w:r>\n    <w:r w:rsidR=\"00A1B2C3\">\n      <w:t>30</w:t>\n    </w:r>\n    <w:r w:rsidR=\"00A1B2C3\">\n      <w:t xml:space=\"preserve\"> days from the date of breach.</w:t>\n    </w:r>\n  </w:p>\n  ```\n\n  I need to change \"30\" to \"90\". Following the \"Redlining workflow\" and the \"Minimal, Precise Edits\" principle, provide the updated XML for this paragraph. \n  \n  Requirements:\n  1. Use tracked changes (redlining) so the user can see the deletion and insertion.\n  2. Use the RSID `00FEDCBA` for the new changes.\n  3. Ensure the XML follows the specific placement rules for `<w:ins>` and `<w:del>` as described in the `docx` skill (specifically `ooxml.md`).\n  4. Only mark the text that actually changes.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The goal is to evaluate if the model correctly applied the \"Minimal, Precise Edits\" principle and the specific technical constraints for redlining found in the skill documentation.\n\n    Correct XML structure should look like this (order of ins/del may vary, but placement is key):\n    1. The unchanged text (\"The termination notice period shall be \") stays in its original `<w:r>`.\n    2. The deletion `<w:del>` wraps the old run containing \"30\". \n    3. The insertion `<w:ins>` wraps a new run containing \"90\".\n    4. The `<w:ins>` and `<w:del>` tags MUST be outside the `<w:r>` tags (as specified in the ooxml.md guidelines).\n    5. The unchanged trailing text (\" days from the date of breach.\") remains in its original `<w:r>`.\n    6. The new changes use the RSID `00FEDCBA`.\n\n    Score 1.0 if:\n    - The model preserves the surrounding unchanged text in their original `<w:r>` elements.\n    - The `<w:del>` and `<w:ins>` tags are correctly placed OUTSIDE the `<w:r>` tags (e.g., `<w:del><w:r>...</w:r></w:del>`).\n    - Only the \"30\" and \"90\" are affected by the tracked changes.\n    - The RSID `00FEDCBA` is applied to the new elements.\n\n    Score 0.5 if:\n    - The model correctly uses `<w:ins>` and `<w:del>` but replaces more text than necessary (e.g., replaces the whole sentence).\n    - OR the model places `<w:ins>`/`<w:del>` inside the `<w:r>` tag (violating the skill's technical guidelines).\n    - OR the RSID is missing/incorrect.\n\n    Score 0.0 if:\n    - The model does not use tracked change tags (`<w:ins>`/`<w:del>`) at all.\n    - The XML is malformed.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates understanding of the 'Minimal, Precise Edits' principle and OOXML tracked changes structure, which are core domain concepts that transfer across document editing contexts. The grader is robust and accepts valid solutions while checking for proper XML structure. However, the test loses some points on realism - while redlining is a real task, providing a pre-extracted XML snippet and asking for the exact replacement is somewhat contrived compared to a more natural workflow like 'edit this document to change the notice period from 30 to 90 days.' The test also borders on testing specific technical details (exact tag placement rules) rather than broader understanding, though these details do represent genuine gotchas practitioners encounter."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nLet me first analyze what this skill document teaches:\n\n**1. Domain knowledge:**\n- Office Open XML (OOXML) structure and document architecture\n- Professional document review workflows and tracked changes (\"redlining\")\n- The principle of minimal, precise edits in tracked changes\n- When to preserve original formatting vs making changes\n- Document workflow decision-making (reading vs editing, creating vs modifying)\n\n**2. Workflow decisions:**\n- **Reading vs Editing**: When to use text extraction (pandoc) vs raw XML access\n- **Creating vs Editing existing**: When to use docx-js vs OOXML manipulation\n- **Simple edits vs Redlining**: Most critically - when to use tracked changes (someone else's document, legal/academic/business/government docs) vs direct edits\n- **Batch organization**: How to group changes for efficiency and debugging\n\n**3. What practitioners struggle with:**\n- **Knowing when tracked changes are required** - This is a professional judgment call\n- **Minimal edits principle** - Junior reviewers often replace entire sentences instead of just marking what changed, which is unprofessional and hard to review\n- **Choosing appropriate workflows** - Using the right tool for the right task\n\nThe most valuable real-world knowledge is understanding **when to use tracked changes and how to apply the minimal edits principle** - this separates professional document reviewers from novices.\n\n## Test Case\n\n```yaml\nname: document-review-workflow-decision\nskills:\n  - docx\nprompt: |\n  You've been asked to help with two document editing tasks:\n  \n  Task A: You created a draft contract last week for your own company. Your manager reviewed it and asked you to make three simple changes: (1) change the payment term from \"30 days\" to \"45 days\", (2) update the effective date to \"January 15, 2026\", and (3) fix a typo where \"recieve\" should be \"receive\". These are straightforward corrections to your own document.\n  \n  Task B: A law firm sent you their client's employment agreement for technical review. They want you to propose changes to the non-compete clause and update some technical terminology. You'll be sending your edits back to the law firm, who will review them with their client.\n  \n  For each task, explain:\n  1. What workflow approach you would use to make the edits\n  2. Why that approach is appropriate for this situation\n  3. What principle(s) guide your decision\n  \n  Be specific about whether you would use direct edits or tracked changes, and explain your reasoning.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of professional document review workflows and when to use tracked changes.\n    \n    Score 1.0 if the response:\n    - Correctly identifies that Task A can use direct/simple edits (since it's your own document with simple changes from your manager)\n    - Correctly identifies that Task B MUST use tracked changes/redlining (since it's someone else's document, from a law firm, going back for review)\n    - Explains the reasoning: tracked changes are for formal review scenarios, collaborative editing, legal/professional documents, or when others need to see what changed\n    - Shows understanding that the approach differs based on document ownership, formality, and review workflow\n    \n    Score 0.5 if the response:\n    - Gets one task correct but misses the other, OR\n    - Identifies both correctly but provides weak or incomplete reasoning, OR\n    - Shows partial understanding of tracked changes but doesn't clearly distinguish when they're required vs optional\n    \n    Score 0.0 if the response:\n    - Recommends the same approach for both tasks (missing the key distinction), OR\n    - Suggests tracked changes are never necessary or always necessary regardless of context, OR\n    - Focuses only on technical implementation details without addressing the workflow decision, OR\n    - Shows no understanding of when formal review processes are appropriate\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of professional document workflows and tests transferable knowledge about when to use tracked changes vs. direct edits. However, it has clarity issues: the grading rubric doesn't account for valid alternative interpretations (e.g., some practitioners might reasonably use tracked changes even for Task A to maintain an audit trail for manager review, especially in regulated industries). The test also conflates 'your own document' with 'simple changes from manager' - these are actually different considerations. The core concept being tested (formal review workflows requiring tracked changes) is valuable and realistic, but the grader is somewhat brittle in rejecting nuanced professional judgment calls that could be equally valid."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this skill document, I need to analyze what core knowledge it teaches:\n\n## Analysis\n\n1. **What domain knowledge does this skill teach?**\n   - Understanding that .docx files are ZIP archives containing XML files\n   - Different workflows for document tasks: reading/analyzing, creating new, editing existing, and reviewing with tracked changes\n   - The concept of redlining (tracked changes) for professional document review\n   - Best practices for implementing tracked changes - marking only what changes, not entire sentences\n   - Batching strategy for managing complex document edits\n\n2. **What workflow decisions does it enable?**\n   - When to use redlining workflow (required for legal/academic/business/government docs)\n   - When to use simple editing vs tracked changes\n   - How to batch changes effectively (3-10 related changes per batch)\n   - Whether to use text extraction, raw XML access, or document creation tools\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Not knowing that tracked changes are required for professional document review\n   - Implementing tracked changes incorrectly (marking entire sentences instead of just changes)\n   - Not understanding the importance of batching changes for manageable debugging\n   - Not knowing how to preserve formatting and RSIDs when editing\n\n## Test Case\n\n```yaml\nname: docx-professional-review-workflow\nskills:\n- docx\nprompt: |\n  You're reviewing a legal services agreement for a client. The document needs several changes:\n  \n  1. The payment terms need to change from \"30 days\" to \"45 days\" in multiple places\n  2. The jurisdiction clause needs to change from \"State of New York\" to \"State of Delaware\"\n  3. A new termination clause needs to be added after the existing payment section\n  4. The effective date needs to be updated from \"January 1, 2024\" to \"March 15, 2024\"\n  \n  The client's legal team requires that all changes be clearly tracked so they can review each modification individually. They've emphasized that the document review must follow professional standards for legal document editing.\n  \n  Explain your approach for making these edits, including:\n  - What workflow you would use and why\n  - How you would organize and implement the changes\n  - Any important considerations for maintaining document integrity\n  \n  Focus on the methodology and best practices rather than specific code implementation.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of professional document review workflows and tracked changes best practices.\n    \n    Score 1.0 if ALL of the following are addressed:\n    - Correctly identifies that the redlining/tracked changes workflow is required for legal documents\n    - Explains the importance of marking only the specific text that changes (not entire sentences or paragraphs)\n    - Describes organizing changes into logical batches (3-10 changes per batch) for manageable implementation\n    - Mentions the need to preserve document formatting and structure during edits\n    - Shows understanding that tracked changes allow reviewers to see and accept/reject each modification individually\n    \n    Score 0.5 if the response:\n    - Recognizes that tracked changes are important for legal document review\n    - Mentions some organization of changes but lacks detail on batching strategy\n    - Shows partial understanding of professional review requirements but misses key best practices\n    \n    Score 0.0 if the response:\n    - Suggests making direct edits without tracked changes\n    - Does not recognize the importance of tracking changes for legal documents\n    - Proposes replacing the entire document or sections wholesale\n    - Shows no understanding of professional document review workflows\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of professional document workflows and tests transferable knowledge about tracked changes and batching strategies. However, it has clarity issues: the grader requires explaining 'methodology and best practices' but then scores based on specific technical details (batching 3-10 changes, preserving RSIDs) that may not be mentioned in a high-level methodology explanation. The prompt asks to 'focus on methodology rather than specific code implementation' but the grader penalizes for not mentioning technical details like 'preserving document formatting and structure.' This creates ambiguity about what level of detail is expected. The test is realistic and measures genuine domain knowledge, but the grader could reject valid high-level responses that correctly identify the redlining workflow without diving into implementation specifics."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: redlining-minimal-tracked-changes-and-comment-ooxml\nskills:\n  - docx\nprompt: |\n  You are asked to review and revise a vendor’s *legal contract* provided as a .docx. The client requires that **all edits be delivered as Word tracked changes**, and they also want **one reviewer comment** inserted.\n\n  You do NOT have access to Word’s UI. You must describe a safe, professional workflow and show one concrete OOXML edit.\n\n  Required edits (5 total):\n  1) Change “within 30 days” → “within sixty (60) days”\n  2) Change party name “Acme LLC” → “Acme, Inc.”\n  3) Insert a new sentence after the payment clause: “Time is of the essence.”\n  4) Delete only the word “reasonable” from the phrase “commercially reasonable efforts”\n  5) Add a reviewer comment attached to the word “receipt” that says: “Define ‘receipt’ (email? invoice date? physical delivery?).”\n\n  Below is the current OOXML for the payment sentence in `word/document.xml` (this is exactly how the text is split into runs):\n\n  ```xml\n  <w:p>\n    <w:r w:rsidR=\"00F00F00\"><w:t>Payment is due within </w:t></w:r>\n    <w:r w:rsidR=\"00F00F00\"><w:t>30</w:t></w:r>\n    <w:r w:rsidR=\"00F00F00\"><w:t xml:space=\"preserve\"> days of receipt.</w:t></w:r>\n  </w:p>\n  ```\n\n  The unpacking step in your environment suggests using RSID `00AB12CD` for any newly created tracked-change runs.\n\n  Deliverables:\n  A) A step-by-step workflow you would follow to implement *all five* edits as tracked changes (include how you’d plan/verify via markdown conversion, and how you’d batch work).\n  B) A corrected OOXML snippet (edited version of the paragraph above) that changes only the “30” → “sixty (60)” using tracked changes, while preserving unchanged text outside the tracked-change elements.\n  C) A brief outline of which XML parts must change to add the requested reviewer comment on “receipt” (you don’t need to write the entire comments.xml, but you must describe the key elements and files involved).\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical DOCX/OOXML redlining competence (tracked changes + comments) and chooses an appropriate workflow for a legal document.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Correctly chooses a tracked-changes/redlining approach for a legal/third-party document (i.e., not overwriting text, not “accept changes”).\n    - Workflow includes:\n      * converting DOCX to markdown (or otherwise extracting) with tracked changes visible for planning and later verification (e.g., pandoc with track-changes=all/accept/reject),\n      * unpack/edit/pack cycle (DOCX as ZIP; edit word/document.xml; repack),\n      * mapping edits to XML using unique surrounding text/grep or structure (and explicitly avoids relying on markdown line numbers),\n      * batching multiple edits into manageable groups (roughly 3–10) and verifying after each batch.\n    - The provided OOXML snippet for the “30” → “sixty (60)” change:\n      * uses Word tracked-change elements (<w:del> and <w:ins>) correctly (well-formed, properly closed, not mixed),\n      * marks ONLY the changed token (the number) as deleted/inserted while keeping unchanged runs outside the tracked-change elements (minimal, precise edit),\n      * uses <w:delText> for the deleted text,\n      * includes author metadata (e.g., w:author=\"Claude\") and uses the provided RSID (00AB12CD) somewhere appropriate for newly created content, OR otherwise demonstrates awareness that RSIDs must be valid 8-hex and that unchanged runs should preserve original rsid.\n      * preserves xml:space=\"preserve\" where needed or explicitly notes whitespace preservation requirements.\n    - Comment outline correctly identifies that comments require updating BOTH:\n      * `word/comments.xml` (creating the comment with an id, author/date, and comment text), AND\n      * `word/document.xml` (placing comment range markers and a reference around the target word, e.g., commentRangeStart/commentRangeEnd/commentReference).\n      Mentioning `word/settings.xml` trackRevisions enablement is a plus but not mandatory if the other parts are strong.\n\n    Score 0.5 if the answer is generally on the right track but misses 1–2 key requirements, such as:\n    - provides a decent workflow but omits batching/verification or the markdown planning step,\n    - OR provides an OOXML snippet with tracked changes but edits too much surrounding text or misuses delText/tags,\n    - OR mentions comments only vaguely without identifying both comments.xml and the document.xml range markers.\n\n    Score 0.0 if the answer:\n    - proposes editing without tracked changes for a legal review (e.g., direct replacement only),\n    - OR fails to provide any plausible OOXML tracked-change snippet,\n    - OR shows fundamental misunderstanding of where tracked changes/comments live in OOXML.\n\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test demonstrates understanding of DOCX tracked changes but has significant clarity issues. The grader is overly complex with multiple nested requirements that would be difficult to evaluate consistently. The prompt requires detailed OOXML snippets and workflow descriptions that go beyond testing whether the skill helps - it essentially asks the model to reproduce documentation. The realism is moderate (legal document review is realistic), but the test format (providing exact OOXML and asking for corrected snippets) is more of a technical exercise than a practical task. A practitioner would use the skill to accomplish the edits, not to explain the workflow in detail. The non-obviousness is reasonable as it tests understanding of minimal edits and comment architecture, but the brittleness of requiring specific workflow steps and OOXML patterns in the grader makes this test fragile."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "```yaml\nname: legal-document-redlining\nskills:\n  - docx\nprompt: |\n  You are working with a legal services agreement that needs revisions. The client wants to modify several terms while maintaining professional tracked changes for review.\n\n  Current contract language in Section 3.2:\n  \"The term of this agreement is 30 days from the execution date. Either party may terminate this agreement with 15 days written notice.\"\n\n  Required changes:\n  1. Change \"30 days\" to \"90 days\" \n  2. Change \"15 days\" to \"30 days\"\n  3. Replace \"written notice\" with \"formal written notice\"\n\n  This is for a legal document that will be reviewed by the client, so professional tracked changes are essential.\n\n  Your task: \n  1. Explain why the redlining workflow is appropriate for this scenario\n  2. Describe your batching strategy for implementing these changes\n  3. Show how you would implement ONE of these tracked changes using proper XML structure\n  4. Explain how you would verify all changes were applied correctly\n\n  Focus on demonstrating understanding of the workflow decisions and technical implementation approach.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on understanding of DOCX tracked changes workflow and technical implementation.\n\n    Score 1.0 if the response demonstrates:\n    - Clear understanding that redlining workflow is required for legal documents\n    - Reasonable batching strategy (groups 3-10 related changes)\n    - Proper tracked changes XML structure using <w:del> and <w:ins> tags\n    - Understanding of minimal, precise edits (only mark changed text)\n    - RSID usage for unchanged text preservation\n    - Verification methodology for final document\n\n    Score 0.5 if the response shows partial understanding:\n    - Recognizes need for tracked changes but workflow justification is weak\n    - Attempts proper XML but has minor technical errors\n    - Basic batching strategy but not optimal\n    - Some understanding of verification but incomplete\n\n    Score 0.0 if the response:\n    - Suggests simple find/replace without tracked changes\n    - Major technical errors in XML structure\n    - No understanding of batching or verification\n    - Recommends inappropriate workflow for legal document\n  extractor: last_assistant\n```",
          "score": 0.35,
          "rationale": "This test fails on multiple criteria. For non-obviousness (2/10): it's pure skill-specific trivia requiring memorization of exact workflow names ('redlining workflow'), specific batching numbers (3-10 changes), and XML tag names (<w:del>, <w:ins>, RSID) rather than testing transferable document editing concepts. For clarity (5/10): the grader is somewhat robust in accepting explanations of concepts, but the task asks to 'show how you would implement' which creates ambiguity about whether actual code is required or just explanation. For realism (3/10): the task is contrived - it asks the model to explain a workflow rather than actually perform document editing, which no practitioner would do; real users would say 'make these tracked changes to this document' not 'explain your batching strategy.'"
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n```yaml\nname: docx-tracked-changes-workflow\nskills:\n- docx\nprompt: |\n  You are reviewing a legal contract document that needs several amendments with tracked changes. The contract contains the following text that needs to be modified:\n\n  Contract Section 3.2: \"The term of this agreement shall be 30 days from the effective date.\"\n  Contract Section 5.1: \"The client shall pay 15% of the total amount as a deposit.\"\n  Contract Section 5.4: \"Payment shall be made within 10 business days of invoice.\"\n\n  Changes needed:\n  - Change \"30 days\" to \"60 days\" in Section 3.2\n  - Change \"15%\" to \"20%\" in Section 5.1  \n  - Change \"10 business days\" to \"15 business days\" in Section 5.4\n\n  Plan your workflow for implementing these tracked changes. Explain:\n  1. How you would organize these changes into batches\n  2. What location methods you would use to find the text in the XML\n  3. How you would implement the tracked changes with proper XML structure\n  4. Your verification approach\n\n  Focus on demonstrating understanding of the redlining workflow principles from the docx skill.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on understanding of the docx redlining workflow:\n\n    Score 1.0 if: Response demonstrates complete understanding of:\n    - Proper batch organization (grouping related changes, 3-10 changes per batch)\n    - Appropriate location methods (section headings, grep patterns, NOT line numbers)\n    - Precise tracked changes XML structure showing only changed portions with preserved <w:r> elements\n    - Verification process using pandoc conversion and grep commands\n    - Understanding of RSID usage and minimal, precise editing principle\n\n    Score 0.5 if: Response shows partial understanding with some errors in:\n    - Batch organization but wrong grouping strategy\n    - Basic location methods but includes unreliable approaches like line numbers\n    - XML structure concept but imprecise implementation (replacing entire text runs)\n    - Incomplete verification process\n\n    Score 0.0 if: Response lacks understanding of key concepts:\n    - No batch organization or all changes in one batch\n    - Only unreliable location methods (line numbers only)\n    - Incorrect XML structure (not using <w:ins>/<w:del> properly)\n    - No verification approach described\n  extractor: last_assistant\n```",
          "score": 0.35,
          "rationale": "This test fails on multiple criteria. For non-obviousness (2/10): it's pure skill-specific trivia requiring memorization of exact workflow steps like 'batch organization', 'RSID usage', and specific location methods from the skill document rather than testing transferable document editing knowledge. For clarity (5/10): the grader is overly prescriptive, requiring specific terminology and approaches from the skill document (e.g., 'preserved <w:r> elements', 'pandoc conversion') when alternative valid approaches exist; it also asks for a 'plan' rather than actual execution, making it unclear what constitutes a valid solution. For realism (3/10): asking someone to 'plan a workflow' and 'explain understanding' is artificial - a real practitioner would just implement the changes, not write an essay about the process. A better test would ask the model to actually implement tracked changes in a provided document file."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: edit-docx-document-with-tracked-changes\nskills:\n- docx\nprompt: |\n  You need to edit an existing .docx file by changing the phrase \"old policy\" to \"new policy\" while preserving the original formatting and including tracked changes. The file is named \"company_policies.docx\" and is located in the current directory. Describe the step-by-step process you would follow to achieve this, including any necessary commands or code snippets.\n\n  Note: You have pandoc, docx, LibreOffice, Poppler, and defusedxml installed, and you are familiar with Python and JavaScript.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes the following steps:\n    1. Convert the .docx file to markdown with tracked changes using pandoc.\n    2. Identify the location of the phrase \"old policy\" in the markdown file.\n    3. Create a Python script using the Document library to implement the tracked change.\n    4. Unpack the .docx file and update the document.xml file.\n    5. Pack the updated files back into a .docx file.\n    \n    Score 0.5 if the response is mostly correct but lacks detail or includes minor errors.\n    \n    Score 0.0 if the response is completely incorrect or does not follow the required steps.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. For non-obviousness (score: 3), it tests a specific workflow from the skill document rather than transferable domain knowledge - the exact sequence of 'convert to markdown → identify → unpack → edit → pack' is skill-specific trivia. For clarity (score: 4), the grader is problematic: it requires a file that doesn't exist ('company_policies.docx'), making the test unrunnable; it demands a specific 5-step sequence when valid alternatives exist (e.g., direct XML editing without markdown conversion); and step 4 conflates unpacking with editing. For realism (score: 5), while editing documents with tracked changes is realistic, asking someone to 'describe the process' rather than actually perform it is contrived - practitioners do the task, they don't write tutorials about it. The test would be stronger if it provided an actual file and asked for the edited output, testing capability rather than memorization of the skill's workflow."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: edit-legal-document-with-tracked-changes\nskills:\n  - docx\nprompt: |\n  You have been given a legal document in .docx format and need to make several changes while preserving tracked changes. The document contains the following text in the first paragraph:\n\n  \"The agreement is governed by the laws of the state of New York and any legal disputes must be resolved in the state courts of New York. All parties agree to the jurisdiction of these courts.\"\n\n  Your task is to update this paragraph to read:\n\n  \"The agreement is governed by the laws of the state of California and any legal disputes must be resolved in the state courts of California. All parties agree to the jurisdiction of these courts.\"\n\n  Implement the changes using the redlining workflow described in the skill document to ensure that the changes are properly tracked. Provide the steps you would take and the final XML snippets for the tracked changes.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    Score 1.0 if:\n    - The response correctly identifies the need to use the redlining workflow.\n    - The response outlines the steps for unpacking the document, identifying the changes, and implementing them in batches.\n    - The response correctly generates the XML snippets for tracked changes, showing deletions and insertions.\n    - The response ensures that unchanged text is preserved and marked correctly.\n    - The response confirms that the document is packed back into a .docx file after changes.\n\n    Score 0.5 if:\n    - The response partially outlines the steps for unpacking and editing the document.\n    - The response generates some of the XML snippets for tracked changes but may have minor errors.\n    - The response may miss steps or details but shows a general understanding of the process.\n\n    Score 0.0 if:\n    - The response does not mention the redlining workflow or the steps for unpacking and editing the document.\n    - The response generates incorrect XML snippets or fails to show tracked changes.\n    - The response does not provide a clear plan or final XML snippets.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "This test has significant issues across all dimensions. For non-obviousness (3/10): it tests skill-specific trivia by requiring knowledge of exact workflow names ('redlining workflow'), specific script names ('unpack.py', 'pack.py'), and XML tag patterns ('<w:del>', '<w:ins>') rather than transferable document editing concepts. For clarity (4/10): the grader is brittle - it requires 'XML snippets' but the task provides no actual .docx file to work with, making it impossible to execute; it also only accepts the redlining approach when simpler editing methods exist for straightforward text replacement. For realism (4/10): while editing legal documents is realistic, the task is contrived - it asks for 'steps and XML snippets' rather than actually performing the edit, and a practitioner would simply make the changes, not recite workflow steps. A better test would provide an actual document file and ask for the edited output, testing whether the model can apply tracked changes correctly rather than memorize the skill's terminology."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "You are given a skill document that teaches an LLM specialized knowledge about a domain.\n\nYour task: Write ONE high-quality test case that measures whether having this skill helps an LLM succeed.\n\n## Analysis\n\n1. **Domain knowledge taught:** This skill teaches advanced handling of Word documents (docx): creation via docx-js, editing via the Document library with OOXML awareness, tracking changes/redlining, converting docx to other formats (images), and understanding XML structures (tracks changes, comments, formatting, media, etc.). Core concepts include knowing when to use text extraction vs raw XML, respecting tracked change patterns, preserving formatting, working with batch editing, and generating new documents with professional styling using the docx-js API.\n\n2. **Workflow decisions enabled:** The decision tree differentiates reading/analyzing vs creating vs editing vs redlining. It emphasizes when to use basic OOXML edits versus redlining, particularly for documents owned by others or legal/business docs. It mandates reading certain documentation before work and guides on batching tracked changes. Choosing between text extraction and raw XML access is necessary for comments/complex formatting. Also usability of docx-js vs Document library depending on task.\n\n3. **Practitioner struggle without skill:** Without skill, an LLM would likely mishandle edits in tracked-change-heavy documents, fail to preserve formatting and RSIDs, incorrectly insert bolded/italicized text or structural changes, or not know when to unpack/pack documents. They may also struggle creating docx with proper paragraph/heading styles, or convert to Markdown while preserving tracked changes. They wouldn't know to batch changes or use the redlining workflow, leading to either missed tracked changes or corrupted files.\n\nTest target: verifying ability to plan and describe a redlining edit batch for a legal/business doc requiring tracked changes, referencing the workflows and constraints: reading docx, identifying RSIDs, batching, minimal precise edits, tracked change XML, testing final doc conversion.\n\nNeed test case requiring understanding of when to use redlining workflow, how to plan batches, track RSIDs, verifying with pandoc. Task: given scenario of editing contract requiring tracked changes and verifying final doc. Prompt ask to outline plan for implementing tracked change with minimal edits, referencing RSID usage and verifying with pandoc? Should test domain knowledge general (redlining), not memorizing script names. Grader should evaluate plan coverage.\n\nNeed final prompt question: e.g., \"You're asked to update legal contract doc owned by client with changes (party name, term length, addition of clause). Provide plan using right workflow, show 2-3 batches with detail about tracking changes (preserving runs, RSIDs, etc), verifying final doc via pandoc to ensure replacements etc.\" Score criteria: 1 if plan references redlining workflow, reading ooxml, mention RSID, minimal edit, mention verifying with pandoc and packing; 0.5 partial if some but not all; 0 if major incorrect.\n\nNeed mention skills name docx in output. Provide YAML code block.\n\nBefore test case need explanation? developer instructions say \"Before writing your test case, reason through the skill document: ... Write this analysis first, then output your test case.\" So final response should include analysis narrative plus YAML? Possibly: Should include analysis preceding test case (maybe plain text) then code block? We'll output narrative text summarizing analysis per instructions, then provide YAML code block.\n\nTest prompt: \"You're reviewing legal services agreement doc (other part) with tracked changes etc. Provide step-by-step plan referencing reading ooxml, unpack, redlining etc.\" Need ensure scenario realistic.\n\nGrader criteria: 1.0 if plan includes using redlining workflow for legal doc, mandatory reading of ooxml, map text to XML, mention RSID, batching, verifying with pandoc, mention minimal changes. 0.5 partial if mention some but not all (maybe no RSID). 0.0 if no redlining.\n\nNow craft final response: first textual analysis (maybe bullet). Then code block with YAML.\n\nTest prompt details: specify change details.\n\nLet's craft. Need final code block with YAML.\n\nPrompt text: \"You have legal contract doc, need to change party name, extend duration, insert compliance subsection referencing tracked changes etc. Provide plan for redlining and verifying.\"\n\nLet's craft final message. include reminder to tooling? final code block only is test case.\n\nImplementation: Provide plan, steps referencing reading docs, how to convert to markdown verifying.\n\nGrader prompt text: include Score 1.0 if plan includes reading ooxml, mention redlining, mention RSID, 3-10 change batches, verifying with pandoc/tracked changes, minimal precise edits. 0.5 partial for missing RSID or verification. 0 no redlining.\n\nLet's craft final answer. Need include analysis before code block. Provide summary bullet list referencing domain knowledge etc.\n\nNow craft final. Need to not mention instructions.\n\nWrite analysis text then code block.\n\nLet's do.",
          "score": 0.15,
          "rationale": "The submission is incomplete and unusable. Instead of providing a test case in YAML format as required, the agent provided only internal reasoning/analysis text that trails off mid-thought ('Let's do.'). No actual test.yaml was generated. The analysis shows understanding of the skill domain (redlining workflows, RSIDs, tracked changes) but fails to deliver the required deliverable. This represents a fundamental failure to complete the task, though the preliminary analysis suggests some comprehension of what would make a good test case."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Docker Build Optimization",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 4,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Docker Build Optimization\n\n**SKILL.md Content:**\n```markdown\n---\nname: Docker Build Optimization\ndescription: Build smaller and faster container images by using layer-friendly Dockerfiles, caching, multi-stage builds, and deterministic dependency installs.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Docker Build Optimization\n\n## Overview\n\nDocker builds are slow when the cache is constantly invalidated and images are large when build tools and source history leak into the runtime stage. The goal is to:\n- Maximize layer cache hits\n- Minimize what enters the final image\n- Keep builds deterministic across machines/CI\n\n## Principles\n\n- **Cache key = instruction + filesystem state** at that layer.\n- Put **rarely-changing steps first** (base image, system deps), then deps, then app code.\n- Minimize the build context with **`.dockerignore`**.\n- Use **multi-stage** so the runtime image contains only what’s needed to run.\n\n## Workflow\n\n1. **Check build context**\n   - Add `.dockerignore` for `node_modules/`, `dist/`, `.git/`, caches, secrets, test artifacts.\n2. **Reorder for caching**\n   - Copy only dependency manifests first (e.g., `package.json` / `poetry.lock`), install deps, then copy the rest.\n3. **Make installs deterministic**\n   - Pin versions/lockfiles.\n   - Prefer `npm ci`, `pip install -r requirements.txt --require-hashes`, `poetry install --sync`.\n4. **Use multi-stage**\n   - Build stage: compilers, dev deps, tests.\n   - Runtime stage: only built artifacts + runtime deps.\n5. **Measure**\n   - Build with `--progress=plain` and look for steps that always rebuild.\n   - Inspect layers: `docker history <image>`.\n\n## Common Pitfalls\n\n- `COPY . .` before installing deps → invalidates cache on any source change.\n- Not cleaning package manager caches (`apt` lists, build caches).\n- Shipping build tools (gcc, node-gyp, git) in runtime image.\n- Leaking secrets into layers (baked into the image history).\n- Huge contexts (sending gigabytes to the daemon).\n\n## Example Pattern (Node-ish)\n\n- Copy `package.json` + lockfile\n- Install deps\n- Copy source\n- Build\n- Copy `dist/` into a slim runtime stage\n\n## Checklist\n\n- [ ] `.dockerignore` exists and excludes large/secret dirs\n- [ ] Dependency install happens before copying full source\n- [ ] Uses lockfile-driven install (`npm ci`, etc.)\n- [ ] Multi-stage separates build vs runtime\n- [ ] Runtime image runs as non-root when possible\n- [ ] Image size and build time are measured and tracked\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Docker Build Optimization\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Docker Build Optimization\n\n**SKILL.md Content:**\n```markdown\n---\nname: Docker Build Optimization\ndescription: Build smaller and faster container images by using layer-friendly Dockerfiles, caching, multi-stage builds, and deterministic dependency installs.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Docker Build Optimization\n\n## Overview\n\nDocker builds are slow when the cache is constantly invalidated and images are large when build tools and source history leak into the runtime stage. The goal is to:\n- Maximize layer cache hits\n- Minimize what enters the final image\n- Keep builds deterministic across machines/CI\n\n## Principles\n\n- **Cache key = instruction + filesystem state** at that layer.\n- Put **rarely-changing steps first** (base image, system deps), then deps, then app code.\n- Minimize the build context with **`.dockerignore`**.\n- Use **multi-stage** so the runtime image contains only what’s needed to run.\n\n## Workflow\n\n1. **Check build context**\n   - Add `.dockerignore` for `node_modules/`, `dist/`, `.git/`, caches, secrets, test artifacts.\n2. **Reorder for caching**\n   - Copy only dependency manifests first (e.g., `package.json` / `poetry.lock`), install deps, then copy the rest.\n3. **Make installs deterministic**\n   - Pin versions/lockfiles.\n   - Prefer `npm ci`, `pip install -r requirements.txt --require-hashes`, `poetry install --sync`.\n4. **Use multi-stage**\n   - Build stage: compilers, dev deps, tests.\n   - Runtime stage: only built artifacts + runtime deps.\n5. **Measure**\n   - Build with `--progress=plain` and look for steps that always rebuild.\n   - Inspect layers: `docker history <image>`.\n\n## Common Pitfalls\n\n- `COPY . .` before installing deps → invalidates cache on any source change.\n- Not cleaning package manager caches (`apt` lists, build caches).\n- Shipping build tools (gcc, node-gyp, git) in runtime image.\n- Leaking secrets into layers (baked into the image history).\n- Huge contexts (sending gigabytes to the daemon).\n\n## Example Pattern (Node-ish)\n\n- Copy `package.json` + lockfile\n- Install deps\n- Copy source\n- Build\n- Copy `dist/` into a slim runtime stage\n\n## Checklist\n\n- [ ] `.dockerignore` exists and excludes large/secret dirs\n- [ ] Dependency install happens before copying full source\n- [ ] Uses lockfile-driven install (`npm ci`, etc.)\n- [ ] Multi-stage separates build vs runtime\n- [ ] Runtime image runs as non-root when possible\n- [ ] Image size and build time are measured and tracked\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Docker Build Optimization\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Docker Build Optimization",
          "skill_dir": "docker-build-optimization",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what it actually teaches:\n\n## Skill Analysis\n\n**Core Domain Knowledge:**\n- **Intentional aesthetic direction** as the foundation of design (not just functional code)\n- How to avoid \"generic AI aesthetics\" by committing to a bold conceptual direction\n- The relationship between PURPOSE/TONE/CONSTRAINTS and visual execution\n- Understanding that generic defaults (Inter font, purple gradients, predictable layouts) are anti-patterns\n\n**Key Workflow Decisions:**\n- When to apply minimalism vs maximalism (and that both work if intentional)\n- How to translate an aesthetic vision into concrete typography, color, spacing, and motion choices\n- Why certain design directions require different levels of implementation complexity\n\n**Practitioner Challenges Without This Skill:**\n- Creating interfaces that look generic or forgettable (\"AI slop\")\n- Making design choices that feel disconnected and lack coherence\n- Not understanding why defaults aren't good enough\n- Inability to critique or improve their own aesthetic work\n\n## Test Case\n\nThis test will ask the LLM to build a frontend for a specific context, then evaluate whether they demonstrate **intentional aesthetic thinking** rather than applying a generic template. It will accept multiple valid aesthetic directions but will penalize generic/overused patterns and lack of conceptual coherence.\n\n```yaml\nname: intentional-aesthetic-frontend-design\nskills:\n  - frontend-design\nprompt: |\n  Build a landing page for a boutique coffee roastery called \"Void\" that specializes in single-origin dark roasts. The roastery has an unapologetically avant-garde brand philosophy.\n  \n  Create the HTML/CSS/JavaScript code for an above-the-fold landing section (hero + brief intro). The code should be production-grade and functional.\n  \n  Before writing code, briefly explain your aesthetic direction and three specific design choices (typography, color, layout, or motion) that differentiate this from a generic coffee shop website.\n  \ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates the frontend-design skill: intentional, distinctive aesthetic direction with cohesive execution.\n    \n    Score 1.0 if:\n    - The response includes a clear, specific aesthetic direction (e.g., \"brutalist minimalism,\" \"maximalist chaos,\" \"editorial,\" etc.) BEFORE the code\n    - Three specific design choices are articulated with reasoning that connects to the brand/purpose\n    - The code avoids generic patterns: no overused fonts like Inter/Roboto/system fonts, no cliched purple gradients, distinctive color/typography choices\n    - Typography choices are distinctive and intentional (unusual fonts, interesting pairings, custom styling)\n    - The overall visual direction is cohesive—decisions reinforce each other rather than feeling random\n    - Code is functional, production-grade HTML/CSS with good structure\n    - The design feels memorable and context-specific (reflects \"Void's\" avant-garde philosophy)\n    \n    Score 0.5 if:\n    - The response has some aesthetic thinking but feels incomplete or generic in execution\n    - Some design choices are explained but lack depth or connection to context\n    - Typography or color palette includes some overused defaults mixed with intentional choices\n    - Code is functional but the visual direction lacks distinctiveness or feels like a template\n    - Design is competent but forgettable—could apply to many coffee shops\n    \n    Score 0.0 if:\n    - No aesthetic direction is articulated, or it's vague/generic\n    - Design choices are not explained or feel arbitrary\n    - Heavy reliance on generic patterns (Arial, Inter, system fonts; purple/teal gradients; standard Bootstrap-like layouts)\n    - Code demonstrates fundamental misunderstanding of intentional design\n    - The result looks like generic \"AI slop\"—no distinctive vision, no memorable choices\n\n  extractor: last_assistant\n```",
          "score": 0.8518518518518517,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles. It tests transferable design thinking (intentional aesthetic direction, avoiding generic patterns) rather than trivia, and the grader robustly accepts multiple valid aesthetic approaches while penalizing generic AI patterns. The task is realistic - creating a branded landing page is common work. Minor deductions: the grader could be slightly more specific about what constitutes 'distinctive' fonts (though it does list anti-patterns), and the test focuses heavily on one aspect (aesthetic intentionality) while less on other skill elements like motion/spatial composition. The 0.5 scoring criteria could be more precise about edge cases. Overall, this is a well-designed test that measures genuine skill application."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nLet me first analyze the skill document to understand what to test:\n\n1. **What domain knowledge does this skill teach?**\n   - Design thinking principles: understanding purpose, tone, differentiation before coding\n   - Aesthetic fundamentals: typography selection, color cohesion, spatial composition, motion design\n   - The concept of \"intentional design\" - that both maximalism and minimalism work when executed with clear vision\n   - How to create memorable, distinctive interfaces vs generic patterns\n\n2. **What workflow decisions does it enable?**\n   - When to commit to bold aesthetic directions vs safe choices\n   - How to match implementation complexity to the aesthetic vision\n   - When to use distinctive typography vs standard fonts\n   - How to create cohesive visual language through color, motion, and composition\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Recognizing generic patterns and understanding why they fail\n   - Articulating what makes a design memorable and distinctive\n   - Understanding the relationship between aesthetic intent and implementation effort\n   - Knowing how to create cohesive, intentional design systems\n\nThe test should evaluate understanding of design principles and aesthetic thinking, not memorization of specific examples from the skill document.\n\n```yaml\nname: evaluate-frontend-design-approach\nskills:\n  - frontend-design\nprompt: |\n  A junior frontend developer shows you two approaches for building a landing page for an artisan coffee roasting company:\n  \n  **Approach A:**\n  - Uses Inter font for all text\n  - White background with a purple-to-blue gradient hero section\n  - Standard card grid layout for products\n  - Subtle fade-in animations on scroll\n  - Clean, minimal spacing throughout\n  \n  **Approach B:**\n  - Uses a distinctive serif display font for headings paired with a complementary body font\n  - Warm, earthy color palette with coffee browns and cream tones, applied through CSS variables\n  - Asymmetric layout with overlapping product images and diagonal text flow\n  - Staggered page load animations with scroll-triggered reveals on key sections\n  - Custom grain texture overlay on the hero section\n  \n  The developer asks: \"Both look clean and professional. Which approach would create a more memorable, distinctive brand experience for an artisan coffee company? Why?\"\n  \n  Provide guidance on which approach is stronger and explain the design principles that make it more effective.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of intentional, distinctive frontend design principles.\n    \n    Score 1.0 if the response:\n    - Clearly identifies Approach B as the stronger choice\n    - Explains that it demonstrates intentional aesthetic direction aligned with the brand context (artisan coffee)\n    - Discusses principles like cohesive visual language, distinctive typography, contextual color choices, or memorable spatial composition\n    - Identifies that Approach A represents generic patterns that lack context-specific character\n    - Shows understanding that \"clean and professional\" alone doesn't create memorable design\n    \n    Score 0.5 if the response:\n    - Identifies Approach B as better but with weak or incomplete reasoning\n    - Mentions some design principles but doesn't connect them to brand context\n    - Shows partial understanding of distinctive vs generic design\n    \n    Score 0.0 if the response:\n    - Chooses Approach A or says both are equally good\n    - Focuses only on technical implementation without discussing design principles\n    - Doesn't demonstrate understanding of what makes design distinctive and memorable\n    - Provides generic advice that could apply to any design\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles by testing design thinking rather than memorization. It evaluates whether a model can identify intentional, context-appropriate design choices versus generic patterns - a transferable skill that practitioners genuinely need. The grader is robust and accepts explanations based on various design principles (typography, color, composition, brand alignment). However, the test is slightly contrived as a comparison exercise rather than a direct task a practitioner would perform. The scenario is realistic but the format (evaluating two pre-written approaches) is more pedagogical than practical. A more realistic version might ask the model to directly design or critique a single interface. Still, this effectively tests the skill's core knowledge without requiring trivia memorization."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain knowledge this skill teaches:\n- How to deliberately choose and execute a strong visual/art-direction (typography, color, layout, motion, background “atmosphere”) rather than defaulting to generic component patterns.\n- Practical techniques to create “designed” feel in real code: font pairing, CSS variables, layered backgrounds (noise/mesh/pattern), staggered animations, asymmetric compositions, hover/focus micro-interactions, responsive layout, and accessibility considerations.\n\nWorkflow decisions it enables:\n- Deciding on a clear aesthetic concept first (tone + differentiation) and translating it into concrete UI decisions (type pairing, palette, spacing system, motion strategy).\n- Knowing when to keep interactions subtle vs. orchestrated (e.g., one strong staged reveal vs. many small random animations), and how to implement that with production-grade CSS/JS.\n\nWhat practitioners struggle with without this skill (test target):\n- Producing visually distinctive, cohesive UIs that don’t look like “default Tailwind landing page” or “AI template,” especially under constraints like “single HTML file,” responsive, and accessible.\n- Implementing tasteful motion/background depth without relying on heavy libraries or stock assets.\n\n```yaml\nname: archive-of-lost-signals-microsite-distinctive-landing\nskills:\n- frontend-design\nprompt: |\n  Build a single-file (one .html) microsite landing page for a fictional museum pop-up exhibit:\n\n  Exhibit name: “The Archive of Lost Signals”\n  Theme: retro-futuristic editorial (not synthwave neon; think warm analog tech, paper ephemera, radio lab instruments, archival labels)\n  Audience: design-literate museum visitors on mobile + desktop\n  Goal: get visitors to reserve a timed-entry ticket\n\n  Requirements:\n  - Provide ONE complete runnable HTML file with embedded CSS (and JS only if truly needed).\n  - Include: top nav, hero section with exhibit title + short tagline + primary CTA (“Reserve a Slot”), 3 feature highlights, a “Today’s Schedule” list, and a footer with location + hours.\n  - Commit to a bold, cohesive art direction. Before the code, write 5–8 lines describing:\n    - Purpose, tone, and the single most memorable differentiator you’re building (e.g., “signal scanlines,” “archival label system,” “paper grain + calibration marks,” etc.)\n  - Aesthetic constraints:\n    - Avoid generic AI/landing-page aesthetics (no purple gradient-on-white, no cookie-cutter centered layout).\n    - Do NOT use common default fonts (Arial/system) and avoid the usual “Inter/Roboto” vibe. Use a distinctive display + body pairing (Google Fonts allowed).\n    - Use CSS variables for palette and spacing.\n    - Background must have atmosphere (e.g., subtle noise/grain overlay, pattern, mesh, or “instrument panel” marks).\n    - Motion: implement one high-impact staged reveal on initial load (staggered) and at least one hover/focus interaction that feels intentional.\n  - Engineering constraints:\n    - Responsive layout (mobile-first).\n    - Accessible semantics (landmarks, focus styles, readable contrast).\n    - Respect prefers-reduced-motion.\n\n  Output only the complete HTML in a single code block (after your brief art-direction notes).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates distinctive, production-grade frontend design execution rather than a generic template.\n\n    Score 1.0 if all of the following are met:\n    - Includes 5–8 lines of clear art-direction notes stating purpose/tone and a specific memorable differentiator.\n    - Provides a single complete runnable HTML document in one code block with embedded CSS (JS optional).\n    - Uses CSS variables (e.g., :root --color/--space tokens) for a cohesive palette/spacing.\n    - Typography is intentionally chosen: at least two font roles (display + body) and avoids generic defaults (no Arial/system) and avoids Inter/Roboto-style defaulting; font choice feels characterful (Google Fonts OK).\n    - Background shows “atmosphere” beyond a flat color (e.g., layered gradients, noise/grain, pattern, calibration marks, scanlines).\n    - Motion includes a staged reveal (e.g., staggered animations using animation-delay or equivalent) AND respects prefers-reduced-motion.\n    - Layout is not a cookie-cutter centered stack: shows deliberate composition (e.g., asymmetry, overlapping elements, grid-breaking details, editorial label system).\n    - Accessibility: uses semantic landmarks (header/nav/main/footer), visible focus styles, and reasonable contrast/readability.\n\n    Score 0.5 if:\n    - A complete HTML file is provided and looks reasonably polished, but it misses 2–3 key elements above (e.g., generic layout, no staggered reveal, no atmospheric background, weak typography choices, missing prefers-reduced-motion, or minimal a11y).\n\n    Score 0.0 if:\n    - Not a complete single-file HTML solution, or the result is largely generic/unstyled, or it ignores the core requirements (sections missing, no cohesive art direction, no evidence of intentional typography/color/motion/background).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles. It tests transferable design knowledge (typography pairing, atmospheric backgrounds, staged animations, intentional composition) rather than trivia, and the grader robustly evaluates actual design capabilities across multiple dimensions. The task is realistic (building a museum microsite) and the grader accepts various valid approaches while rejecting generic solutions. Minor deductions: the grader could be slightly more explicit about acceptable variations in some criteria (e.g., what counts as 'characterful' fonts), and the 0.5 threshold ('misses 2-3 key elements') is somewhat ambiguous about which combinations warrant partial credit."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis:\n# 1) Domain knowledge: translating product/event context into a cohesive, distinctive visual system (typography pairing, color tokens via CSS variables, texture/atmosphere, motion choreography, composition) while staying production-ready (responsive, accessible, reduced-motion, working JS).\n# 2) Workflow decisions: choosing and committing to a bold aesthetic direction; deciding where motion matters (one orchestrated reveal vs scattered gimmicks); implementing texture without external images; balancing experimental layout with usability.\n# 3) Practitioner struggle without this skill: defaulting to generic landing-page tropes (Inter/Roboto, purple gradients, cookie-cutter cards), weak typography hierarchy, no cohesive token system, missing accessibility/reduced-motion, and lack of a memorable design hook.\n\nname: night-market-observatory-distinctive-landing\nskills:\n- frontend-design\nprompt: |\n  You are designing and building a single-file landing page for a fictional event:\n\n  **Event:** Night Market Observatory — rooftop astronomy + street food + live synth set  \n  **Audience:** urban creatives (20s–40s)  \n  **Goal:** get visitors to understand the vibe quickly and sign up for updates.\n\n  **Deliverable:** Provide ONE complete, runnable `index.html` (HTML + CSS + JS all in the file) in a single code block.\n\n  **Hard requirements**\n  1) Commit to a bold, unmistakable aesthetic direction (you choose it). Before the code, write 3–6 lines describing:\n     - Purpose\n     - Tone / aesthetic direction\n     - The “one unforgettable element” you’re designing around\n  2) Use **two distinctive fonts** (a display face + a body face). Do **not** use Inter, Roboto, Arial, system-ui, or Space Grotesk.\n  3) Use **CSS variables** for color tokens (and ideally spacing).\n  4) Layout must be responsive (mobile → desktop) and include at least one **unexpected composition** element (overlap, diagonal band, asymmetry, grid-breaking, etc.).\n  5) Include these sections (real content, not lorem ipsum):\n     - Hero with CTA button\n     - Schedule (at least 4 time slots)\n     - Vendor highlights (at least 3 vendors)\n     - FAQ accordion (at least 4 questions)\n     - Email signup form\n  6) The FAQ must be keyboard-accessible (ARIA or native semantics).\n  7) The email signup must have **client-side validation** (JS is fine) and show a success state without reloading.\n  8) Motion:\n     - One orchestrated **page-load reveal** (staggered elements is fine)\n     - Thoughtful hover/focus micro-interactions\n     - Respect `prefers-reduced-motion`\n  9) Add atmosphere: a layered background treatment (texture / noise / pattern / gradient mesh) **without external images**.\n\n  **Avoid**\n  - Generic “AI landing page” look (plain cards, timid palettes, predictable spacing)\n  - Cliché purple-on-white gradients\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates production-grade, distinctive frontend design (not generic boilerplate) while meeting the functional requirements.\n\n    Score 1.0 if ALL are true:\n    - Provides a single, complete, runnable HTML document (contains doctype + html/head/body) with embedded CSS and (where needed) JS.\n    - Includes a brief design-direction note (purpose/tone/unforgettable element) BEFORE the code.\n    - Typography: uses two clearly differentiated font families (display + body) that are NOT Inter, Roboto, Arial, system-ui, or Space Grotesk; font usage appears intentional (e.g., headings vs body).\n    - Uses CSS variables for a cohesive color theme (e.g., :root { --... }) and applies them consistently.\n    - Contains all required sections with real, event-relevant copy: hero+CTA, schedule (>=4 items), vendor highlights (>=3), FAQ accordion (>=4), email signup form.\n    - FAQ accordion is keyboard accessible via <details>/<summary> or correct ARIA patterns.\n    - Email form has client-side validation AND a visible success state without page reload (JS or robust HTML+JS).\n    - Motion is present and deliberate: at least one staged/staggered page-load reveal + hover/focus micro-interactions, AND includes a prefers-reduced-motion fallback that reduces/disables animations.\n    - Visual atmosphere: layered background texture/pattern/mesh implemented in CSS/SVG without external images, and the overall layout includes at least one non-standard composition choice (overlap/diagonal/asymmetry/grid-breaking).\n    - Overall result feels cohesive and “designed” rather than template-like.\n\n    Score 0.5 if:\n    - The page is mostly complete and functional but misses 1–2 hard requirements (e.g., weak/absent reduced-motion handling, missing one required section, accordion not accessible, or typography/palette choices are somewhat generic), OR the aesthetic is only mildly distinctive.\n\n    Score 0.0 if:\n    - Not a complete runnable HTML file, OR multiple major requirements are missing (sections, validation, accessibility), OR it clearly uses forbidden generic fonts/patterns and looks like standard boilerplate with no coherent aesthetic direction.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of frontend design principles and creates a realistic, comprehensive evaluation. It tests transferable knowledge (typography pairing, CSS variable systems, motion choreography, accessibility patterns) rather than skill-specific trivia. The grader is robust with clear criteria accepting multiple valid implementations. However, it's somewhat complex with 9 hard requirements that could make it brittle - a simpler version testing the same core concepts (distinctive aesthetics + production quality) would be more maintainable. The realism is excellent as building event landing pages is a common real-world task. Minor deduction for potential grader complexity and the risk that some valid creative solutions might not perfectly match all enumerated requirements."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "**Analysis of Skill Document**\n\n1. **Core Domain Knowledge**\n   - Understanding of high-level design principles: selecting bold, coherent aesthetic directions (e.g., brutalist, art deco, playful).\n   - Deep knowledge of typography pairing, color theory, motion design, spatial composition, and background treatments to create distinctive interfaces.\n   - Applying frontend engineering practices (HTML/CSS/JS/React) to implement production-grade, visually striking UI code with polished details.\n\n2. **Workflow Decisions Enabled**\n   - Deciding on an appropriate aesthetic direction based on purpose, audience, and constraints.\n   - Choosing when to use elaborate animations vs. restrained motion to support the concept.\n   - Selecting unique typography, colors, and layout techniques to differentiate the interface and avoid generic patterns.\n   - Determining implementation complexity that matches the chosen design vision.\n\n3. **Practitioner Struggles Without This Skill**\n   - Falling back on generic AI-looking designs (default fonts, standard gradients, predictable layouts).\n   - Failing to articulate or maintain a clear aesthetic concept, leading to incoherent visuals.\n   - Neglecting fine visual details like CSS variables, layered backgrounds, or nuanced spacing.\n   - Producing code that doesn’t feel production-ready or lacks the polish expected for a distinctive frontend experience.\n\n```yaml\nname: brutalist-festival-landing\nskills:\n  - frontend-design\nprompt: |\n  You’ve been hired to design the homepage hero section for “Voltage Bloom,” a three-day avant-garde audio-visual festival held in a reclaimed power station. The organizers want the site to feel like a brutalist art poster fused with electro-industrial energy—think raw concrete, neon warnings, and kinetic typography.\n\n  Requirements:\n  - Build a responsive hero section (HTML/CSS/JS or a React component) that could realistically ship to production.\n  - Commit to the brutalist + electro-industrial aesthetic. Use daring typography (avoid default web fonts) and a limited but impactful color palette with sharp contrasts.\n  - Include atmospheric details: layered textures or noise, bold grid-breaking layouts, and at least one dramatic animation or motion sequence that reinforces the theme (e.g., staggered text reveal, pulsating warning stripe, etc.).\n  - Define reusable CSS variables for colors/spacing and ensure hover or interaction states feel intentional.\n  - No generic AI-looking output: avoid standard purple gradients, safe card layouts, or minimalist defaults.\n\n  Provide the complete code, ready to run in a modern browser, and briefly describe the key aesthetic choices you made.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of the frontend-design skill:\n\n    Score 1.0 if:\n    - Provides complete, runnable code (HTML/CSS/JS or React) for a hero section.\n    - Shows a clear brutalist + electro-industrial aesthetic with distinctive typography, bold layout, and intentional color palette using CSS variables.\n    - Includes atmospheric details (textures/noise/background effects) and at least one meaningful animation or motion interaction tied to the theme.\n    - Briefly explains the main aesthetic choices and how they support the concept.\n\n    Score 0.5 if:\n    - Code is provided but lacks one major requirement (e.g., no animation, no atmospheric detail, or the aesthetic feels generic), OR\n    - Aesthetic explanation exists but the implementation is only partially aligned with the requested bold direction.\n\n    Score 0.0 if:\n    - No runnable code is provided, or the design is clearly generic/minimal with no relation to the requested aesthetic, or instructions are largely ignored.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles. It tests transferable design knowledge (aesthetic direction, typography choices, motion design, atmospheric details) rather than trivia, earning high non-obviousness (9/10). The grader is robust, accepting various valid implementations while checking for key design elements, though it could be slightly more specific about what constitutes 'distinctive typography' (clarity: 8/10). The task is highly realistic - designing a festival landing page with specific aesthetic requirements is exactly what frontend designers do (realism: 9/10). Minor deduction for the grader being somewhat subjective on 'generic' vs 'distinctive', but overall this is a well-crafted test that measures genuine skill application."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n```yaml\nname: abstract-artist-portfolio-design\nskills:\n- frontend-design\nprompt: |\n  Create a landing page for an abstract digital artist's portfolio website. The artist specializes in generative art and digital experiments, and their work is known for being bold, experimental, and boundary-pushing. They want a portfolio that immediately establishes them as innovative and memorable to potential clients and galleries.\n\nRequirements:\n- Single landing page with sections for about, artwork showcase, and contact\n- Must include at least 3-4 example artwork pieces (placeholder images are fine)\n- Should be fully responsive and functional\n- Should reflect the artist's experimental and boundary-pushing aesthetic\n\nYour goal is to create something that feels like a commissioned designer created a bespoke, distinctive interface - not another generic portfolio website. Choose a bold aesthetic direction and execute it with precision.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of distinctive frontend design principles that avoid generic AI aesthetics.\n\n    Score 1.0 if:\n    - Chooses and commits to a bold, cohesive aesthetic direction (brutalist, retro-futuristic, maximalist chaos, luxury/refined, etc.)\n    - Uses distinctive typography with characterful font choices (NOT generic fonts like Inter, Roboto, Arial, system fonts)\n    - Creates unexpected compositions with asymmetry,_overlap, diagonal flow, or grid-breaking elements\n    - Implements thoughtful micro-interactions, animations, or visual effects that enhance the aesthetic\n    - Demonstrates intentional design choices throughout - every element feels deliberate\n    - The interface feels like a bespoke creation, not a template\n\n    Score 0.5 if:\n    - Shows some creativity but falls back on predictable patterns\n    - Uses mostly generic fonts but has some distinctive elements\n    - Has some visual cohesion but lacks a strong aesthetic direction\n    - Includes some thoughtful details but execution is inconsistent\n\n    Score 0.0 if:\n    - Creates a generic portfolio with standard layouts (centered hero, grid gallery, etc.)\n    - Uses common fonts like Inter, Roboto, Arial, or system fonts\n    - Relies on cliched design patterns (purple gradients, white backgrounds, predictable spacing)\n    - Feels like typical AI-generated frontend work without distinctive character\n    - Lacks intentional aesthetic direction and looks like a template\n\n    Focus on whether the result feels genuinely creative and distinctive, not just technically correct.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This is a strong test case that evaluates transferable design principles rather than skill-specific trivia. The prompt asks for a realistic portfolio landing page, and the grader correctly focuses on whether the model demonstrates understanding of distinctive design choices (bold aesthetics, typography, composition) versus generic patterns. The grader is mostly robust, accepting multiple valid aesthetic directions while penalizing generic AI patterns. Minor deduction: the grader could be slightly more specific about what constitutes 'distinctive typography' to avoid subjectivity, and the task, while realistic, is somewhat common as a test scenario. Overall, it effectively tests whether the skill knowledge transfers to creating genuinely creative frontend work."
        },
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n**What domain knowledge does this skill teach?**\n- Design thinking process: understanding purpose, audience, and committing to a bold aesthetic direction before coding\n- Typography choices: avoiding generic fonts (Inter, Roboto, Arial) in favor of distinctive, characterful selections\n- Color theory: cohesive palettes with dominant colors and sharp accents rather than timid, evenly-distributed schemes\n- Motion design: purposeful animations with staggered reveals, scroll-triggered effects, and meaningful micro-interactions\n- Spatial composition: asymmetry, overlap, grid-breaking elements, deliberate use of negative space\n- Visual depth: textures, gradients, patterns, and atmospheric backgrounds instead of flat solid colors\n\n**What workflow decisions does it enable?**\n- When to use maximalist vs minimalist approaches (both valid when executed with intentionality)\n- How to match implementation complexity to aesthetic vision\n- Choosing to differentiate through memorable design elements vs defaulting to safe, generic patterns\n\n**What would a practitioner struggle with without this skill?**\n- Defaulting to generic, predictable designs that look like every other AI-generated interface\n- Using overused fonts and cliché color schemes (like purple gradients on white)\n- Missing opportunities for visual interest through motion, texture, and spatial composition\n- Not establishing a clear design direction before implementation\n\n```yaml\nname: distinctive-landing-page-design\nskills:\n  - frontend-design\nprompt: |\n  Create a landing page for a specialty coffee roastery called \"Ember & Stone\" that sources beans from small farms in Guatemala and Ethiopia. The page should include a hero section, a brief \"our story\" section, and a featured product highlight.\n  \n  Build this as a single HTML file with embedded CSS (and JS if needed for interactions). Make the design memorable and distinctive.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates intentional, distinctive frontend design rather than generic AI-generated aesthetics.\n    \n    Check for these qualities:\n    \n    1. **Clear Aesthetic Direction**: Does the design commit to a specific visual identity (e.g., minimalist editorial, rustic warmth, modern luxury, brutalist, etc.)? Is there evidence of intentional design thinking?\n    \n    2. **Typography Choices**: Does it use distinctive, characterful fonts rather than defaulting to generic system fonts or overused choices like Inter, Roboto, Arial, or Open Sans? Look for intentional font pairing.\n    \n    3. **Color & Theme**: Is there a cohesive color palette with dominant colors and accents? Does it avoid the cliché purple-gradient-on-white or generic blue schemes?\n    \n    4. **Visual Interest**: Does it include any of: meaningful animations/transitions, textures or gradients, interesting spatial composition (asymmetry, overlap, generous whitespace), or atmospheric backgrounds?\n    \n    5. **Contextual Appropriateness**: Does the design feel crafted specifically for a specialty coffee roastery rather than a generic template that could apply to anything?\n    \n    Score 1.0 if: The design demonstrates strong intentionality with a clear aesthetic direction, uses distinctive typography, has a cohesive and non-generic color scheme, includes visual interest through motion or texture, and feels specifically designed for this coffee brand.\n    \n    Score 0.5 if: The design shows some intentionality and avoids the most generic choices, but falls short in 2-3 areas (e.g., uses somewhat generic fonts, lacks meaningful animation, or has a forgettable color scheme).\n    \n    Score 0.0 if: The design uses generic fonts like Inter/Roboto/Arial, has a predictable or cliché color scheme, lacks any distinctive visual elements, and could be a template for any business.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (distinctive design, avoiding generic aesthetics, intentional choices) and creates a realistic scenario. However, it has moderate clarity issues: the grader relies heavily on subjective judgment of 'distinctive' and 'intentional' without concrete criteria, making it difficult to consistently distinguish between scores. The grader could reject valid minimalist approaches that are intentionally restrained, and the multi-factor rubric (5 criteria) makes the 0.5 threshold ambiguous. The non-obviousness is good (tests transferable design thinking, not trivia), and realism is excellent (landing pages are common tasks), but the grader's subjectivity and potential brittleness lower the overall score."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this frontend-design skill, I need to understand what it's really teaching:\n\n## Analysis\n\n1. **Domain Knowledge**: The skill teaches how to create distinctive, memorable frontend interfaces that avoid generic \"AI slop\" aesthetics. It emphasizes design thinking, intentional aesthetic choices, and attention to details like typography, motion, and spatial composition.\n\n2. **Workflow Decisions**: It enables choosing between different aesthetic extremes (minimal vs maximal, retro vs futuristic), selecting non-generic fonts and colors, and matching implementation complexity to the design vision.\n\n3. **Practitioner Struggles**: Without this skill, a developer would likely create forgettable, generic interfaces using overused patterns (Inter font, purple gradients, predictable layouts) and miss opportunities to create truly memorable user experiences.\n\n## Test Case\n\n```yaml\nname: portfolio-hero-distinctive-design\nskills:\n  - frontend-design\nprompt: |\n  Create a hero section for a creative developer's portfolio website. The developer specializes in experimental web experiences and wants their portfolio to immediately communicate their unique creative vision.\n  \n  Requirements:\n  - Include their name \"Alex Chen\"\n  - A tagline about creating experimental web experiences\n  - A call-to-action to view their work\n  - Should work as standalone HTML/CSS\n  \n  Make it memorable and avoid generic design patterns.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response creates a distinctive, memorable hero section that avoids generic AI design patterns.\n    \n    Score 1.0 if ALL of the following are met:\n    - Commits to a clear, distinctive aesthetic direction (not generic or safe)\n    - Uses creative, non-standard typography choices (avoids Inter, Roboto, Arial, system fonts)\n    - Implements unique visual elements (creative use of color, motion, layout, or effects)\n    - Code is functional and production-ready\n    - Overall design feels intentional and memorable, not like typical AI-generated interfaces\n    \n    Score 0.5 if:\n    - Shows some creative choices but falls back on some generic patterns\n    - OR has good aesthetic vision but implementation is incomplete or buggy\n    - OR uses some distinctive elements but lacks overall cohesion\n    \n    Score 0.0 if:\n    - Uses generic, predictable design patterns (standard fonts, purple gradients, centered layouts)\n    - OR provides only conceptual discussion without working code\n    - OR creates something that looks like typical AI-generated design\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (avoiding generic design, distinctive aesthetics) and creates a realistic scenario. However, the grader has moderate brittleness issues: it relies on subjective judgment of what constitutes 'distinctive' vs 'generic' design, and the criteria like 'avoids Inter, Roboto, Arial' could reject valid solutions that use these fonts creatively or in unexpected ways. The non-obviousness is good (tests transferable design thinking rather than trivia), and realism is excellent (portfolio hero sections are common real-world tasks). The main weakness is grader robustness - determining whether a design is 'memorable' or 'generic' requires nuanced aesthetic judgment that may be inconsistently applied."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: distinctive-single-file-landing-page-aesthetic-commitment\nskills:\n  - frontend-design\nprompt: |\n  Build a **single-file** (one HTML file) responsive landing page for a new product:\n\n  **Product:** “Tidepool” — a tiny, high-end weather app for sailors and surfers  \n  **Audience:** experienced ocean people; they care about clarity, craft, and trust (not cute)  \n  **Goal:** convert visitors into email signups for early access\n\n  ### Hard constraints\n  - Deliver **one self-contained HTML file** with embedded CSS + JS (no build tools).\n  - You may load **web fonts** (e.g., Google Fonts), but do **not** use generic fonts (no Inter, Roboto, Arial, system-ui stacks).\n  - No external images. If you need visuals, create them with CSS/SVG you write in the file.\n  - Must be accessible: semantic HTML, visible focus states, reasonable contrast, labels, and `prefers-reduced-motion` handling.\n\n  ### Content requirements (include all)\n  - Top nav with wordmark “Tidepool”, and links: Forecast, Safety, Pricing\n  - Hero section:\n    - Headline: “Read the water before it reads you.”\n    - Subcopy: “Hyperlocal swell, wind, tide, and hazard windows—built for people who actually go out.”\n    - Primary CTA button: “Request early access”\n    - Secondary CTA: “See a sample forecast”\n  - A “Sample forecast” panel (static demo UI) showing: Swell (ft), Period (s), Wind (kt), Tide (ft), and a small 12-hour timeline bar visualization.\n  - 3 feature blocks with short titles + 1–2 lines each\n  - Pricing teaser with two tiers (names + prices + 3 bullet points each)\n  - Email signup form (email input + submit). Use small JS to validate email and show an inline success state.\n  - Footer with small-print disclaimer (marine conditions change fast, etc.)\n\n  ### Design requirements (this is what’s being tested)\n  - Before the code, briefly state:\n    1) the **bold aesthetic direction** you chose (e.g., brutalist utilitarian, luxury nautical instruments, editorial, retro-futuristic radar, etc. — pick ONE and commit),\n    2) the one “**unforgettable**” signature detail (e.g., an instrument-like dial, a textured chart grid, a unique hover behavior, etc.).\n  - The implementation should look intentionally designed, not a generic SaaS template.\n  - Use CSS variables for theme consistency.\n  - Include at least one orchestrated motion moment (e.g., staggered entrance on load), but avoid motion overload and respect reduced-motion.\n  - Include at least one atmospheric background/detail (texture/noise, chart grid, gradient mesh, engraved borders, custom cursor, etc.) that matches the concept.\n\n  Output only the final HTML file contents (including CSS/JS inside it).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates production-grade frontend design with a distinctive, intentional aesthetic (not generic \"AI SaaS\").\n\n    Score 1.0 if ALL are true:\n    - Provides a single, complete HTML document with embedded CSS and (if used) JS; page is plausibly runnable as-is.\n    - Clearly states a bold aesthetic direction AND a specific “signature/unforgettable” detail, and the code actually implements that direction (not just described).\n    - Avoids generic fonts (no Inter, Roboto, Arial, or system-ui stacks) and instead uses a distinctive font pairing (e.g., display + text) loaded via @import or <link>.\n    - Uses CSS variables for palette/theme and shows cohesive color decisions (not default purple-on-white SaaS gradients).\n    - Implements the required sections/content: nav with specified links, hero with exact headline/subcopy and CTAs, sample forecast panel with the specified metrics + a timeline visualization, 3 feature blocks, pricing teaser with two tiers and bullets, email signup with JS validation + inline success state, and footer disclaimer.\n    - Includes at least one orchestrated motion sequence (e.g., staggered reveal) AND includes prefers-reduced-motion handling to disable/reduce animations.\n    - Demonstrates attention to craft: responsive layout, intentional spacing/typography, visible focus states, labeled form controls, and at least basic accessibility considerations.\n\n    Score 0.5 if:\n    - Mostly complete and runnable, but missing 1–2 key requirements (e.g., weak/unfinished sample forecast visualization, no prefers-reduced-motion, or design is only mildly distinctive), OR\n    - Uses a decent aesthetic but falls back into generic template patterns, OR\n    - Accessibility/responsiveness is present but incomplete.\n\n    Score 0.0 if:\n    - Not a single-file HTML solution, or missing major required sections/content, or\n    - Design is clearly generic/cookie-cutter (e.g., Inter/Roboto/system fonts, common purple gradient SaaS look) with no distinctive signature detail, or\n    - Lacks functional structure (no meaningful CSS, no working form behavior) such that it cannot be considered production-grade.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (distinctive aesthetics, avoiding generic AI patterns, production-grade implementation) and creates a realistic scenario. However, it has moderate clarity issues: the grader is somewhat brittle, requiring exact content matches ('Read the water before it reads you', specific nav links) rather than testing the transferable design knowledge. The test conflates content requirements with design assessment, making it partially a memorization test. The non-obviousness is good (tests aesthetic commitment and implementation quality), and realism is excellent (landing pages are common tasks), but the grader's specificity about exact wording and content structure reduces robustness."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**  \n- **Domain knowledge taught:** The skill teaches how to conceive frontend designs that are distinctive, production-grade, and far from generic AI-generated aesthetics. It emphasizes design thinking principles such as understanding purpose, tone, constraints, differentiation, and translating those into concrete aesthetic decisions (typography, color, spatial composition, motion, textures) plus execution details (animation, CSS variables, custom layouts).  \n- **Workflow decisions enabled:** It helps decide how to translate a brief into a cohesive creative direction: choosing when to go bold vs restrained, selecting typography pairs, determining whether to lean into maximalism (complex motion, layered textures) or refined minimalist restraint, and how to integrate visual details (grid-breaking layouts, dramatic backgrounds) while honoring constraints. It also informs when to invest in motion effects versus subtle interactions, and how to infuse the interface with memorable differentiation.  \n- **Practitioner struggles without this skill:** Without this skill, an LLM would likely revert to safe, generic UI designs with standard fonts, predictable gradients, overused layout patterns, and a lack of intentional aesthetic concept. It would struggle to articulate and implement a unique tone, cohesive color system, purposeful typography pairing, thoughtful motion, and sensory-rich details that align with a given context and user need.\n\n```yaml\nname: immersive-immersive-art-gallery-showcase\nskills:\n  - frontend-design\nprompt: |\n  You are designing a single-page promotional experience for “Nebula Atelier,” an immersive digital art gallery specializing in large-scale, responsive installations that blend light, sound, and sculpture. The target audience is adventurous collectors and curators who crave sensory depth and a feeling of entering another realm. The technical constraints are: use HTML/CSS/vanilla JS only, ensure the layout is responsive from 360px to 1440px, and keep performance optimized (no heavy libraries).  \n\n  Provide a complete working code snippet for the page, including:\n  - A bold tonal direction (e.g., retro-futuristic, brutalist, organic) that informs typography, colors, and textures.\n  - A hero section with an unforgettable focal point, atmospheric background/overlay, and a clear call to action.\n  - At least two distinct content sections (e.g., featured exhibition, visiting logistics) that break the grid with asymmetry or layered elements.\n  - Motion: a dramatic entrance animation or scroll-reveal for a key element plus a micro-interaction (hover, accent pulse, etc.).\n  - Custom typography choices (use Google Fonts or similar) and a cohesive CSS variable palette.\n  - A small detail (texture, grain layer, diagonal glow, decorative border) that reinforces the chosen atmosphere.\n\n  Explain in a short paragraph the conceptual direction, how the aesthetic choices connect to the gallery’s purpose, and what differentiates this layout from ordinary art promo pages.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates high-quality frontend design thinking and execution.\n\n    Score 1.0 if: The submission includes complete working HTML/CSS/JS code satisfying the constraints, articulates a bold aesthetic concept, and reflects it through typography, colors, spatial composition, and motion (entrance animation plus micro-interaction). There should be an atmospheric hero, at least two grid-breaking sections, CSS variables for the palette, and a unique detail (texture, decorative overlay, etc.), plus the descriptive paragraph explaining how the aesthetic supports the gallery’s purpose and differentiation.\n\n    Score 0.5 if: Most structure is present but one major expectation is missing (e.g., lacks the descriptive paragraph, motion is absent or minimal, or the visual language stays generic), yet the code still attempts a cohesive direction.\n\n    Score 0.0 if: The response lacks a cohesive aesthetic concept, misses multiple constraints (no motion, no unique texture/detail, no paragraph), or provides incomplete/non-working code that fails to render the described experience.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (bold aesthetic direction, typography, motion, spatial composition) and creates a realistic scenario. However, it has clarity issues: the grader requires 'complete working HTML/CSS/JS code' which is difficult to verify programmatically via model_judge, and the prompt is quite prescriptive (demanding specific elements like 'at least two distinct content sections'), which may reject valid alternative approaches that achieve the same design goals through different structural choices. The non-obviousness is good as it tests transferable design thinking rather than trivia, and realism is high as promotional gallery pages are common real-world tasks."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: frontend-design-editorial-404\nskills:\n  - frontend-design\nprompt: |\n  Create a \"404 Not Found\" page for a high-end luxury fashion brand called \"Éclat\". \n  \n  The design should feel like an avant-garde editorial magazine spread, not a standard technical error page. It needs to be visually striking, using sophisticated typography (mix of serif/sans-serif), asymmetrical spatial composition, and a bold aesthetic direction.\n  \n  Implement the solution as a single HTML file with embedded CSS/JS. Do not use placeholder images; use CSS shapes, typography, or creative layout to convey the visual impact.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Analyze the HTML/CSS code provided by the assistant. The goal is to verify if the model applied the \"frontend-design\" skill to create a distinct, high-quality aesthetic rather than a generic one.\n\n    **Evaluation Criteria:**\n\n    1.  **Typography & Aesthetic (40%):**\n        *   Does the code use distinctive typography (e.g., distinct font stacks like Didot, Playfair Display, Bodoni, or stark monospaced fonts) rather than generic system fonts (Arial, Roboto)?\n        *   Is there a clear \"Luxury/Editorial\" vibe (e.g., high contrast, large display text, generous whitespace, or elegant overlapping)?\n\n    2.  **Layout & Composition (30%):**\n        *   Does the layout demonstrate \"Spatial Composition\" as taught in the skill (asymmetry, grid-breaking, overlap, or diagonal flow)?\n        *   It should NOT look like a standard centered text column with a header and a home button.\n\n    3.  **Technical Execution (30%):**\n        *   Are CSS variables used for theming?\n        *   Are there animations/micro-interactions (e.g., hover effects, load animations)?\n        *   Is the visual result \"production-grade\" (polished spacing, no default browser styles)?\n\n    **Scoring:**\n    *   **Score 1.0**: The design is visually distinct, uses custom/imported fonts, employs asymmetrical or editorial layout techniques, and feels like a bespoke design. It avoids \"AI slop\" (generic rounded corners, standard blue buttons).\n    *   **Score 0.5**: The code is functional and uses some styling (e.g., a serif font), but the layout is generic (centered box) or lacks the \"avant-garde\" boldness requested.\n    *   **Score 0.0**: The response uses default browser styles, generic Bootstrap-like components, or is broken code.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong design: it tests transferable frontend design principles (typography, asymmetry, editorial aesthetics) rather than skill-specific trivia, making it non-obvious (8/10). The grader is mostly robust, evaluating actual design capabilities with clear criteria, though it could be slightly more specific about what constitutes 'asymmetry' or 'editorial' (7/10). The realism is excellent - creating a branded 404 page is a common real-world task that practitioners actually do (9/10). The main weakness is minor potential brittleness in the grader's subjective assessment of 'avant-garde' and 'luxury editorial' aesthetics, but overall this is a well-constructed test that measures genuine design skill application."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    It teaches the intersection of high-level UI/UX design theory and advanced CSS/Frontend implementation. Specifically, it covers typography hierarchy (choosing characterful fonts over system defaults), atmospheric styling (using noise, mesh gradients, and textures), and non-standard layout composition (breaking the grid, asymmetry, and intentional negative space).\n\n2.  **What workflow decisions does it enable?**\n    It enables the decision to commit to a specific, extreme aesthetic (e.g., Brutalist, Minimalist, or Maximalist) *before* writing code, ensuring that every subsequent choice (color palette, animation timing, font pairing) serves that single vision. It moves the LLM from \"generating a functional component\" to \"designing a brand-aligned experience.\"\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    The \"AI Slop\" effect: generating interfaces that look like every other generic SaaS landing page (Inter font, rounded blue buttons, white backgrounds, and standard cards). Without this skill, a model defaults to the safest, most common patterns found in its training data rather than creating something memorable and professionally \"designed.\"\n\n---\n\n```yaml\nname: boutique-industrial-aesthetic-landing\nskills:\n  - frontend-design\nprompt: |\n  Develop a high-fidelity landing page for 'VOLT & VOID', a boutique brand that manufactures hand-wired, analog modular synthesizers.\n\n  The brand aesthetic must be \"High-End Industrial Brutalist.\" \n  Requirements:\n  1. Avoid generic AI aesthetics: do NOT use Inter, Arial, or standard system fonts. Do NOT use a basic white-and-light-gray background.\n  2. Typography: Use a striking, characterful display font pairing.\n  3. Layout: Use an unconventional, grid-breaking layout that feels architectural.\n  4. Atmosphere: Implement at least one advanced visual detail such as a noise texture overlay, a mesh gradient, or a CSS-based \"raw metal\" texture.\n  5. Motion: Include a sophisticated entrance animation or a scroll-triggered micro-interaction that feels intentional and mechanical.\n\n  Provide the complete implementation in React (using Tailwind CSS or standard CSS).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response for design quality and adherence to the \"Industrial Brutalist\" aesthetic.\n\n    Score 1.0 if:\n    - The design completely avoids \"AI slop\" (no Inter/system fonts, no generic purple/blue gradients).\n    - It uses a bold, distinctive typography choice (e.g., a monospace, high-contrast serif, or architectural sans).\n    - The layout is non-traditional (uses asymmetry, overlapping elements, or unexpected spacing).\n    - It includes high-quality visual details (noise textures, CSS-based material effects, or grain).\n    - The code is production-grade and includes sophisticated CSS or Motion animations.\n\n    Score 0.5 if:\n    - The design is better than a basic template but still uses some common patterns (like standard card grids).\n    - The font choice is \"safe\" but slightly better than Arial.\n    - There is an attempt at \"Brutalism\" but it lacks the refined details (e.g., missing textures or motion).\n\n    Score 0.0 if:\n    - The result looks like a generic SaaS template (white background, blue buttons, Inter font).\n    - It ignores the \"Industrial Brutalist\" prompt.\n    - The code is non-functional or extremely minimal.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (avoiding AI slop, distinctive typography, atmospheric details) and creates a realistic scenario. However, it has moderate clarity issues: the grader uses subjective terms like 'bold' and 'sophisticated' without clear operationalization, and the 0.5 scoring criteria ('slightly better than Arial') is vague. The test also risks being somewhat brittle by requiring specific aesthetic judgments rather than testing whether the model applied the skill's methodology (choosing an aesthetic direction, implementing it with intentionality). The non-obviousness is good as it tests transferable design thinking rather than memorizing specific font names from the skill doc."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this frontend-design skill, I need to analyze what it teaches and design a test that measures whether having this skill helps create better frontend interfaces.\n\n## Skill Analysis\n\n**Core Knowledge**: This skill teaches creating distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. The key learnings are:\n\n1. **Design Thinking**: Choose a bold aesthetic direction (minimalist, maximalist, retro-futuristic, etc.) and commit to it intentionally\n2. **Anti-Pattern Awareness**: Avoid overused fonts (Inter, Roboto), purple gradients on white, predictable layouts, and generic AI aesthetics\n3. **Execution Excellence**: Match implementation complexity to aesthetic vision with attention to typography, color, motion, spatial composition, and visual details\n\n**What Practitioners Struggle With**: Without this skill, developers default to generic, cookie-cutter designs with predictable choices that lack distinctive character.\n\n**Test Target**: Can the LLM make distinctive, memorable design choices and avoid common AI aesthetic pitfalls?\n\nHere's my test case:\n\n```yaml\nname: creative-agency-portfolio-frontend\nskills:\n  - frontend-design\nprompt: |\n  Build a portfolio website for \"Neural Drift\" - a creative agency that specializes in AI-generated art and experimental digital experiences. The site should showcase their work and attract luxury brand clients.\n\n  Context:\n  - Target audience: Luxury fashion brands, tech companies, art collectors\n  - Need to convey: Cutting-edge creativity, experimental approach, premium quality\n  - Must avoid: Generic portfolio layouts, overused fonts, predictable design patterns\n\n  Create a complete, functional website that demonstrates exceptional design thinking and execution. Focus on creating something memorable and distinctive that stands out from typical agency portfolios.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates the frontend-design skill by creating a distinctive, production-grade interface that avoids generic AI aesthetics.\n\n    Score 1.0 if: \n    - Shows clear aesthetic vision and design thinking process\n    - Makes distinctive, non-generic design choices (fonts, colors, layout, visual elements)\n    - Avoids common AI pitfalls (no Inter/Roboto/Arial fonts, no purple gradients on white, no predictable layouts)\n    - Creates memorable, cohesive aesthetic that fits the luxury creative agency context\n    - Demonstrates production-grade code with attention to visual details\n    - Shows creative thinking beyond default patterns\n\n    Score 0.5 if:\n    - Shows some design awareness but makes predictable or generic choices\n    - Mostly avoids AI pitfalls but includes 1-2 common patterns\n    - Functional but lacks distinctive character or memorable elements\n    - Adequate execution but not exceptional\n\n    Score 0.0 if:\n    - Uses multiple generic AI aesthetic patterns (common fonts, purple gradients, predictable layouts)\n    - Shows no distinctive design vision or creative thinking\n    - Lacks production-quality execution\n    - Defaults to cookie-cutter agency portfolio design\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles (avoiding generic AI aesthetics, making distinctive choices) and creates a realistic scenario. However, the grader has moderate brittleness issues: it relies heavily on subjective judgment ('distinctive,' 'memorable') without concrete criteria, and the scoring thresholds are vague (what constitutes '1-2 common patterns' vs 'multiple'?). The test appropriately focuses on transferable design thinking rather than trivia, and the portfolio website scenario is genuinely realistic. The main weakness is grader robustness - different judges could reasonably score the same submission very differently due to ambiguous criteria."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: design-bold-aesthetic-frontend\nskills:\n  - frontend-design\nprompt: |\n  You are tasked with designing a landing page for a new luxury skincare brand. The brand aims to convey sophistication, elegance, and a sense of exclusivity. The target audience is affluent individuals who value high-quality, natural products.\n\n  **Requirements:**\n  - The landing page should feature a hero section with a large, high-resolution image of a product.\n  - Below the hero section, include a section that highlights the key features of the products (e.g., organic ingredients, cruelty-free, dermatologically tested).\n  - Add a testimonials section with a few quotes from satisfied customers.\n  - Include a call-to-action (CTA) button at the bottom of the page.\n\n  **Constraints:**\n  - Use React for the implementation.\n  - Ensure the page is responsive and accessible.\n  - The design should be visually striking and memorable, with a clear aesthetic point-of-view.\n  - Avoid generic AI-generated aesthetics.\n\n  Create the HTML, CSS, and React components for this landing page, focusing on the following aspects:\n  - Typography: Choose fonts that elevate the aesthetic.\n  - Color & Theme: Use a cohesive color palette that conveys luxury.\n  - Motion: Implement subtle animations to enhance user experience.\n  - Spatial Composition: Use unexpected layouts and spatial arrangements.\n  - Backgrounds & Visual Details: Add textures and effects that match the luxury theme.\n\n  Provide the complete code for the landing page.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n    \n    Score 1.0 if:\n    - The design is visually striking and memorable.\n    - The typography choices are unique and appropriate for the luxury theme.\n    - The color palette is cohesive and conveys luxury.\n    - Subtle animations are implemented effectively to enhance user experience.\n    - The spatial composition is unexpected and adds to the luxury feel.\n    - Backgrounds and textures are used to create a rich, atmospheric design.\n    - The code is production-grade, responsive, and accessible.\n    \n    Score 0.5 if:\n    - The design is generally good but lacks some of the above criteria.\n    - Some aspects of the design (typography, color, motion, spatial composition, backgrounds) are well-executed but others are lacking.\n    - The code is functional but may have minor issues.\n    \n    Score 0.0 if:\n    - The design is generic or fails to convey the luxury theme.\n    - The code is non-functional or significantly flawed.\n    - The response does not meet the given requirements.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case is realistic and tests genuine frontend design knowledge, but has significant clarity issues. The grader is too vague and subjective ('visually striking', 'unique typography', 'unexpected layouts') without concrete criteria for what constitutes these qualities. It would struggle to consistently evaluate responses since different judges could interpret 'luxury aesthetic' very differently. The test also somewhat contradicts the skill's emphasis on variety by prescribing a specific aesthetic (luxury skincare) rather than testing whether the model can choose and execute ANY bold aesthetic direction. The non-obviousness is moderate - it tests design thinking and execution rather than trivia, but the specific luxury skincare scenario is somewhat contrived rather than testing transferable design principles."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: create-unique-frontend-component\nskills:\n- frontend-design\nprompt: |\n  Create a production-grade frontend interface for a music streaming application. The interface should have a retro-futuristic aesthetic with a bold color scheme and striking typography. Consider the purpose of the application, the target audience, and any technical constraints. Implement working code (HTML/CSS/JS) that is visually striking, cohesive, and refined in every detail.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria:\n    Score 1.0 if: The submitted code implements a production!-grade frontend interface with a clear retro-futuristic aesthetic, bold color scheme, and striking typography. The code should be functional, cohesive, and meticulously refined in every detail.\n    Score 0.5 if: The submitted code demonstrates some understanding of the retro-futuristic aesthetic and attempts to implement a production-grade frontend interface, but lacks refinement or cohesion in some areas.\n    Score 0.0 if: The submitted code does not demonstrate an understanding of the retro-futuristic aesthetic or fails to implement a production-grade frontend interface.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. For non-obviousness (score: 3/10), it overly constrains the aesthetic to 'retro-futuristic' when the skill emphasizes choosing ANY bold direction - this tests compliance with arbitrary requirements rather than transferable design thinking. For clarity (score: 4/10), the grader is extremely brittle, requiring the specific 'retro-futuristic aesthetic' when valid solutions could use brutalist, art deco, or any other bold aesthetic from the skill's examples. For realism (score: 4/10), while building music streaming interfaces is realistic, mandating a specific aesthetic direction is contrived - real practitioners choose aesthetics based on brand/context, not arbitrary requirements. The test would reject perfectly valid solutions that demonstrate excellent design thinking but choose different aesthetic directions."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-docx",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 5,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** docx\n\n**SKILL.md Content:**\n```markdown\n---\nname: docx\ndescription: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)\n```\n\n## Additional Skill Files\n\n**ooxml.md:**\n```markdown\n# Office Open XML Technical Reference\n\n**Important: Read this entire document before starting.** This document covers:\n- [Technical Guidelines](#technical-guidelines) - Schema compliance rules and validation requirements\n- [Document Content Patterns](#document-content-patterns) - XML patterns for headings, lists, tables, formatting, etc.\n- [Document Library (Python)](#document-library-python) - Recommended approach for OOXML manipulation with automatic infrastructure setup\n- [Tracked Changes (Redlining)](#tracked-changes-redlining) - XML patterns for implementing tracked changes\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<w:pPr>`**: `<w:pStyle>`, `<w:numPr>`, `<w:spacing>`, `<w:ind>`, `<w:jc>`\n- **Whitespace**: Add `xml:space='preserve'` to `<w:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n  - **Character encoding reference**: Curly quotes `\"\"` become `&#8220;&#8221;`, apostrophe `'` becomes `&#8217;`, em-dash `—` becomes `&#8212;`\n- **Tracked changes**: Use `<w:del>` and `<w:ins>` tags with `w:author=\"Claude\"` outside `<w:r>` elements\n  - **Critical**: `<w:ins>` closes with `</w:ins>`, `<w:del>` closes with `</w:del>` - never mix\n  - **RSIDs must be 8-digit hex**: Use values like `00AB1234` (only 0-9, A-F characters)\n  - **trackRevisions placement**: Add `<w:trackRevisions/>` after `<w:proofState>` in settings.xml\n- **Images**: Add to `word/media/`, reference in `document.xml`, set dimensions to prevent overflow\n\n## Document Content Patterns\n\n### Basic Structure\n```xml\n<w:p>\n  <w:r><w:t>Text content</w:t></w:r>\n</w:p>\n```\n\n### Headings and Styles\n```xml\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Title\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Document Title</w:t></w:r>\n</w:p>\n\n<w:p>\n  <w:pPr><w:pStyle w:val=\"Heading2\"/></w:pPr>\n  <w:r><w:t>Section Heading</w:t></w:r>\n</w:p>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<w:r><w:rPr><w:b/><w:bCs/></w:rPr><w:t>Bold</w:t></w:r>\n<!-- Italic -->\n<w:r><w:rPr><w:i/><w:iCs/></w:rPr><w:t>Italic</w:t></w:r>\n<!-- Underline -->\n<w:r><w:rPr><w:u w:val=\"single\"/></w:rPr><w:t>Underlined</w:t></w:r>\n<!-- Highlight -->\n<w:r><w:rPr><w:highlight w:val=\"yellow\"/></w:rPr><w:t>Highlighted</w:t></w:r>\n```\n\n### Lists\n```xml\n<!-- Numbered list -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>First item</w:t></w:r>\n</w:p>\n\n<!-- Restart numbered list at 1 - use different numId -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"2\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>New list item 1</w:t></w:r>\n</w:p>\n\n<!-- Bullet list (level 2) -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"1\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n    <w:ind w:left=\"900\"/>\n  </w:pPr>\n  <w:r><w:t>Bullet item</w:t></w:r>\n</w:p>\n```\n\n### Tables\n```xml\n<w:tbl>\n  <w:tblPr>\n    <w:tblStyle w:val=\"TableGrid\"/>\n    <w:tblW w:w=\"0\" w:type=\"auto\"/>\n  </w:tblPr>\n  <w:tblGrid>\n    <w:gridCol w:w=\"4675\"/><w:gridCol w:w=\"4675\"/>\n  </w:tblGrid>\n  <w:tr>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 1</w:t></w:r></w:p>\n    </w:tc>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 2</w:t></w:r></w:p>\n    </w:tc>\n  </w:tr>\n</w:tbl>\n```\n\n### Layout\n```xml\n<!-- Page break before new section (common pattern) -->\n<w:p>\n  <w:r>\n    <w:br w:type=\"page\"/>\n  </w:r>\n</w:p>\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Heading1\"/>\n  </w:pPr>\n  <w:r>\n    <w:t>New Section Title</w:t>\n  </w:r>\n</w:p>\n\n<!-- Centered paragraph -->\n<w:p>\n  <w:pPr>\n    <w:spacing w:before=\"240\" w:after=\"0\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Centered text</w:t></w:r>\n</w:p>\n\n<!-- Font change - paragraph level (applies to all runs) -->\n<w:p>\n  <w:pPr>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n  </w:pPr>\n  <w:r><w:t>Monospace text</w:t></w:r>\n</w:p>\n\n<!-- Font change - run level (specific to this text) -->\n<w:p>\n  <w:r>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n    <w:t>This text is Courier New</w:t>\n  </w:r>\n  <w:r><w:t> and this text uses default font</w:t></w:r>\n</w:p>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`word/_rels/document.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/numbering\" Target=\"numbering.xml\"/>\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Override PartName=\"/word/numbering.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocess\n... (truncated)\n```\n\n**docx-js.md:**\n```markdown\n# DOCX Library Tutorial\n\nGenerate .docx files with JavaScript/TypeScript.\n\n**Important: Read this entire document before starting.** Critical formatting rules and common pitfalls are covered throughout - skipping sections may result in corrupted files or rendering issues.\n\n## Setup\nAssumes docx is already installed globally\nIf not installed: `npm install -g docx`\n\n```javascript\nconst { Document, Packer, Paragraph, TextRun, Table, TableRow, TableCell, ImageRun, Media, \n        Header, Footer, AlignmentType, PageOrientation, LevelFormat, ExternalHyperlink, \n        InternalHyperlink, TableOfContents, HeadingLevel, BorderStyle, WidthType, TabStopType, \n        TabStopPosition, UnderlineType, ShadingType, VerticalAlign, SymbolRun, PageNumber,\n        FootnoteReferenceRun, Footnote, PageBreak } = require('docx');\n\n// Create & Save\nconst doc = new Document({ sections: [{ children: [/* content */] }] });\nPacker.toBuffer(doc).then(buffer => fs.writeFileSync(\"doc.docx\", buffer)); // Node.js\nPacker.toBlob(doc).then(blob => { /* download logic */ }); // Browser\n```\n\n## Text & Formatting\n```javascript\n// IMPORTANT: Never use \\n for line breaks - always use separate Paragraph elements\n// ❌ WRONG: new TextRun(\"Line 1\\nLine 2\")\n// ✅ CORRECT: new Paragraph({ children: [new TextRun(\"Line 1\")] }), new Paragraph({ children: [new TextRun(\"Line 2\")] })\n\n// Basic text with all formatting options\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  spacing: { before: 200, after: 200 },\n  indent: { left: 720, right: 720 },\n  children: [\n    new TextRun({ text: \"Bold\", bold: true }),\n    new TextRun({ text: \"Italic\", italics: true }),\n    new TextRun({ text: \"Underlined\", underline: { type: UnderlineType.DOUBLE, color: \"FF0000\" } }),\n    new TextRun({ text: \"Colored\", color: \"FF0000\", size: 28, font: \"Arial\" }), // Arial default\n    new TextRun({ text: \"Highlighted\", highlight: \"yellow\" }),\n    new TextRun({ text: \"Strikethrough\", strike: true }),\n    new TextRun({ text: \"x2\", superScript: true }),\n    new TextRun({ text: \"H2O\", subScript: true }),\n    new TextRun({ text: \"SMALL CAPS\", smallCaps: true }),\n    new SymbolRun({ char: \"2022\", font: \"Symbol\" }), // Bullet •\n    new SymbolRun({ char: \"00A9\", font: \"Arial\" })   // Copyright © - Arial for symbols\n  ]\n})\n```\n\n## Styles & Professional Formatting\n\n```javascript\nconst doc = new Document({\n  styles: {\n    default: { document: { run: { font: \"Arial\", size: 24 } } }, // 12pt default\n    paragraphStyles: [\n      // Document title style - override built-in Title style\n      { id: \"Title\", name: \"Title\", basedOn: \"Normal\",\n        run: { size: 56, bold: true, color: \"000000\", font: \"Arial\" },\n        paragraph: { spacing: { before: 240, after: 120 }, alignment: AlignmentType.CENTER } },\n      // IMPORTANT: Override built-in heading styles by using their exact IDs\n      { id: \"Heading1\", name: \"Heading 1\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 32, bold: true, color: \"000000\", font: \"Arial\" }, // 16pt\n        paragraph: { spacing: { before: 240, after: 240 }, outlineLevel: 0 } }, // Required for TOC\n      { id: \"Heading2\", name: \"Heading 2\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 28, bold: true, color: \"000000\", font: \"Arial\" }, // 14pt\n        paragraph: { spacing: { before: 180, after: 180 }, outlineLevel: 1 } },\n      // Custom styles use your own IDs\n      { id: \"myStyle\", name: \"My Style\", basedOn: \"Normal\",\n        run: { size: 28, bold: true, color: \"000000\" },\n        paragraph: { spacing: { after: 120 }, alignment: AlignmentType.CENTER } }\n    ],\n    characterStyles: [{ id: \"myCharStyle\", name: \"My Char Style\",\n      run: { color: \"FF0000\", bold: true, underline: { type: UnderlineType.SINGLE } } }]\n  },\n  sections: [{\n    properties: { page: { margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 } } },\n    children: [\n      new Paragraph({ heading: HeadingLevel.TITLE, children: [new TextRun(\"Document Title\")] }), // Uses overridden Title style\n      new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Heading 1\")] }), // Uses overridden Heading1 style\n      new Paragraph({ style: \"myStyle\", children: [new TextRun(\"Custom paragraph style\")] }),\n      new Paragraph({ children: [\n        new TextRun(\"Normal with \"),\n        new TextRun({ text: \"custom char style\", style: \"myCharStyle\" })\n      ]})\n    ]\n  }]\n});\n```\n\n**Professional Font Combinations:**\n- **Arial (Headers) + Arial (Body)** - Most universally supported, clean and professional\n- **Times New Roman (Headers) + Arial (Body)** - Classic serif headers with modern sans-serif body\n- **Georgia (Headers) + Verdana (Body)** - Optimized for screen reading, elegant contrast\n\n**Key Styling Principles:**\n- **Override built-in styles**: Use exact IDs like \"Heading1\", \"Heading2\", \"Heading3\" to override Word's built-in heading styles\n- **HeadingLevel constants**: `HeadingLevel.HEADING_1` uses \"Heading1\" style, `HeadingLevel.HEADING_2`\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - docx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** docx\n\n**SKILL.md Content:**\n```markdown\n---\nname: docx\ndescription: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)\n```\n\n## Additional Skill Files\n\n**ooxml.md:**\n```markdown\n# Office Open XML Technical Reference\n\n**Important: Read this entire document before starting.** This document covers:\n- [Technical Guidelines](#technical-guidelines) - Schema compliance rules and validation requirements\n- [Document Content Patterns](#document-content-patterns) - XML patterns for headings, lists, tables, formatting, etc.\n- [Document Library (Python)](#document-library-python) - Recommended approach for OOXML manipulation with automatic infrastructure setup\n- [Tracked Changes (Redlining)](#tracked-changes-redlining) - XML patterns for implementing tracked changes\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<w:pPr>`**: `<w:pStyle>`, `<w:numPr>`, `<w:spacing>`, `<w:ind>`, `<w:jc>`\n- **Whitespace**: Add `xml:space='preserve'` to `<w:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n  - **Character encoding reference**: Curly quotes `\"\"` become `&#8220;&#8221;`, apostrophe `'` becomes `&#8217;`, em-dash `—` becomes `&#8212;`\n- **Tracked changes**: Use `<w:del>` and `<w:ins>` tags with `w:author=\"Claude\"` outside `<w:r>` elements\n  - **Critical**: `<w:ins>` closes with `</w:ins>`, `<w:del>` closes with `</w:del>` - never mix\n  - **RSIDs must be 8-digit hex**: Use values like `00AB1234` (only 0-9, A-F characters)\n  - **trackRevisions placement**: Add `<w:trackRevisions/>` after `<w:proofState>` in settings.xml\n- **Images**: Add to `word/media/`, reference in `document.xml`, set dimensions to prevent overflow\n\n## Document Content Patterns\n\n### Basic Structure\n```xml\n<w:p>\n  <w:r><w:t>Text content</w:t></w:r>\n</w:p>\n```\n\n### Headings and Styles\n```xml\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Title\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Document Title</w:t></w:r>\n</w:p>\n\n<w:p>\n  <w:pPr><w:pStyle w:val=\"Heading2\"/></w:pPr>\n  <w:r><w:t>Section Heading</w:t></w:r>\n</w:p>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<w:r><w:rPr><w:b/><w:bCs/></w:rPr><w:t>Bold</w:t></w:r>\n<!-- Italic -->\n<w:r><w:rPr><w:i/><w:iCs/></w:rPr><w:t>Italic</w:t></w:r>\n<!-- Underline -->\n<w:r><w:rPr><w:u w:val=\"single\"/></w:rPr><w:t>Underlined</w:t></w:r>\n<!-- Highlight -->\n<w:r><w:rPr><w:highlight w:val=\"yellow\"/></w:rPr><w:t>Highlighted</w:t></w:r>\n```\n\n### Lists\n```xml\n<!-- Numbered list -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>First item</w:t></w:r>\n</w:p>\n\n<!-- Restart numbered list at 1 - use different numId -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"2\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>New list item 1</w:t></w:r>\n</w:p>\n\n<!-- Bullet list (level 2) -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"1\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n    <w:ind w:left=\"900\"/>\n  </w:pPr>\n  <w:r><w:t>Bullet item</w:t></w:r>\n</w:p>\n```\n\n### Tables\n```xml\n<w:tbl>\n  <w:tblPr>\n    <w:tblStyle w:val=\"TableGrid\"/>\n    <w:tblW w:w=\"0\" w:type=\"auto\"/>\n  </w:tblPr>\n  <w:tblGrid>\n    <w:gridCol w:w=\"4675\"/><w:gridCol w:w=\"4675\"/>\n  </w:tblGrid>\n  <w:tr>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 1</w:t></w:r></w:p>\n    </w:tc>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 2</w:t></w:r></w:p>\n    </w:tc>\n  </w:tr>\n</w:tbl>\n```\n\n### Layout\n```xml\n<!-- Page break before new section (common pattern) -->\n<w:p>\n  <w:r>\n    <w:br w:type=\"page\"/>\n  </w:r>\n</w:p>\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Heading1\"/>\n  </w:pPr>\n  <w:r>\n    <w:t>New Section Title</w:t>\n  </w:r>\n</w:p>\n\n<!-- Centered paragraph -->\n<w:p>\n  <w:pPr>\n    <w:spacing w:before=\"240\" w:after=\"0\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Centered text</w:t></w:r>\n</w:p>\n\n<!-- Font change - paragraph level (applies to all runs) -->\n<w:p>\n  <w:pPr>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n  </w:pPr>\n  <w:r><w:t>Monospace text</w:t></w:r>\n</w:p>\n\n<!-- Font change - run level (specific to this text) -->\n<w:p>\n  <w:r>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n    <w:t>This text is Courier New</w:t>\n  </w:r>\n  <w:r><w:t> and this text uses default font</w:t></w:r>\n</w:p>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`word/_rels/document.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/numbering\" Target=\"numbering.xml\"/>\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Override PartName=\"/word/numbering.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocess\n... (truncated)\n```\n\n**docx-js.md:**\n```markdown\n# DOCX Library Tutorial\n\nGenerate .docx files with JavaScript/TypeScript.\n\n**Important: Read this entire document before starting.** Critical formatting rules and common pitfalls are covered throughout - skipping sections may result in corrupted files or rendering issues.\n\n## Setup\nAssumes docx is already installed globally\nIf not installed: `npm install -g docx`\n\n```javascript\nconst { Document, Packer, Paragraph, TextRun, Table, TableRow, TableCell, ImageRun, Media, \n        Header, Footer, AlignmentType, PageOrientation, LevelFormat, ExternalHyperlink, \n        InternalHyperlink, TableOfContents, HeadingLevel, BorderStyle, WidthType, TabStopType, \n        TabStopPosition, UnderlineType, ShadingType, VerticalAlign, SymbolRun, PageNumber,\n        FootnoteReferenceRun, Footnote, PageBreak } = require('docx');\n\n// Create & Save\nconst doc = new Document({ sections: [{ children: [/* content */] }] });\nPacker.toBuffer(doc).then(buffer => fs.writeFileSync(\"doc.docx\", buffer)); // Node.js\nPacker.toBlob(doc).then(blob => { /* download logic */ }); // Browser\n```\n\n## Text & Formatting\n```javascript\n// IMPORTANT: Never use \\n for line breaks - always use separate Paragraph elements\n// ❌ WRONG: new TextRun(\"Line 1\\nLine 2\")\n// ✅ CORRECT: new Paragraph({ children: [new TextRun(\"Line 1\")] }), new Paragraph({ children: [new TextRun(\"Line 2\")] })\n\n// Basic text with all formatting options\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  spacing: { before: 200, after: 200 },\n  indent: { left: 720, right: 720 },\n  children: [\n    new TextRun({ text: \"Bold\", bold: true }),\n    new TextRun({ text: \"Italic\", italics: true }),\n    new TextRun({ text: \"Underlined\", underline: { type: UnderlineType.DOUBLE, color: \"FF0000\" } }),\n    new TextRun({ text: \"Colored\", color: \"FF0000\", size: 28, font: \"Arial\" }), // Arial default\n    new TextRun({ text: \"Highlighted\", highlight: \"yellow\" }),\n    new TextRun({ text: \"Strikethrough\", strike: true }),\n    new TextRun({ text: \"x2\", superScript: true }),\n    new TextRun({ text: \"H2O\", subScript: true }),\n    new TextRun({ text: \"SMALL CAPS\", smallCaps: true }),\n    new SymbolRun({ char: \"2022\", font: \"Symbol\" }), // Bullet •\n    new SymbolRun({ char: \"00A9\", font: \"Arial\" })   // Copyright © - Arial for symbols\n  ]\n})\n```\n\n## Styles & Professional Formatting\n\n```javascript\nconst doc = new Document({\n  styles: {\n    default: { document: { run: { font: \"Arial\", size: 24 } } }, // 12pt default\n    paragraphStyles: [\n      // Document title style - override built-in Title style\n      { id: \"Title\", name: \"Title\", basedOn: \"Normal\",\n        run: { size: 56, bold: true, color: \"000000\", font: \"Arial\" },\n        paragraph: { spacing: { before: 240, after: 120 }, alignment: AlignmentType.CENTER } },\n      // IMPORTANT: Override built-in heading styles by using their exact IDs\n      { id: \"Heading1\", name: \"Heading 1\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 32, bold: true, color: \"000000\", font: \"Arial\" }, // 16pt\n        paragraph: { spacing: { before: 240, after: 240 }, outlineLevel: 0 } }, // Required for TOC\n      { id: \"Heading2\", name: \"Heading 2\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 28, bold: true, color: \"000000\", font: \"Arial\" }, // 14pt\n        paragraph: { spacing: { before: 180, after: 180 }, outlineLevel: 1 } },\n      // Custom styles use your own IDs\n      { id: \"myStyle\", name: \"My Style\", basedOn: \"Normal\",\n        run: { size: 28, bold: true, color: \"000000\" },\n        paragraph: { spacing: { after: 120 }, alignment: AlignmentType.CENTER } }\n    ],\n    characterStyles: [{ id: \"myCharStyle\", name: \"My Char Style\",\n      run: { color: \"FF0000\", bold: true, underline: { type: UnderlineType.SINGLE } } }]\n  },\n  sections: [{\n    properties: { page: { margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 } } },\n    children: [\n      new Paragraph({ heading: HeadingLevel.TITLE, children: [new TextRun(\"Document Title\")] }), // Uses overridden Title style\n      new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Heading 1\")] }), // Uses overridden Heading1 style\n      new Paragraph({ style: \"myStyle\", children: [new TextRun(\"Custom paragraph style\")] }),\n      new Paragraph({ children: [\n        new TextRun(\"Normal with \"),\n        new TextRun({ text: \"custom char style\", style: \"myCharStyle\" })\n      ]})\n    ]\n  }]\n});\n```\n\n**Professional Font Combinations:**\n- **Arial (Headers) + Arial (Body)** - Most universally supported, clean and professional\n- **Times New Roman (Headers) + Arial (Body)** - Classic serif headers with modern sans-serif body\n- **Georgia (Headers) + Verdana (Body)** - Optimized for screen reading, elegant contrast\n\n**Key Styling Principles:**\n- **Override built-in styles**: Use exact IDs like \"Heading1\", \"Heading2\", \"Heading3\" to override Word's built-in heading styles\n- **HeadingLevel constants**: `HeadingLevel.HEADING_1` uses \"Heading1\" style, `HeadingLevel.HEADING_2`\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - docx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "docx",
          "skill_dir": "docx",
          "skill_file_tree": "LICENSE.txt\nSKILL.md\ndocx-js.md\nooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/pml.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "additional_file_names": [
            "ooxml.md",
            "docx-js.md"
          ]
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n**1. What domain knowledge does this skill teach?**\n- Hook events and when each fires (PreToolUse, Stop, SessionStart, etc.)\n- Two hook types: prompt-based (LLM reasoning) vs command (bash scripts)\n- Configuration format differences between plugin hooks.json (wrapped) vs settings (direct)\n- Hook output formats for different decisions (allow/deny/block)\n- Environment variables like `$CLAUDE_PLUGIN_ROOT` for portability\n- Matchers for targeting specific tools (regex, wildcard, pipe-separated)\n\n**2. What workflow decisions does it enable?**\n- Choosing the right hook event for a validation scenario\n- When to use prompt-based hooks (complex reasoning) vs command hooks (deterministic, fast)\n- How to structure output to allow, deny, or request user confirmation\n- Understanding parallel execution implications\n\n**3. What would a practitioner struggle with without this skill?**\n- Knowing which hook event to use (e.g., PreToolUse to block operations vs Stop to verify completeness)\n- Understanding the wrapper format required for plugin hooks.json vs direct format for settings\n- Properly returning structured JSON to influence Claude's behavior (deny vs block vs ask)\n\n## Test Case\n\n```yaml\nname: hook-event-selection-and-configuration\nskills:\n  - Hook Development\nprompt: |\n  I'm building a Claude Code plugin and need to implement two safety features:\n\n  1. Before any file write or edit operation, I want to block writes to files in a `config/` directory unless the user explicitly confirms. The check should use LLM reasoning to understand context.\n\n  2. When Claude is about to finish a task, I want to verify that if any TypeScript files were modified, the build still succeeds. This should be a fast deterministic check using a bash script located in my plugin's scripts folder.\n\n  Write the hooks.json configuration for my plugin that implements both of these features. Explain which hook events you chose and why.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly designs Claude Code plugin hooks for the described requirements.\n\n    **Requirement 1 (Block config/ writes with confirmation):**\n    - Should use PreToolUse hook event (fires before tool execution)\n    - Should match Write and/or Edit tools (e.g., matcher like \"Write|Edit\" or \"Write\" and \"Edit\")\n    - Should use a prompt-based hook (type: \"prompt\") since LLM reasoning was requested\n    - Output should mention asking for confirmation (permissionDecision: \"ask\" or similar concept)\n\n    **Requirement 2 (Verify build on task completion):**\n    - Should use Stop hook event (fires when agent considers stopping)\n    - Should use a command-based hook (type: \"command\") since fast/deterministic was requested\n    - Should reference the script using ${CLAUDE_PLUGIN_ROOT} or similar portable path variable\n    - Should be able to block completion if build fails\n\n    **Configuration format:**\n    - For plugin hooks.json, should use the wrapper format with a \"hooks\" key containing the event configurations\n    - Should have proper JSON structure with matchers and hooks arrays\n\n    Score 1.0 if: Both hooks use correct events (PreToolUse and Stop), correct hook types (prompt for #1, command for #2), proper plugin wrapper format with \"hooks\" key, and uses portable path variable for the script.\n    \n    Score 0.5 if: Gets the hook events correct OR the hook types correct, but makes errors on the other aspects, or uses settings format instead of plugin format.\n    \n    Score 0.0 if: Uses wrong hook events (e.g., PostToolUse instead of PreToolUse, or SessionStart instead of Stop), or fundamentally misunderstands when hooks execute.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively evaluates transferable hook development knowledge: choosing appropriate hook events (PreToolUse vs Stop), selecting hook types based on requirements (prompt-based for reasoning vs command for deterministic checks), and understanding configuration format differences. The grader is robust and accepts multiple valid approaches (e.g., 'Write|Edit' or separate matchers). The scenario is realistic - practitioners commonly need to validate operations and verify completion criteria. Minor deduction: the test could be slightly more explicit about accepting variations in matcher syntax and JSON structure details, though the current grader is reasonably flexible."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze the skill:\n\n1. **What domain knowledge does this skill teach?**\n   - Event-driven automation concepts and lifecycle hooks\n   - Validation and policy enforcement patterns\n   - The distinction between prompt-based (LLM-driven, flexible) and command-based (deterministic, scripted) hooks\n   - Hook output contracts and decision formats\n   - Security patterns for input validation\n\n2. **What workflow decisions does it enable?**\n   - Choosing the right hook event (PreToolUse vs PostToolUse vs Stop vs SessionStart, etc.)\n   - Deciding between prompt-based hooks (flexible reasoning) vs command hooks (deterministic checks)\n   - Structuring hook outputs correctly (permission decisions, system messages, exit codes)\n   - Understanding when hooks load and execute (session start, parallel execution)\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Knowing which hook event to use for a specific validation need (e.g., \"prevent Claude from stopping before tests are run\")\n   - Understanding the output format required for different hook types\n   - Choosing between prompt-based and command-based approaches\n   - Understanding hook lifecycle (when changes take effect)\n\nThe realistic scenario: A practitioner wants to enforce a policy (like \"don't finish until tests pass\") but doesn't know which hook event handles that or what output format to return.\n\n```yaml\nname: hook-event-selection-for-validation\nskills:\n  - Hook Development\nprompt: |\n  You're building a Claude Code plugin that needs to ensure Claude doesn't consider a task complete until tests have been run successfully. \n  \n  Your goal: Configure a hook that intercepts when Claude tries to stop/finish, checks if tests were run, and either allows Claude to stop (if tests passed) or blocks it from stopping (if tests weren't run or failed).\n  \n  Provide:\n  1. Which hook event should you use for this requirement?\n  2. Should you use a prompt-based or command-based hook, and why?\n  3. Write a sample hook configuration (JSON format) that would accomplish this. Show the complete hook event entry.\n  4. What output format should this hook return to either allow or block Claude from stopping?\n  \n  Focus on the conceptual approach rather than implementation details.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of hook event selection and validation patterns.\n    \n    Score 1.0 if the response:\n    - Correctly identifies the \"Stop\" hook event (or SubagentStop for subagent contexts) as the appropriate choice\n    - Provides reasonable justification for hook type selection (prompt-based is recommended for flexible reasoning, command-based for deterministic checks - either is acceptable with justification)\n    - Shows a hook configuration with proper structure (matcher, hooks array, type, and either prompt or command)\n    - Correctly describes the decision output format with \"approve\" or \"block\" decision field\n    - Demonstrates understanding that the hook validates completion criteria before allowing stop\n    \n    Score 0.5 if the response:\n    - Identifies Stop hook but has minor errors in configuration structure\n    - Shows understanding of validation concept but incomplete output format\n    - Provides a working approach but misses some details (like decision field names)\n    - Confuses Stop with PreToolUse but still shows understanding of validation flow\n    \n    Score 0.0 if the response:\n    - Suggests wrong hook events that don't handle stop decisions (like SessionStart, PostToolUse without Stop)\n    - Shows no understanding of hook output format or decision structures\n    - Provides configuration that wouldn't validate completeness\n    - Demonstrates fundamental misunderstanding of when hooks execute\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively evaluates transferable knowledge about event-driven validation patterns and hook lifecycle concepts rather than trivia. It tests understanding of when different hook events fire, output format contracts, and validation flow - all generalizable concepts. The grader is robust, accepting both prompt-based and command-based approaches with justification, and doesn't require exact syntax. The scenario is realistic (enforcing test completion before task finish). Minor deduction: the test could be slightly more open-ended to avoid guiding toward specific hook names, though the core evaluation remains concept-focused rather than memorization-based."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me first analyze what it actually teaches:\n\n## Analysis\n\n**Core Domain Knowledge:**\n1. Hook events and their purposes (PreToolUse for validation before execution, PostToolUse for reactions, Stop for completion checks, SessionStart for initialization, etc.)\n2. Two hook types with different trade-offs: prompt-based hooks (flexible, context-aware, LLM reasoning) vs command hooks (deterministic, performance-critical, bash-based)\n3. Configuration format differences: plugin hooks.json uses a wrapper structure vs settings uses direct format\n4. Hook lifecycle: loaded at session start, requires session restart to reload changes\n5. Security considerations: input validation, path traversal checks, variable quoting in bash\n6. Matcher patterns for filtering which tools trigger hooks\n\n**Key Practitioner Decisions:**\n- Choosing the right hook event for a use case (validation before vs after vs on completion?)\n- Deciding prompt vs command hooks (do I need flexible LLM reasoning or fast deterministic checks?)\n- Understanding the restart requirement (why hooks don't update live)\n- Implementing secure validation (how to safely parse inputs in bash)\n\n**What would practitioners struggle with:**\nSomeone might know hooks exist but not understand when to use PreToolUse vs PostToolUse, or when a prompt hook is better than a command hook, or that they need to restart the session for changes to take effect.\n\nNow I'll write a test that measures this knowledge:\n\n```yaml\nname: choose-hook-strategy-for-security-use-case\nskills:\n  - Hook Development\nprompt: |\n  You're implementing a Claude Code plugin for a team that needs to prevent developers from accidentally deleting important files or accessing sensitive directories like .env, .credentials, config/, and /etc.\n\n  Your task is to design a hook strategy that:\n  1. Identifies which hook event(s) should intercept this use case\n  2. Recommends whether to use prompt-based or command-based hooks (and why)\n  3. Explains how developers would need to deploy this (what they need to restart, if anything)\n  4. Describes at least one security consideration for validating the dangerous operations\n\n  Provide a brief but complete explanation for each point.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of hook development principles for a security use case.\n\n    Score 1.0 if the response:\n    - Correctly identifies PreToolUse as the primary hook event (validation must happen BEFORE the tool executes)\n    - Explains that command-based hooks are more appropriate here because you need fast, deterministic validation of paths (not complex reasoning)\n    - Notes that developers must restart the Claude Code session for hook configuration changes to take effect\n    - Describes at least one concrete security concern like path traversal detection (../ in paths), checking for sensitive file names, or validating against a blocklist\n\n    Score 0.5 if the response:\n    - Identifies a hook event but may confuse PreToolUse with PostToolUse or Stop\n    - Recommends hooks but doesn't clearly justify the prompt vs command choice\n    - Mentions deployment but doesn't explain the restart requirement\n    - Discusses security only in vague terms or with weak examples\n\n    Score 0.0 if the response:\n    - Chooses wrong hook events entirely (e.g., SessionStart or Notification for this use case)\n    - Provides no reasoning about prompt vs command hooks\n    - Doesn't acknowledge deployment/restart considerations\n    - Ignores security considerations\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hook development principles by testing transferable knowledge about when to use PreToolUse vs PostToolUse, prompt vs command hooks, and session restart requirements. The grader is robust and accepts multiple valid approaches while focusing on conceptual understanding rather than memorization. However, the test could be slightly more realistic - while security validation is a genuine use case, the specific scenario feels somewhat constructed for testing purposes rather than emerging from a natural practitioner workflow. The grader appropriately awards partial credit for understanding some but not all concepts, making it fair and well-calibrated."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze the Hook Development skill:\n\n1. **What domain knowledge does this skill teach?**\n   - Event-driven automation through hooks in development environments\n   - Two hook paradigms: prompt-based (LLM-driven, flexible) vs command (deterministic, fast)\n   - Hook event lifecycle (PreToolUse, PostToolUse, Stop, SessionStart, etc.) and their specific purposes\n   - Security validation patterns for file operations and command execution\n   - Output format requirements for different hook types (decision objects, exit codes)\n   - Performance considerations with parallel execution\n\n2. **What workflow decisions does it enable?**\n   - Choosing between prompt-based hooks for complex reasoning vs command hooks for deterministic checks\n   - Selecting appropriate hook events for different automation needs\n   - Structuring validation logic to prevent dangerous operations\n   - Designing hooks for independence since they run in parallel\n   - Understanding when hooks can approve, deny, or ask for permission\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Not knowing which hook type fits their use case (e.g., using slow command hooks for complex logic instead of prompt-based)\n   - Implementing the wrong output format for hook decisions\n   - Not understanding that PreToolUse can block operations while PostToolUse is reactive\n   - Missing critical security validations for file operations\n   - Not knowing hooks load at session start and can't be hot-swapped\n\n## Test Case\n\n```yaml\nname: hook-implementation-strategy\nskills:\n- Hook Development\nprompt: |\n  You're developing a Claude Code plugin for a team that wants to prevent accidental damage to their codebase. They need the following protections:\n\n  1. Before any file write operations, check if the file path contains sensitive patterns like \".env\", \"credentials\", or system directories\n  2. After code edits, analyze the changes to detect potential syntax errors or breaking changes and provide feedback\n  3. When Claude thinks it's finished with a task, verify that all tests have been run before allowing it to stop\n  4. When a session starts, load project-specific context from a configuration file\n\n  For each protection requirement above, specify:\n  - Which hook event to use\n  - Whether to use a prompt-based or command hook (and why)\n  - The key output format requirement for that hook type\n\n  Focus on making the right architectural decisions for maintainability and performance.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the response correctly identifies hook implementation strategies for each requirement.\n\n    Score 1.0 if the response correctly identifies ALL of the following:\n    - Requirement 1: PreToolUse hook, can use either prompt-based (for complex pattern matching) or command (for fast deterministic checks), must output permissionDecision: allow/deny\n    - Requirement 2: PostToolUse hook, prompt-based (for analyzing code changes contextually), systemMessage for feedback\n    - Requirement 3: Stop hook, prompt-based (for evaluating test completion contextually), must output decision: approve/block\n    - Requirement 4: SessionStart hook, command hook (for file operations), can persist env vars via CLAUDE_ENV_FILE\n\n    Score 0.5 if the response gets the hook events correct but misses some implementation details like:\n    - Wrong hook type choices without good justification\n    - Missing or incorrect output format requirements\n    - Gets 3 out of 4 requirements fully correct\n\n    Score 0.0 if the response:\n    - Uses wrong hook events (e.g., PostToolUse for prevention instead of PreToolUse)\n    - Fundamentally misunderstands hook purposes\n    - Gets fewer than 3 requirements correct\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hook development concepts by testing architectural decision-making rather than trivia. It correctly evaluates knowledge of hook event selection (PreToolUse for prevention, PostToolUse for reaction, Stop for validation, SessionStart for setup), hook type trade-offs (prompt vs command), and output format requirements. The grading rubric is mostly robust, accepting valid alternatives (e.g., either prompt or command for requirement 1 with justification). However, there's minor brittleness: the rubric could be more explicit about accepting alternative valid approaches (e.g., PreToolUse with prompt-based hooks for requirement 1 is actually preferred per the skill document's recommendations, but the rubric treats command hooks as equally valid without emphasizing this). The task is realistic—teams genuinely need to implement such protections. Overall, this is a high-quality test with only minor clarity issues in the grading criteria."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (for test authoring)\n# 1) Domain knowledge: Claude Code hook system as event-driven automation; choosing prompt vs command hooks;\n#    correct config shapes for plugin vs user settings; matchers; hook IO/exit codes; env vars like\n#    ${CLAUDE_PLUGIN_ROOT} and $CLAUDE_ENV_FILE; security patterns (input validation, quoting, path safety);\n#    lifecycle limitations (hooks load at session start; parallel execution implications).\n# 2) Workflow decisions: when to use prompt-based reasoning vs deterministic bash validation; how to structure\n#    a portable plugin hook configuration; how to block dangerous actions pre-execution and enforce completion at Stop.\n# 3) What practitioners struggle with: mixing up hooks.json wrapper vs settings format, forgetting portability vars,\n#    returning wrong output schema, unsafe shell parsing, assuming hook ordering, and expecting hot reload.\n\nname: safe-ops-plugin-hooks-pretooluse-stop-sessionstart\nskills:\n  - Hook Development\nprompt: |\n  You’re building a Claude Code plugin called \"safe-ops\" for a team repo. Create the hook configuration and one hook script skeleton.\n\n  Requirements:\n  1) This is a *plugin* (config file is hooks/hooks.json). Include the correct plugin hooks.json wrapper format.\n  2) PreToolUse:\n     - For the Bash tool: run a *command hook* that deterministically blocks dangerous shell commands.\n       Block if the bash command contains any of:\n         - \"rm -rf /\" (or \"rm -rf /*\")\n         - \"curl ... | sh\" or \"wget ... | sh\"\n       Otherwise allow.\n     - For Write or Edit tools: use a *prompt hook* that reviews the tool input for path traversal (\"..\"),\n       writes to sensitive files like \".env\" or SSH keys, or obvious credential material. It should return allow/deny.\n  3) SessionStart: run a command hook that executes a project context loader script using a *portable path* (no hardcoded absolute paths).\n     Also show (in the script snippet or explanation) how it could persist an environment variable using $CLAUDE_ENV_FILE.\n  4) Stop: use a prompt hook that blocks stopping if the agent didn’t run tests *when the project looks like Node.js*\n     (assume Node.js if a package.json exists). It should return approve/block with a reason.\n  5) Provide:\n     - The complete hooks/hooks.json content\n     - A skeleton for scripts/validate-bash.sh that reads stdin JSON, validates safely, and returns a blocking error appropriately\n     - One or two sentences on how you would debug/test these hooks, and any important lifecycle caveat(s)\n\n  Keep it concise but correct.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates correct, practitioner-usable knowledge of Claude Code plugin hook development.\n\n    Score 1.0 if the response includes ALL of:\n    - A valid plugin hooks/hooks.json structure using the wrapper format with a required top-level \"hooks\" object (not the settings format).\n    - PreToolUse configuration that:\n      * Matches Bash via matcher (e.g., \"Bash\") and uses a command hook (not prompt) to validate dangerous commands.\n      * Matches Write|Edit via matcher and uses a prompt hook that checks for traversal (\"..\"), sensitive files (.env/SSH keys),\n        and credential-like content; indicates allow/deny decision (either via described JSON output shape or clear hook instructions).\n    - Uses ${CLAUDE_PLUGIN_ROOT} for command hook paths (portability) and includes reasonable timeouts or acknowledges defaults.\n    - SessionStart includes a command hook invoking a script via ${CLAUDE_PLUGIN_ROOT}, and mentions/presents persisting env vars\n      via $CLAUDE_ENV_FILE.\n    - Stop includes a prompt hook that enforces completeness by blocking if tests weren’t run when Node.js is detected\n      (package.json existence). It must specify approve/block decision semantics (explicitly or via example JSON).\n    - scripts/validate-bash.sh skeleton shows secure patterns:\n      * Reads JSON from stdin, extracts fields with jq, uses set -euo pipefail, quotes variables,\n        checks for the specified dangerous patterns, and blocks with exit code 2 and error JSON or stderr message.\n    - Mentions at least one realistic debugging/testing step (e.g., claude --debug, echo JSON into script, jq validate output)\n      AND at least one lifecycle caveat (hooks load at session start; restart required; parallel execution means no ordering assumptions).\n\n    Score 0.5 if it is mostly correct but missing ONE major element above (e.g., forgets the plugin wrapper, or omits $CLAUDE_ENV_FILE,\n    or Stop hook lacks approve/block semantics, or the bash script lacks exit-code-2 blocking behavior), while still demonstrating\n    generally correct hook concepts.\n\n    Score 0.0 if it uses the wrong configuration format (settings format without wrapper), fails to distinguish prompt vs command hooks,\n    hardcodes paths instead of ${CLAUDE_PLUGIN_ROOT}, or does not provide a plausible secure validate-bash.sh skeleton.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case demonstrates strong domain knowledge of Claude Code hooks, testing genuine practitioner skills like choosing prompt vs command hooks, using ${CLAUDE_PLUGIN_ROOT} for portability, understanding plugin wrapper format vs settings format, secure bash validation patterns, and lifecycle caveats. The grader is comprehensive and accepts valid approaches. However, it has minor clarity issues: the prompt is somewhat dense with multiple requirements that could overwhelm evaluation, and the grader's 0.5 scoring criterion ('missing ONE major element') may be too lenient given the test's complexity. The realism is excellent—building a safety plugin with PreToolUse validation, SessionStart context loading, and Stop enforcement is a common real-world task. The test effectively distinguishes between memorization (would fail on trivia like exact script names) and transferable knowledge (security patterns, hook lifecycle, configuration structure)."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\n*Domain knowledge taught:* This skill is about building event-driven automation hooks within Claude Code plugins, particularly prompt-based and command hooks for events such as PreToolUse, PostToolUse, Stop, etc. It teaches how to reason about when to intercept tool usage, validate inputs, enforce security/policy decisions, set up configuration structure (plugin hooks.json vs user settings), and handle output/input formats, matchers, environment variables, and lifecycle constraints.\n\n*Workflow decisions enabled:* It teaches when to employ prompt-based hooks (for context-aware reasoning, flexible validation) versus command hooks (for deterministic checks or file system tasks), how to choose appropriate hook events for different enforcement points (e.g., PreToolUse for validation, Stop/SubagentStop for task completion, SessionStart for context loading), and how to configure matchers and outputs so that hooks run in the right circumstances without unsafe side effects.\n\n*Challenges without this skill:* Without it, a practitioner would struggle to implement policy or safety checks before executing tools, ensure compliance with stopping conditions, and integrate context-aware automation into Claude Code workflows. They would likely misconfigure hook structures, pick wrong events or matcher patterns, or fail to produce valid prompt output that influences agent behavior.\n\n**Test Case**\n\n```yaml\nname: enforce-sensitive-write-hook\nskills:\n- Hook Development\nprompt: |\n  You are writing hooks for a Claude Code plugin that must block any Write or Edit tool call that targets configuration files (any path ending in `.env` or `config.json`). Design a prompt-based PreToolUse hook that runs before Write/Edit tools, inspects `$TOOL_INPUT`, and returns a permission decision that denies such changes while allowing others. Include matcher criteria, hook type, and the prompt text that explains what Claude should check. Mention how the hook should respond (approve/deny) and what contextual information it should return to Claude after the evaluation.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria explaining what a correct answer looks like.\n\n    Score 1.0 if: Response shows a PreToolUse hook configuration for Write/Edit tools using the prompt-based hook type, includes a matcher that targets Write and Edit, describes a prompt that inspects `$TOOL_INPUT` for `.env` or `config.json`, explains returning permissionDecision (approve/deny) with a reason or system message when the path is sensitive, and mentions allowing non-sensitive writes.\n    Score 0.5 if: Response mentions a PreToolUse hook and sensitive path checks but omits either the matcher detail, the prompt-based format, or how the hook communicates approve/deny decisions to Claude.\n    Score 0.0 if: Response fails to describe a PreToolUse hook, lacks sensitivity to `.env`/`config.json`, or misunderstands how the hook communicates permission decisions/output.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about hook architecture (event selection, prompt-based vs command hooks, permission decisions) rather than trivia. The grader is mostly robust, accepting various valid approaches to describing the hook configuration. However, the grader could be slightly more explicit about accepting alternative valid matcher syntaxes (e.g., 'Write|Edit' vs separate matchers) and different ways to structure the permission decision output. The task is highly realistic - blocking sensitive file writes is a common security requirement. Minor deduction for grader specificity around acceptable variations in hook structure representation."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1. **What domain knowledge does this skill teach?**\n   The skill covers event-driven architecture within an agentic loop. It teaches how to intercept an LLM's workflow at specific lifecycle stages (before/after tool use, upon submission, before exiting) to enforce security policies, validate task completion, or inject project-specific context.\n\n2. **What workflow decisions does it enable?**\n   It enables the decision between **deterministic validation** (Command hooks using bash/regex for speed and safety) and **semantic validation** (Prompt-based hooks using LLM reasoning for complex quality or logic checks). It also clarifies the structural requirements for distributing these hooks as standalone plugins versus local user settings.\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - **Schema Mismatches:** Using the flat settings format inside a plugin's `hooks.json` (missing the required `{\"hooks\": { ... }}` wrapper).\n   - **Incorrect Output Keys:** Confusing the output requirements for different events (e.g., `PreToolUse` requires `permissionDecision` while `Stop` requires `decision`).\n   - **Environment Portability:** Hardcoding paths instead of using `${CLAUDE_PLUGIN_ROOT}`, which causes plugins to fail on different machines.\n   - **Execution Lifecycle:** Not realizing hooks are parallel and stateless, or failing to understand that they only load at session start.\n\n---\n\n### Test Case\n\n```yaml\nname: hook-development-plugin-implementation\nskills:\n- Hook Development\nprompt: |\n  I am developing a Claude Code plugin called \"security-guardian\". I need you to create the `hooks/hooks.json` file for this plugin. \n\n  The plugin must implement two specific behaviors:\n  1. **Safety Check**: Before any tool that matches \"Delete\" or \"Bash\" is executed, run a shell script located at `scripts/guard.sh` within the plugin directory. This should be a deterministic command-type hook.\n  2. **Quality Gate**: When the agent is about to finish its task (Stop event), use a prompt-based hook to evaluate if the agent has actually verified its changes with a build or test. If not, it should block the stop.\n\n  Please provide the complete JSON content for `hooks/hooks.json`, ensuring you use the correct format for a plugin and the appropriate output structures for these specific events.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response for a correct implementation of Claude Code plugin hooks.\n\n    Score 1.0 if the following criteria are met:\n    1. The JSON uses the plugin-specific wrapper format: `{\"hooks\": { \"EventName\": [...] }}`. (Using the flat settings format is incorrect for a plugin).\n    2. The \"PreToolUse\" hook correctly uses a \"matcher\" for \"Delete|Bash\" (or similar regex) and uses `${CLAUDE_PLUGIN_ROOT}` in the command path to `scripts/guard.sh`.\n    3. The \"Stop\" hook is implemented as a \"prompt\" type hook.\n    4. The response correctly identifies that a \"Stop\" hook must return a \"decision\" field (approve/block) and a \"PreToolUse\" hook must return \"permissionDecision\" (allow/deny/ask) in its `hookSpecificOutput`.\n    5. The JSON syntax is valid.\n\n    Score 0.5 if:\n    - The JSON is valid and the logic is mostly correct, but it misses the plugin `hooks` wrapper (using the flat direct format instead).\n    - OR it misses the `${CLAUDE_PLUGIN_ROOT}` variable, using a hardcoded or relative path instead.\n    - OR it confuses the output keys (e.g., using 'decision' for PreToolUse).\n\n    Score 0.0 if:\n    - The JSON structure is fundamentally wrong for Claude Code hooks.\n    - The hook types (command vs prompt) are swapped or missing.\n    - The events (PreToolUse/Stop) are incorrect.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about event-driven hook architecture, configuration formats, and environment portability—concepts that apply broadly beyond this specific skill. The grader is mostly robust, accepting valid implementations while checking for key gotchas (plugin wrapper format, ${CLAUDE_PLUGIN_ROOT}, correct output keys). However, the grader could be slightly more explicit about accepting alternative valid matchers (e.g., 'Delete|Bash' vs 'Bash|Delete' vs regex patterns) and could clarify that the exact prompt text doesn't matter as long as it's type 'prompt'. The task is realistic—creating a plugin with safety and quality gates is a genuine practitioner need. Minor deduction for grader brittleness around matcher variations and potential over-specification of prompt content."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "Let me analyze the skill document first to understand what domain knowledge and workflow decisions it teaches.\n\n## Analysis\n\n**What domain knowledge does this skill teach?**\nThe skill teaches Claude Code plugin hook development - specifically how to create event-driven automation scripts that respond to Claude Code events. It covers:\n- Hook types (PreToolUse, PostToolUse, Stop, SessionStart, etc.)\n- Configuration formats (plugin vs settings)\n- Prompt-based vs command hooks\n- Security best practices\n- Performance considerations\n- Debugging and validation\n\n**What workflow decisions does it enable?**\nThe key workflow decisions are:\n1. **When to use prompt-based hooks vs command hooks** - prompt-based for flexible, context-aware decisions; command for deterministic, fast checks\n2. **How to choose appropriate hook events** - PreToolUse for validation, PostToolUse for reactions, Stop for completeness checks, etc.\n3. **When to use plugin format vs settings format** - plugin format with wrapper for plugins, direct format for user settings\n4. **How to handle security and performance** - input validation, path safety, parallel execution considerations\n\n**What would a practitioner actually struggle with without this skill?**\nWithout this skill, a practitioner would struggle with:\n- Choosing the right hook event for their use case\n- Deciding between prompt-based and command hooks appropriately\n- Understanding the configuration format differences between plugins and settings\n- Implementing security validation properly\n- Understanding that hooks load at session start and require restarts\n- Knowing that hooks run in parallel and designing them accordingly\n\nNow let me create a realistic test case that measures understanding of these core workflow decisions:\n\n```yaml\nname: hook-architecture-decisions\nskills:\n- Hook Development\nprompt: |\n  You are designing security automation for a Claude Code plugin. You need to implement three security policies:\n\n  1. **Policy 1:** Validate all file write operations to prevent path traversal and protect sensitive files like .env files\n  2. **Policy 2:** Check if user prompts about authentication/security might need additional warnings\n  3. **Policy 3:** Ensure tasks aren't marked complete until tests pass and build succeeds\n\n  For each policy, explain:\n  - Which hook event(s) you would use and why\n  - Whether to use prompt-based or command hooks and your reasoning\n  - The basic structure of the hook configuration\n\n  Focus on the architectural decisions rather than exact implementation details.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on understanding of hook development workflow decisions:\n\n    Score 1.0 if: Correctly identifies hook events (PreToolUse for file validation, UserPromptSubmit for prompt checking, Stop for completion validation), appropriately chooses hook types (command for path/sensitive file checks, prompt for context-aware security evaluation, prompt for completion verification), and shows understanding of configuration structure. Explains the reasoning behind each decision.\n\n    Score 0.5 if: Gets some hook events right or hook types, but shows confusion about one or more policies. May mix up when to use prompt vs command hooks or choose wrong hook events.\n\n    Score 0.0 if: Shows fundamental misunderstanding of hook events, hook types, or cannot articulate a coherent architectural approach.\n\n    Do NOT penalize for minor JSON syntax errors or specific implementation details - focus on the conceptual decisions.\n  extractor: last_assistant\n```Looking at the Hook Development skill document, I need to create a test that measures the practitioner's ability to make proper hook development decisions.\n\n## Analysis\n\n**Domain Knowledge:** The skill teaches Claude Code plugin hook development, covering event-driven automation, hook types, configuration formats, and security practices.\n\n**Key Workflow Decisions:** \n- Choosing between prompt-based vs command hooks based on flexibility vs performance needs\n- Selecting appropriate hook events for different scenarios (PreToolUse for validation, Stop for completeness checks, etc.)\n- Understanding configuration format differences between plugins and settings\n- Designing for parallel execution and session lifecycle constraints\n\n**Struggle Points Without This Skill:**\n- Choosing wrong hook events for the task at hand\n- Inappropriately using command hooks when context-aware validation is needed\n- Not understanding that hooks run in parallel and must be designed independently\n- Missing security considerations like input validation and path safety\n\n```yaml\nname: hook-architecture-design\nskills:\n- Hook Development\nprompt: |\n  You are building a Claude Code plugin for enterprise development teams that needs to implement the following security policies:\n\n  **Policy 1:** Prevent developers from accidentally committing secrets or overwriting production configuration files\n  **Policy 2:** Ensure code reviews are completed before major refactoring tasks are marked as finished\n  **Policy 3:** Add legal compliance warnings when users ask about GDPR, CCPA, or other data privacy regulations\n\n  For each policy, design a hook implementation that specifies:\n  1. Which hook event(s) to use and your reasoning\n  2. Whether to use prompt-based or command hooks with justification\n  3. The basic hook configuration structure showing matcher and hook definition\n  4. Any important security or performance considerations\n\n  Focus on explaining your architectural choices rather than writing complete implementation code.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the hook architecture design based on the skill's core concepts:\n\n    Score 1.0 if: Correctly identifies appropriate hook events (PreToolUse for file protection, Stop for task completion validation, UserPromptSubmit for compliance warnings), makes right choices between prompt/command hooks (command for deterministic file path validation, prompt for context-aware completion review, prompt for regulatory compliance detection), and demonstrates understanding of key considerations like security input validation and parallel execution.\n\n    Score 0.5 if: Shows partial understanding - gets some hook events or hook type choices correct but makes significant errors in one or two policies. May show confusion about when to use prompt vs command hooks or pick suboptimal hook events.\n\n    Score 0.0 if: Demonstrates fundamental misunderstanding of hook concepts, chooses completely inappropriate hook events, cannot articulate differences between prompt and command hooks, or ignores security considerations entirely.\n\n    Accept variation in specific implementation details but evaluate the core architectural reasoning.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hook development workflow decisions. It correctly tests transferable knowledge about choosing hook events (PreToolUse for file validation, Stop for completion checks, UserPromptSubmit for prompt analysis) and hook types (command vs prompt-based) rather than trivia. The grader is robust, explicitly accepting variation in implementation details while focusing on architectural reasoning. The scenario is realistic - enterprise security policies are common real-world requirements. Minor deduction: The test could be slightly more explicit about accepting alternative valid approaches (e.g., PostToolUse for some scenarios), and the grader could provide more granular partial credit criteria between 0.5 and 1.0."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: claude-code-plugin-hooks-safety-guardrails\nskills:\n  - Hook Development\nprompt: |\n  You are building a Claude Code *plugin* (not user settings) for a team that keeps accidentally:\n  - writing secrets into tracked files (especially .env / *.pem), and\n  - running destructive shell commands.\n\n  Create a proposed implementation that includes:\n\n  1) A `hooks/hooks.json` suitable for a Claude Code plugin that:\n     - Uses a **SessionStart command hook** to persist an environment variable `PROJECT_ENV=prod` for the session (must use `$CLAUDE_ENV_FILE` correctly).\n     - Uses **PreToolUse** to guard:\n       a) `Write|Edit`: prevent path traversal (`..`) and writing to files that look like secrets (at least `.env` and `*.pem`). Prefer a **prompt-based hook** for this policy.\n       b) `Bash`: block obviously dangerous commands (at minimum `rm -rf /` and `curl ... | sh`). Prefer a **command hook** for deterministic checks.\n     - Uses a **Stop prompt hook** that blocks stopping if the agent didn’t run tests when code was changed (describe the heuristic in the prompt).\n     - Uses matchers appropriately and includes reasonable timeouts.\n     - Uses `${CLAUDE_PLUGIN_ROOT}` for any script paths (no hardcoded absolute paths).\n\n  2) The bash script for the Bash validation hook (e.g., `scripts/validate-bash.sh`), written defensively:\n     - reads the hook JSON from stdin\n     - extracts the bash command from the tool input\n     - denies the dangerous patterns above\n     - returns a blocking signal using the correct exit code behavior (stderr + exit code) and outputs valid JSON for Claude Code hooks\n     - includes safe bash practices (`set -euo pipefail`, quotes variables)\n\n  3) One short note explaining any operational caveat a developer must remember after editing hook configuration (e.g., how changes take effect).\n\n  Output all artifacts inline (the JSON and the script), with clear file headings.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates practical, correct Claude Code plugin hook development.\n\n    Score 1.0 if the response includes ALL of:\n    - A plugin-style `hooks/hooks.json` that uses the required WRAPPER structure with a top-level object containing `\"hooks\": { ... }` (an optional `\"description\"` is fine).\n    - SessionStart is implemented as a COMMAND hook (not prompt) and correctly persists `PROJECT_ENV=prod` by appending an export line to `$CLAUDE_ENV_FILE` (or equivalent correct use), and does not hardcode absolute paths.\n    - PreToolUse protections:\n      * For Write/Edit: uses a prompt-based hook OR otherwise demonstrates context-aware validation, and explicitly checks for path traversal (`..`) AND secret-ish targets (at least `.env` and `.pem`). The prompt should instruct returning an allow/deny/ask style decision or clearly indicate denial behavior; it must be plausibly usable as a PreToolUse validator.\n      * For Bash: uses a command hook that calls a script via `${CLAUDE_PLUGIN_ROOT}` and has a matcher that targets Bash tool calls.\n    - A Stop prompt hook that attempts to prevent premature stopping by checking whether tests were run when code changed (heuristic description is acceptable).\n    - The bash validation script:\n      * reads stdin JSON and parses fields with jq (or another safe parser approach)\n      * blocks at least `rm -rf /` and `curl ... | sh` (pattern-based matching is fine)\n      * uses `set -euo pipefail` and quotes variables\n      * on denial: writes a JSON error to stderr and exits with code 2 (or otherwise clearly uses the documented blocking behavior)\n    - Mentions the operational caveat that hooks load at session start and require restarting Claude Code/session to apply changes (or equivalent).\n\n    Score 0.5 if the response is mostly correct but is missing ONE key element above (e.g., forgets the plugin wrapper, or forgets SessionStart env persistence, or the script does not use exit 2/stderr for blocking), while still showing clear understanding of hooks/events and matchers.\n\n    Score 0.0 if it does not demonstrate plugin hook structure and lifecycle (e.g., provides only generic advice, uses the wrong config format without wrapper, omits required hooks, or provides no meaningful script/policies).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong domain knowledge of Claude Code hooks (SessionStart env persistence, PreToolUse validation patterns, Stop hooks, plugin wrapper format, exit codes, matchers, ${CLAUDE_PLUGIN_ROOT}). However, it has moderate clarity issues: the grader is somewhat brittle by requiring specific implementation choices (e.g., 'prompt-based hook OR otherwise demonstrates context-aware validation' creates ambiguity), and the 0.5 partial credit criterion of 'missing ONE key element' is difficult to apply objectively when there are ~8 distinct requirements. The test is realistic and tests transferable concepts (security guardrails, hook lifecycle) rather than trivia, but the grader complexity and potential for rejecting valid alternative approaches (e.g., different JSON parsing methods, alternative blocking patterns) reduce clarity."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (per skill doc)\n# 1) Domain knowledge: Claude Code plugin hooks are event-driven automations (PreToolUse/PostToolUse/Stop/SessionStart/etc.)\n#    that can be implemented as prompt-based or command-based hooks; they use matchers, structured outputs, and key env vars\n#    (${CLAUDE_PLUGIN_ROOT}, $CLAUDE_PROJECT_DIR, $CLAUDE_ENV_FILE).\n# 2) Workflow decisions enabled: choosing prompt hooks for nuanced policy decisions vs command hooks for fast deterministic checks;\n#    placing controls at the right lifecycle points (PreToolUse for safety gates, SessionStart for context/env setup, Stop for completeness);\n#    designing hooks assuming parallel execution and understanding reload/debug workflows.\n# 3) Practitioner struggle target: correctly wiring a plugin hooks/hooks.json (wrapper format), writing safe/portable command hooks that\n#    parse stdin JSON and return the right allow/deny/ask semantics, persisting env vars via $CLAUDE_ENV_FILE, and remembering hooks\n#    require session restart + debug tooling to iterate.\n\nname: claude-code-plugin-security-hooks\nskills:\n  - Hook Development\nprompt: |\n  You are building a Claude Code plugin for a security-conscious team.\n\n  Requirements:\n  1) In the plugin’s `hooks/hooks.json` (PLUGIN format, not user settings format), add `PreToolUse` guards:\n     - For the `Bash` tool: run a fast deterministic COMMAND hook that blocks obviously dangerous patterns\n       (examples: `rm -rf /`, `mkfs`, `shutdown`, `curl ... | bash`, etc.) and returns an `allow|deny|ask` style decision.\n       Also add a PROMPT-based hook for nuanced/edge cases (e.g., `rm -rf ./dist` might be OK) with clear instructions on what to return.\n     - For `Write|Edit`: block writes to sensitive paths (`.env`, `/etc/*`, `~/.ssh/*`) and path traversal (`..`).\n  2) Add a `SessionStart` hook that detects project type (node if `package.json` exists, python if `pyproject.toml` exists, else `unknown`)\n     and persists `PROJECT_TYPE` for the session by appending to `$CLAUDE_ENV_FILE`.\n  3) Add a `Stop` hook that blocks stopping if the agent hasn’t run tests (or at least explicitly explained why tests can’t be run).\n\n  Deliverables:\n  - The complete `hooks/hooks.json` content (remember the plugin wrapper format).\n  - Bash snippets for any command hooks you reference. Use portable paths via `${CLAUDE_PLUGIN_ROOT}` and safe bash practices\n    (quoting, input validation, `jq`).\n  - 3–5 bullet notes on debugging/lifecycle pitfalls (e.g., restart needed to reload hooks).\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical Claude Code *plugin hook* development competence (not generic “security advice”).\n\n    Score 1.0 if the response includes ALL of the following (allowing reasonable variations in structure/wording):\n    - A `hooks/hooks.json` example in **plugin wrapper format**: top-level object contains a required `\"hooks\": { ... }` mapping.\n      (Optional `\"description\"` is fine.) If the answer uses settings-style top-level events without the `\"hooks\"` wrapper, do not give full credit.\n    - Correct use of hook **events and matchers**:\n      - `PreToolUse` has a matcher that targets `Bash`, and a matcher that targets `Write|Edit` (or equivalent).\n      - For `Bash`, includes BOTH a `\"type\": \"command\"` hook and a `\"type\": \"prompt\"` hook (or clearly states they run in parallel).\n    - Portability best practice: command hooks reference scripts using `${CLAUDE_PLUGIN_ROOT}` (not hardcoded absolute paths).\n    - At least one concrete command-hook script snippet that:\n      - Reads JSON from stdin and extracts fields with `jq` (e.g., tool_name/tool_input).\n      - Uses safe bash patterns (e.g., `set -euo pipefail`, quotes variables).\n      - Implements the required policy checks (dangerous bash patterns; sensitive file/path traversal checks) and returns a structured decision\n        consistent with PreToolUse semantics (allow/deny/ask), using a blocking exit (commonly exit 2) OR an explicit JSON decision payload.\n    - `SessionStart` hook present and describes/implements persisting `PROJECT_TYPE` by appending `export PROJECT_TYPE=...` to `$CLAUDE_ENV_FILE`\n      (or equivalent correct use of the env file).\n    - `Stop` hook present and uses prompt-based approve/block style logic tied to whether tests were run / explained.\n    - Includes 3–5 operational notes mentioning at least one of:\n      - hooks load at session start and require restarting Claude Code to apply changes\n      - debugging via `claude --debug` and/or inspecting loaded hooks (e.g., `/hooks`)\n      (Parallel-execution note is a plus but not strictly required if other criteria are met.)\n\n    Score 0.5 if the response is directionally correct but misses 2+ major elements above (e.g., wrong config format, no `${CLAUDE_PLUGIN_ROOT}`,\n    no scripts, missing SessionStart env persistence, or missing Stop completeness gate).\n\n    Score 0.0 if it does not provide a meaningful hook configuration + implementation approach (e.g., only high-level advice, unrelated content,\n    or no concrete hooks/scripts).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong domain knowledge by requiring understanding of plugin wrapper format, parallel hook types (command+prompt), environment variable persistence via $CLAUDE_ENV_FILE, and lifecycle constraints. However, it has moderate clarity issues: the grader is overly prescriptive about implementation details (requiring both command AND prompt hooks for Bash, specific exit codes, exact jq usage patterns) when the skill document shows multiple valid approaches. A practitioner could correctly implement security hooks using only prompt-based hooks or different JSON output formats, yet the grader would penalize these valid solutions. The realism is high (security validation is a genuine use case), but the test conflates 'demonstrates knowledge' with 'follows one specific implementation pattern,' making it somewhat brittle despite testing transferable concepts."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this skill document, I need to analyze the core domain knowledge and identify what practitioners would genuinely struggle with.\n\n**Domain Knowledge Analysis:**\n1. **Hook Events & Use Cases** - When to use PreToolUse vs Stop vs SessionStart, etc.\n2. **Configuration Formats** - Plugin hooks.json wrapper format vs settings direct format\n3. **Hook Types** - Prompt-based (LLM-driven) vs Command (bash-driven) \n4. **Security Practices** - Input validation, path safety, variable quoting\n5. **Plugin Integration** - Using ${CLAUDE_PLUGIN_ROOT}, hook lifecycle\n\n**What Practitioners Struggle With:**\n- Knowing which hook event to use for specific scenarios\n- Understanding the plugin vs settings configuration format differences\n- Choosing between prompt-based and command hooks\n- Implementing proper security validation\n- Debugging hook configuration issues\n\nHere's my test case:\n\n```yaml\nname: plugin-file-write-validation-hook\nskills:\n  - Hook Development\nprompt: |\n  You're developing a Claude Code plugin called \"secure-file-ops\" that validates file write operations for security.\n\n  Create the hook configuration that:\n  1. Uses a PreToolUse hook to validate Write operations\n  2. Checks for path traversal attacks (deny \"..\" in paths)\n  3. Prevents writing to sensitive files (*.env, *.key, config.json)\n  4. Uses the plugin-specific configuration format\n  5. Uses ${CLAUDE_PLUGIN_ROOT} for portability\n  6. Choose appropriate hook type (prompt-based or command) with justification\n\n  Show the complete hooks.json file content and explain your hook type choice.\n\n  Your plugin directory structure:\n  ```\n  secure-file-ops/\n    hooks/\n      hooks.json\n      scripts/\n        validate-write.sh\n  ```\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the response demonstrates proper hook development knowledge.\n\n    Score 1.0 if:\n    - Uses correct plugin hooks.json wrapper format (with \"description\" and \"hooks\" wrapper)\n    - Implements PreToolUse hook for Write operations with appropriate matcher\n    - Includes security checks for path traversal and sensitive files\n    - Uses ${CLAUDE_PLUGIN_ROOT} for file paths\n    - Justifies choice between prompt-based vs command hooks appropriately\n    - Provides complete, valid JSON configuration\n\n    Score 0.5 if:\n    - Shows understanding of hooks but has format issues (missing wrapper or incorrect structure)\n    - Includes security concepts but implementation is incomplete\n    - Correct hook type choice but lacks justification or has minor errors\n\n    Score 0.0 if:\n    - Uses wrong configuration format (settings format instead of plugin format)\n    - No security validation or dangerous operations allowed\n    - Missing ${CLAUDE_PLUGIN_ROOT} portability\n    - No hook type justification or inappropriate choice\n  extractor: last_assistant\n```\n",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of hook development concepts and tests transferable knowledge (security validation, configuration formats, hook types). However, it has moderate clarity issues: the grader accepts both prompt-based and command hooks 'appropriately' without defining what that means, creating ambiguity. The test also requires knowing the specific plugin wrapper format with 'description' and 'hooks' fields, which borders on trivia about JSON structure rather than core hook concepts. The realism is good - validating file writes for security is a genuine use case. The grader could be more robust by explicitly accepting both hook types with clear criteria for each."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "**Skill Analysis**\n\n1. **Core Domain Knowledge**  \n   - Understanding Claude Code’s hook system: event lifecycle, hook types (prompt vs command), matcher patterns, and standardized input/output JSON schemas.  \n   - Knowledge of configuration contexts: plugin `hooks/hooks.json` wrapper versus direct settings format, portability requirements (`${CLAUDE_PLUGIN_ROOT}`), environment variables, and execution semantics (parallelism, exit codes, timeouts).\n\n2. **Workflow Decisions Enabled**  \n   - Choosing prompt-based vs command hooks based on need for contextual reasoning vs deterministic checks.  \n   - Selecting appropriate events (PreToolUse, Stop, SessionStart, etc.) to enforce policies, load context, or gate outputs.  \n   - Structuring hook definitions properly depending on deployment target (plugin vs settings) and designing safe, portable command workflows.\n\n3. **Likely Practitioner Struggles Without Skill**  \n   - Misconfiguring hooks.json (forgetting wrapper, wrong event placement) leading to non-functional hooks.  \n   - Failing to justify hook type choices or misuse events (e.g., using PreToolUse for completion validation).  \n   - Neglecting security/performance best practices: not using `${CLAUDE_PLUGIN_ROOT}`, missing timeouts, unvalidated inputs.  \n   - Not understanding parallel execution implications or required output schema for decisions.\n\n---\n\n```yaml\nname: pretooluse-safety-policy-hook\nskills:\n  - Hook Development\nprompt: |\n  You are building a Claude Code plugin that must prevent risky file writes while still allowing legitimate tool use. Author a complete `hooks/hooks.json` snippet for the plugin that satisfies all of the following requirements:\n\n  1. Use the correct plugin wrapper structure (include an optional description plus the required `hooks` object).\n  2. For `PreToolUse`, ensure any tool whose name matches `Write|Edit` is checked by:\n     - A prompt-based hook that decides whether the write is safe, referencing concerns such as system paths, credentials, path traversal, and sensitive files. It must clearly instruct the model to return allow/deny guidance.\n     - A command hook that calls a validation script located under `${CLAUDE_PLUGIN_ROOT}/scripts/validate-write.sh` with a 20-second timeout. Assume the script already exists.\n  3. Add a `SessionStart` hook that runs a command to load project context via `${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh` and persists any needed environment variables (hint that the hook should append to `$CLAUDE_ENV_FILE`).\n  4. Include a `Stop` hook that uses a prompt to verify overall task completeness (tests run, builds passing, outstanding questions resolved) and blocks stopping if requirements are unmet.\n  5. Briefly document in comments (JSON `description` fields or string literals) why each hook exists and any security considerations.\n\n  Adhere strictly to JSON syntax, ensure matchers are correct, and use portable paths. Do not invent new events beyond those specified.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant's hooks/hooks.json snippet fulfills all requirements:\n\n    Score 1.0 if:\n    - Uses plugin wrapper with optional description and nested `hooks` map.\n    - `PreToolUse` entry targets a matcher `Write|Edit` (or equivalent regex) and contains BOTH:\n      * A prompt-based hook describing safety checks (system paths, credentials, path traversal, sensitive files) and instructing allow/deny decision-making.\n      * A command hook invoking `${CLAUDE_PLUGIN_ROOT}/scripts/validate-write.sh` with a 20-second timeout.\n    - `SessionStart` hook runs `${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh` and mentions persisting data via `$CLAUDE_ENV_FILE`.\n    - `Stop` hook is prompt-based, covering tests/builds/questions and blocking stop when unmet.\n    - Comments/documentation provided via descriptions or strings explaining purpose/security.\n    - JSON syntax is valid (object braces, arrays, quoting, commas).\n\n    Score 0.5 if most elements are correct but one major requirement is missing or incorrect (e.g., missing command hook timeout, absent SessionStart env mention, or incorrect structure).\n\n    Score 0.0 if multiple major requirements are unmet, structure is not plugin format, or JSON is invalid.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test demonstrates moderate quality across dimensions. **Non-obviousness (6/10)**: While it tests genuine hook development patterns (wrapper structure, hook types, matchers, portable paths), it leans heavily on memorizing specific details like exact timeout values (20 seconds), specific script names (validate-write.sh, load-context.sh), and the $CLAUDE_ENV_FILE variable name - these are skill-specific trivia rather than transferable concepts. **Clarity (7/10)**: The grader is mostly robust, accepting variations in implementation while checking core requirements, though it could be more flexible about equivalent approaches (e.g., different matcher patterns, alternative security checks). **Realism (7/10)**: The task is realistic - practitioners do need to implement safety policies for file operations - but the specific combination of requirements feels somewhat contrived to test multiple skill aspects simultaneously rather than emerging from a genuine use case. The test would be stronger if it focused on conceptual understanding (choosing appropriate hook types, structuring decisions) rather than exact identifiers."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: implement-safety-hook-plugin\nskills:\n  - Hook Development\nprompt: |\n  I am developing a Claude Code plugin and I need to implement a safety mechanism. \n  \n  Please write the content for `hooks/hooks.json` that accomplishes the following:\n  1. Intercepts any attempt by Claude to write or edit files.\n  2. Uses an LLM prompt to evaluate whether the file operation is safe (specifically checking if it's a production configuration file).\n  3. Returns a \"deny\" decision if the file is deemed critical/production-related.\n  \n  Ensure the format is correct for a plugin distribution.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Analyze the JSON output provided by the assistant.\n    \n    The user asked for a *plugin* hook configuration (`hooks/hooks.json`), which requires a specific wrapper structure compared to user settings.\n    \n    Score 1.0 if:\n    - The JSON follows the Plugin wrapper format: Root object contains a \"hooks\" key which contains the events (e.g., `{ \"hooks\": { \"PreToolUse\": ... } }`).\n    - Uses the `PreToolUse` event.\n    - Uses a `matcher` that targets file modification tools (e.g., \"Write\", \"Edit\", \"Write|Edit\", or \"*\").\n    - Defines a hook of `type`: \"prompt\".\n    - The prompt text generally aligns with the requirement to check for production config files.\n    \n    Score 0.5 if:\n    - The logic is correct (PreToolUse, prompt-based, correct matcher), BUT it uses the User Settings format (missing the root \"hooks\" wrapper), effectively returning `{ \"PreToolUse\": ... }` directly.\n    - OR if they use a command hook instead of a prompt hook despite the instruction.\n    \n    Score 0.0 if:\n    - The JSON is invalid.\n    - Uses the wrong event (e.g., PostToolUse).\n    - Fails to target file writing tools.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test measures genuine transferable knowledge about hook architecture (plugin wrapper format vs settings format, PreToolUse event selection, prompt-based hooks) rather than trivia. However, clarity is compromised: the grader penalizes using command hooks 'despite the instruction,' but the prompt only says 'uses an LLM prompt' without explicitly forbidding command hooks—a valid implementation could use a command hook that calls an LLM. The grader also doesn't verify the output format structure (permissionDecision field) or handle edge cases like valid regex matchers. Realism is strong as implementing safety hooks for file operations is a common plugin development task. The test would benefit from either explicitly requiring prompt-based hooks in the prompt or accepting command-based LLM solutions in the grader."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "## Analysis\n\n1. **Domain Knowledge:** The skill teaches comprehensive guidance for creating and implementing Claude Code plugin hooks, focusing on advanced prompt-based hooks API. It covers event-driven automation, validation, integration, and context-aware decisions.\n\n2. **Workflow Decisions:** The skill enables decisions on when to use specific hook types (prompt-based vs. command hooks), how to configure hooks for different events (PreToolUse, PostToolUse, Stop, etc.), and best practices for security, performance, and debugging.\n\n3. **Practitioner Struggles:** Without this skill, a practitioner might struggle with:\n   - Understanding the appropriate use of different hook types and their configurations.\n   - Implementing effective validation and integration strategies within Claude Code.\n   - Ensuring security and performance considerations are adequately addressed in hook development.\n   - Debugging and troubleshooting issues related to hook implementation.\n\n## Test Case\n\n```yaml\nname: implementing-hook-validation\nskills:\n- Hook Development\nprompt: |\n  You are tasked with creating a hook for the Claude Code platform that validates file write operations before they are executed. The hook should check if the file path contains sensitive directories or if the operation might lead to path traversal vulnerabilities. If the operation is deemed unsafe, the hook should deny the file write request. Describe how you would implement this validation hook, including the type of hook you would use, the event it would trigger on, and any specific validation logic you would implement.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response accurately describes using a PreToolUse hook with prompt-based validation logic that checks for path traversal and sensitive directories, and correctly outlines how to return a deny decision with a appropriate reason.\n    Score 0.5 if the response mentions using a hook for validation but lacks specific details on the type of hook, event, or validation logic, or if it incorrectly suggests using a different type of hook or event.\n    Score 0.0 if the response does not address the use of hooks for validation or suggests a completely unrelated approach to solving the problem.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates moderate quality across dimensions. **Non-obviousness (6/10)**: While it tests transferable security validation concepts (path traversal, sensitive directories), the test is somewhat obvious given the skill explicitly provides examples of this exact pattern in the 'Path Safety' section. **Clarity (7/10)**: The grader is reasonably robust but has issues - it only awards full credit for prompt-based hooks when command hooks are equally valid for this deterministic validation task (the skill shows both approaches). The grader also doesn't specify what constitutes 'appropriate reason' or how to handle partial implementations. **Realism (7/10)**: File write validation is a realistic security concern, though the task is somewhat contrived as it asks to 'describe' rather than 'implement' the hook, making it more of a knowledge recall exercise than a practical implementation task."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-a-file-write-safety-hook\nskills:\n  - Hook Development\nprompt: |\n  You are developing a Claude Code plugin that needs to ensure file safety before any operations that involve writing to files. Create a prompt-based hook that validates file write operations. The hook should check for path traversal, sensitive file writes, and any other relevant security concerns. The hook should return 'approve' if the write is safe, 'deny' with a reason if the write is unsafe, or 'ask' if additional human approval is required.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if:\n    - The response includes a prompt-based hook configuration.\n    - The hook checks for path traversal.\n    - The hook checks for writes to sensitive files (e.g., `.env`).\n    - The hook returns 'approve', 'deny', or 'ask' with appropriate reasons.\n    - The hook uses the `${CLAUDE_PLUGIN_ROOT}` environment variable for portability.\n\n    Score 0.5 if:\n    - The response includes a hook configuration but misses one or two of the security checks (path traversal, sensitive files, etc.).\n    - The hook returns 'approve', 'deny', or 'ask' but with some inaccuracies in the reasons.\n\n    Score 0.0 if:\n    - The response does not include a hook configuration.\n    - The response does not address the security checks or the return values.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues across all dimensions. **Non-obviousness (3/10)**: The grading criteria explicitly requires checking for '${CLAUDE_PLUGIN_ROOT}' usage, which is skill-specific trivia about an environment variable name rather than testing transferable security concepts. **Clarity (4/10)**: The grader is brittle - it requires ${CLAUDE_PLUGIN_ROOT} usage even though the prompt asks for a prompt-based hook (which doesn't use environment variables), and it's unclear how to evaluate 'appropriate reasons' objectively. The grader also doesn't specify what format the hook configuration should take (plugin hooks.json wrapper vs settings format). **Realism (6/10)**: While file write validation is a realistic security concern, the task is somewhat contrived - asking to 'create a prompt-based hook' with specific return values feels more like testing documentation recall than solving a real problem a practitioner would face organically."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-frontend-design",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 6,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** frontend-design\n\n**SKILL.md Content:**\n```markdown\n---\nname: frontend-design\ndescription: Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, or applications. Generates creative, polished code that avoids generic AI aesthetics.\nlicense: Complete terms in LICENSE.txt\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - frontend-design\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** frontend-design\n\n**SKILL.md Content:**\n```markdown\n---\nname: frontend-design\ndescription: Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, or applications. Generates creative, polished code that avoids generic AI aesthetics.\nlicense: Complete terms in LICENSE.txt\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - frontend-design\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "frontend-design",
          "skill_dir": "frontend-design",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n### What domain knowledge does this skill teach?\n\nThe skill teaches how to architect background coding agents that run in remote sandboxed environments. Key concepts include:\n- Image registry pattern (pre-building environments on a cadence)\n- Predictive warm-up (starting sandbox preparation before user submits)\n- Per-session state isolation\n- Three-layer architecture (sandbox, API, clients)\n- Self-spawning agents for parallel work\n\n### What workflow decisions does it enable?\n\n- **When to pre-build vs on-demand**: Pre-build images on regular cadence (~30 min) rather than building fresh for each session\n- **When to allow operations during sync**: Allow file reads before git sync completes, block only writes\n- **When to start warming**: Begin sandbox warm-up when user starts typing, not when they submit\n- **How to structure state**: Isolate state per session to prevent cross-session interference\n\n### What would a practitioner struggle with without this skill?\n\nThe main struggle would be **slow session startup times** and poor user experience. Without understanding the image registry pattern and predictive warm-up strategy, practitioners would naively spin up fresh environments on each request, leading to multi-minute wait times that make the system unusable.\n\nA realistic test: Ask about optimizing startup latency for a hosted agent system.\n\n```yaml\nname: hosted-agent-startup-optimization\nskills:\n  - hosted-agents\nprompt: |\n  I'm building a hosted coding agent that runs in sandboxed VMs (using Modal or similar). Users can start sessions that clone their repository, install dependencies, and let the agent make code changes.\n\n  The problem: Session startup takes 3-5 minutes because we clone the repo, run npm install, and build the project fresh each time a user starts a session. Users are complaining about the wait time.\n\n  What architectural patterns should I implement to get session startup time down to near-instant (a few seconds)? Describe the key strategies and how they work together.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of hosted agent infrastructure optimization patterns.\n\n    A strong answer should include MOST of these key strategies:\n\n    1. **Pre-built images / Image registry pattern**: Building environment images on a regular schedule (e.g., every 30 minutes) with dependencies already installed, so new sessions start from a recent snapshot rather than building fresh.\n\n    2. **Warm pool / Pre-warming**: Maintaining a pool of already-running sandboxes ready to serve requests immediately, rather than spinning up cold VMs.\n\n    3. **Predictive warm-up**: Starting to prepare the sandbox when the user begins typing or shows intent, before they actually submit their request.\n\n    4. **Snapshot and restore**: Taking filesystem snapshots at key points (after setup, after changes) to enable instant restoration.\n\n    5. **Moving work to build time**: Doing as much as possible during image build (dependency install, initial builds, cache warming) since build-time is invisible to users.\n\n    Score 1.0 if: Response explains at least 3 of the key strategies clearly with understanding of why they work (pre-built images, warm pools, predictive warming, snapshots, or build-time work maximization).\n\n    Score 0.5 if: Response mentions 1-2 relevant strategies but lacks depth, OR mentions caching/optimization in generic terms without the specific architectural patterns.\n\n    Score 0.0 if: Response only suggests generic solutions (faster hardware, better network, parallelization of the existing flow) without addressing the fundamental architectural patterns of pre-building and pre-warming.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It tests transferable domain knowledge about pre-building environments, warm pools, and predictive warm-up—patterns that apply broadly to hosted infrastructure, not just this specific skill. The grader is robust, accepting multiple valid strategies and focusing on conceptual understanding rather than exact terminology. The scenario is highly realistic (optimizing startup latency is a common pain point). Minor deduction: The grader could be slightly more explicit about accepting alternative valid approaches (e.g., lazy loading, incremental sync), though it does allow for partial credit with 1-2 strategies."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\n**What domain knowledge does this skill teach?**\n- Distributed systems architecture for running agents in sandboxed environments\n- Performance optimization through pre-building, caching, and predictive warm-up\n- The fundamental challenge: development environments are slow to bootstrap (git clone, dependency installation, builds)\n- State isolation patterns for concurrent multi-user sessions\n- Trade-offs between build-time vs runtime work\n\n**What workflow decisions does it enable?**\n- When to pre-build vs build on-demand\n- How to structure warm pools and predictive startup\n- What work can be parallelized during user typing\n- When to allow file operations during synchronization\n- How to architect for multiplayer collaboration\n\n**What would a practitioner actually struggle with without this skill?**\nThe core problem: session startup time. Without this skill, a practitioner might:\n- Build environments on-demand, causing 2-5 minute waits per session\n- Not realize they can parallelize work while users type\n- Miss the opportunity to move expensive work to build-time\n- Fail to cache appropriately, repeating expensive operations\n\nThe test should focus on **recognizing the bottleneck and applying the right optimization strategy**.\n\n---\n\n```yaml\nname: optimize-agent-session-startup\nskills:\n  - hosted-agents\nprompt: |\n  You're building a hosted coding agent system where users submit prompts to agents that run in isolated sandboxed environments. Each sandbox needs to:\n  - Clone a 500MB Git repository\n  - Install dependencies (takes ~90 seconds)\n  - Run initial build steps (takes ~60 seconds)\n  \n  Currently, users wait 3-4 minutes after submitting a prompt before the agent can start working. This is unacceptable for production use.\n  \n  Your team has compute budget to run sandboxes continuously if needed, and can maintain persistent storage between sessions.\n  \n  Design an architecture that reduces time-to-first-response to under 10 seconds. Explain:\n  1. What is the core bottleneck you're addressing?\n  2. What specific strategy would you use to eliminate most of the wait time?\n  3. How would you handle repository updates while maintaining fast startup?\n  4. What trade-offs does your approach introduce?\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of pre-building and caching strategies for fast agent session startup.\n    \n    Score 1.0 if the response:\n    - Correctly identifies that on-demand environment setup is the bottleneck\n    - Proposes pre-building environment images/snapshots with dependencies already installed\n    - Describes a strategy for keeping images reasonably up-to-date (periodic rebuilds, warm pools, or similar)\n    - Acknowledges trade-offs (staleness, storage costs, or complexity)\n    - May also mention predictive warm-up, parallel operations, or similar optimizations\n    \n    Score 0.5 if the response:\n    - Identifies the right problem (environment setup time)\n    - Proposes some form of caching or pre-building\n    - But misses key details about keeping images current OR doesn't explain trade-offs\n    \n    Score 0.0 if the response:\n    - Proposes only minor optimizations (faster dependency manager, better network)\n    - Focuses on parallelizing work without addressing the fundamental setup time\n    - Misunderstands the problem (e.g., focuses on model latency instead of environment setup)\n    - Proposes solutions that don't actually solve the 3-4 minute wait time\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about pre-building/caching strategies for reducing environment setup time, which is a genuine architectural pattern applicable beyond this specific skill. The grader is robust and accepts multiple valid approaches (periodic rebuilds, warm pools, predictive warm-up). The scenario is realistic - practitioners actually face this exact problem when building hosted agent systems. Minor deduction: the test could be slightly more open-ended to avoid telegraphing the solution structure through the numbered questions, though this doesn't significantly compromise quality."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what domain knowledge it teaches:\n\n**Core Domain Knowledge:**\n- The key insight that session speed depends on pre-completed infrastructure setup (image building, warm pools), NOT runtime startup\n- Architectural layers: sandbox infrastructure → API layer → client interfaces\n- Per-session state isolation and real-time synchronization patterns\n- Trade-offs between different performance optimization strategies\n\n**Critical Workflow Decisions:**\n- WHEN to activate pre-warming vs on-demand sandbox creation\n- WHETHER to block file operations during git sync or allow reads-only\n- HOW to structure frameworks (server-first with multiple clients vs monolithic)\n- WHY per-session state isolation matters (prevents cross-session performance interference)\n\n**What Practitioners Struggle With:**\n- Not realizing that user-facing latency is limited by model provider time, not infrastructure setup (if done right)\n- Missing the importance of predictive warm-up during user typing\n- Trying to sync code synchronously rather than in parallel\n- Building single-client solutions instead of server-first architectures that scale\n\nNow I'll create a realistic scenario test:\n\n```yaml\nname: design-hosted-agent-performance-strategy\nskills:\n  - hosted-agents\nprompt: |\n  You're designing a hosted agent system for a large codebases. Your users expect to start a session and begin seeing the agent working almost immediately—within 2-3 seconds of submitting their prompt.\n  \n  A junior engineer on your team suggests this architecture:\n  - When a user submits a prompt, spin up a fresh sandbox\n  - Clone the repository and install all dependencies\n  - Then start the agent work\n  \n  This takes about 45 seconds before the agent produces any output. Your users are frustrated with the latency.\n  \n  Explain the core architectural changes needed to meet the 2-3 second goal, and justify why each change matters. Address:\n  1. How you would structure environment preparation to be invisible to users\n  2. When to start preparing the sandbox relative to user behavior\n  3. How to handle code synchronization without blocking the agent from working\n  4. Why this approach enables consistent performance at scale\n  \n  Be specific about the patterns and tradeoffs involved.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of hosted agent performance optimization patterns.\n\n    Score 1.0 if the response covers ALL of the following:\n    - Image pre-building/registry pattern: Pre-building environment images on a regular cadence (e.g., every 30 minutes) with dependencies, repos, and build steps already completed\n    - Predictive warm-up: Starting sandbox warming during user typing, not after submission, to have infrastructure ready before model work begins\n    - Parallel synchronization: Allowing the agent to start reading files immediately while git sync happens in parallel, rather than blocking until sync completes\n    - Per-session isolation: Understanding that separate execution environments per session prevent cross-session performance interference and contention\n    - Correct latency model: Recognizing that with proper pre-work, user-facing latency should be dominated by model provider time-to-first-token, not infrastructure setup\n\n    Score 0.5 if the response covers most of the above (4 out of 5 elements) with reasonable accuracy, but misses some nuance or only partially explains the reasoning.\n\n    Score 0.0 if the response:\n    - Only suggests obvious solutions like \"install things faster\" without understanding the architectural patterns\n    - Misses the core insight that setup should be pre-completed and warm pools should exist\n    - Focuses on single isolated optimizations without explaining how they work together\n    - Confuses architectural layers or suggests synchronous operations where async is key\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It tests transferable domain knowledge about performance optimization patterns (image pre-building, predictive warm-up, parallel synchronization) rather than trivia. The grader is robust and accepts multiple valid explanations while checking for genuine understanding of the architectural patterns. The scenario is highly realistic—addressing user-facing latency in hosted agent systems is a common real-world challenge. Minor deduction: the grader could be slightly more explicit about accepting alternative valid approaches (e.g., different cadences for image building, alternative warm pool strategies), though it does avoid being overly prescriptive about specific implementation details."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain analysis (what this skill teaches and what we should test)\n1. Core domain knowledge: how to design “hosted” coding agents that run in remote, sandboxed environments with fast startup, reproducible dev environments, and support for high concurrency + multiplayer collaboration.\n2. Key workflow decisions enabled: when/how to pre-build images vs on-demand setup; warm pools and predictive warm-up to minimize time-to-first-token; snapshot/restore for fast follow-ups; allowing read-only access before git sync completes while blocking writes; per-session state isolation to avoid cross-session interference; server-first agent framework with thin clients; correct git/auth attribution (clone via app token, PRs/commits attributed to prompting user); and real-time multi-client synchronization (WebSockets/streaming).\n3. Practitioner struggle without this skill: they’d propose a generic “spin up a VM per request and git clone on start” design that is too slow, expensive, hard to scale, and fails enterprise requirements (authorship/PR attribution, multiplayer, state isolation, and multi-client sync).\n\n```yaml\nname: hosted-agent-architecture-fast-multiplayer-pr-flow\nskills:\n- hosted-agents\nprompt: |\n  You are designing a hosted background coding agent for an internal engineering org. It must support:\n  - Near-instant session start perceived by users (they should see the model respond quickly).\n  - 500+ concurrent sessions across many repos.\n  - Multi-client access: Slack bot, web UI, and a VS Code-in-browser experience. Users can switch clients mid-session.\n  - Multiplayer sessions: multiple people can join the same session link and prompt collaboratively; code changes must be attributed to the prompter.\n  - The agent can run tools (tests, builds) inside a sandbox and open PRs on GitHub.\n  - Cost control: idle sessions should not burn compute, but users should still see streaming progress while work is active.\n\n  Produce a concrete high-level architecture and workflow that an engineer could implement. Include:\n\n  1) The main components (sandbox layer, API/state layer, clients) and what each is responsible for.\n  2) A session lifecycle from “user starts typing in Slack” → “sandbox ready” → “agent edits code” → “PR created”.\n  3) How you achieve fast startup while still having correct repo contents and dependencies.\n  4) How you handle git authentication and authorship so commits/PRs are on behalf of the prompting user (not the system).\n  5) How you support real-time streaming updates and multi-client synchronization.\n  6) How you isolate per-session state so sessions don’t impact each other.\n  7) How you handle follow-up messages arriving while a tool run is executing (choose an approach and justify it).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates practical hosted-agent infrastructure competence (not generic cloud talk).\n\n    Score 1.0 if the response includes ALL (or nearly all) of the following, in an implementation-oriented way:\n    - Clear 3-layer split: sandboxed execution layer + API/state coordination layer + multiple thin clients (Slack/web/VS Code) with shared session state.\n    - Fast-start strategy based on pre-built environment images (regular cadence or on-merge), not only “git clone + install on startup”.\n    - Some combination of warm pool and/or predictive warm-up that begins provisioning before the user submits (e.g., when they start typing) OR an equivalent latency-hiding mechanism.\n    - Snapshot/restore (filesystem or workspace snapshots) to speed follow-ups and/or resume sessions without redoing setup.\n    - Correct handling of repo sync latency: allows reading files before full sync completes but blocks writes/edits until sync is done (or an equivalent safe gating mechanism).\n    - Per-session state isolation (e.g., one DB per session / durable object per session / isolated state store) with reasoning about preventing cross-session interference at scale.\n    - Real-time streaming and synchronization across clients (e.g., WebSockets/event stream) so switching clients preserves state and live progress.\n    - GitHub auth + attribution details: cloning can use an app/install token, but commits/PRs must use the prompting user’s identity/token; commits use prompter name/email; PR creation on behalf of user; includes a sensible sandbox→API→GitHub flow.\n    - Multiplayer considerations: sessions not tied to a single author, prompts carry authorship metadata, and changes are attributed correctly.\n\n    Score 0.5 if the response proposes a plausible hosted architecture but misses several key hosted-agent patterns above (e.g., no image prebuild, no snapshotting, no per-session isolation, or no user-attributed PR flow), OR it’s mostly generic without concrete workflow details.\n\n    Score 0.0 if the response is largely generic or incorrect (e.g., local-only execution, “just use a single shared VM,” no isolation, no streaming/multi-client sync, or attributes commits/PRs to the system/app without addressing user attribution).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hosted-agent infrastructure by requiring knowledge of pre-built images, warm pools, predictive warm-up, snapshot/restore, per-session isolation, and user-attributed PRs—all transferable patterns. The grader is comprehensive and accepts multiple valid approaches. However, it's somewhat dense and could be slightly contrived (requiring all 9+ specific patterns in one answer may be more of a 'recite the skill' exercise than a realistic single design task). Minor deduction for potential brittleness if valid alternative architectures exist that don't hit every enumerated point."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain knowledge this skill teaches:\n- How to design **hosted/background coding agents** that run in **remote sandboxed environments** with fast start times, reproducibility, and high concurrency.\n- Infrastructure patterns that make sessions feel instant: **prebuilt images**, **warm pools**, **predictive warm-up**, and **snapshot/restore**.\n- How to structure an agent system as **server-first** with thin clients, **real-time streaming**, and **per-session isolated state** to support multiplayer and many interfaces (Slack/web/VS Code/etc.).\n- Correct handling of GitHub auth and attribution: **app tokens for cloning**, but **user identity for commits/PRs**.\n\nWorkflow decisions it enables:\n- When to shift work from “session start” to **image build time**.\n- When to use **warm pools** vs on-demand provisioning.\n- How to permit **reads before sync** but block **writes until sync completes**.\n- Choosing **queue vs insert** semantics for follow-up messages during execution.\n- Designing APIs/state to support **multi-client + multiplayer** without re-architecting later.\n\nWhat practitioners struggle with without this skill (test target):\n- They design a system that “works” but has **slow TTFT**, costly cold starts, inconsistent environments, and brittle single-client assumptions.\n- They mishandle git identity/auth (commits attributed to the app), or can’t safely support multiplayer/state isolation.\n- They fail to provide a coherent sandbox lifecycle (images, snapshots, warming, sync rules) and real-time streaming.\n\n```yaml\nname: hosted-agents-fast-multiplayer-architecture\nskills:\n- hosted-agents\nprompt: |\n  You are designing a “background coding agent” platform (like a hosted Devin-style agent) for a company with ~200 engineers.\n\n  Current pain:\n  - Cold start is 2–4 minutes because it clones repos + installs deps on every session.\n  - Engineers want to start a session from Slack or a web UI and later “pick up” the same session in a VS Code-in-browser client.\n  - Multiple people want to join the same session for live debugging/QA (multiplayer).\n  - The agent must open PRs and commits must be attributed to the human prompter (not a bot), but image builds/clones should not require a user token.\n\n  Task:\n  Propose an end-to-end architecture and operational plan that gets “time to first model token” as low as possible (target: a few seconds) while keeping sandboxes isolated and scalable to hundreds of concurrent sessions.\n\n  Your answer must include (in your own words):\n  1) Sandbox lifecycle design (image building cadence, what goes into the image, and how sessions start).\n  2) Any warm pool / pre-warming strategy and when it triggers.\n  3) Snapshot/restore points you would use and why.\n  4) How you handle git authentication for cloning during image build vs committing/pushing during a session, including git user.name/user.email attribution.\n  5) Rules for allowing reads/writes while syncing latest changes from the base branch.\n  6) API/state layer design for per-session isolation + multi-client sync + real-time streaming.\n  7) How multiplayer is supported without breaking attribution or state safety.\n  8) How you handle follow-up messages sent while the agent is executing (queue vs interrupt/insert), including a stop mechanism.\n  9) The top 3 metrics you’d track to know the system is delivering value.\n\n  Keep it practical: bullet points and a short sequence diagram (text is fine) are encouraged.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates strong hosted-agent infrastructure reasoning (not generic microservices fluff).\n\n    Score 1.0 if the answer covers MOST of the following with concrete, correct design choices:\n    - Prebuilt environment image strategy that moves setup work to build time (repo clone, deps install, initial build/tests/caches) AND mentions a regular rebuild cadence or equivalent.\n    - Warm pool and/or predictive pre-warming triggered before prompt submission (e.g., “when user starts typing” or equivalent early signal), with rationale for TTFT.\n    - Snapshot/restore strategy with at least two meaningful snapshot points (e.g., base snapshot after image build, session snapshot after changes, snapshot before exit) and why it helps speed/resume.\n    - Git auth split: non-user credential (e.g., GitHub App installation token) for cloning/builds AND user-based attribution for commits/PRs; explicitly mentions setting git user.name/user.email to the prompting user (or equivalent attribution mechanism).\n    - Read/write gating during sync: allows reading files before git sync completes, but blocks edits/writes until sync is complete (or an equally safe alternative that preserves fast research start).\n    - Server-first agent architecture + thin clients concept OR a clear separation of sandbox execution, API/state, and clients.\n    - Per-session isolated state storage (e.g., SQLite per session, durable object per session, or equivalent) and multi-client synchronization model (shared state) plus real-time streaming (WebSockets or equivalent).\n    - Multiplayer support described as “nearly free” given shared state; includes authorship attribution per prompt/user and avoids tying sessions to a single author.\n    - Follow-up message handling choice (queue vs insert/interrupt) with justification AND includes a mechanism to stop/cancel the agent mid-execution.\n    - Metrics include \"sessions resulting in merged PRs\" (or equivalent outcome metric), plus TTFT/time-to-first-token, and at least one PR-quality metric (approval rate/revisions) or usage/concurrency signal.\n\n    Score 0.5 if the answer is generally sensible but misses several key hosted-agent specifics (e.g., mentions containers but not image cadence/warm pool/snapshots; or mentions streaming but not per-session isolation; or git attribution is vague).\n\n    Score 0.0 if the answer is mostly generic system design, lacks a concrete plan to reduce cold-start latency, omits key elements like prebuilt images/warming/snapshots, and does not address git attribution + per-session isolation + multi-client/multiplayer.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hosted-agent infrastructure patterns and tests genuine domain knowledge rather than trivia. It covers critical concepts like prebuilt images, warm pools, snapshot/restore, git attribution, per-session isolation, and multiplayer support. The grader is comprehensive and accepts multiple valid approaches while checking for specific hosted-agent patterns. Minor deductions: (1) The test is somewhat lengthy and could be slightly contrived - while realistic, asking for all 9 components in one prompt is ambitious; (2) The grader criteria are very detailed but could potentially be brittle if a valid alternative architecture is proposed that doesn't match the exact patterns listed. Overall, this is a high-quality test that measures transferable infrastructure knowledge."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis\n# 1) Domain knowledge this skill teaches:\n#    - Designing remote/sandboxed execution for coding agents with fast startup via prebuilt images,\n#      warm pools, snapshot/restore, and predictive warm-up.\n#    - Server-first agent architecture with an API/state layer that supports real-time streaming,\n#      multi-client synchronization (Slack/web/IDE), and multiplayer collaboration.\n#    - AuthN/Z and GitHub flows: app tokens for cloning/builds vs user attribution for commits/PRs.\n# 2) Workflow decisions it enables:\n#    - Prebuild vs on-demand environment setup; when to use warm pools/predictive warm-up.\n#    - Allowing reads before full git sync vs blocking writes until sync completion.\n#    - Queue vs insert handling of follow-up messages; stop/cancel semantics.\n#    - Per-session isolated state (e.g., SQLite/DO) vs shared DB; streaming vs polling.\n# 3) Practitioner struggles without it (test target):\n#    - Naive “spin up VM, git clone, npm install” leads to multi-minute latency and inconsistent envs.\n#    - Cross-session interference from shared state; hard-to-sync Slack/web; poor multiplayer UX.\n#    - Incorrect commit attribution (bot commits), insecure PR flow, and lack of review guardrails.\n\nname: hosted-agent-architecture-low-latency-multiplayer-pr-flow\nskills:\n- hosted-agents\nprompt: |\n  You’re the infra lead for an internal “hosted coding agent” product. It will run AI coding sessions inside remote sandboxes (not on user laptops) and be used from both Slack and a web UI.\n\n  Constraints / requirements:\n  - Peak load: ~200 concurrent sessions across a handful of large repos.\n  - From a cold start, cloning + installing deps + initial build/test takes 5–10 minutes.\n  - UX SLO: after the user hits Enter, time-to-first-agent-token should be dominated by the LLM provider (i.e., your infra should add as little latency as possible).\n  - The agent must be able to: read/edit files, run commands/tests, commit/push a branch, and open a PR.\n  - Commits and PRs must be attributed to the prompting user (not a bot), and users must not be able to “rubber-stamp” their own changes.\n  - “Multiplayer” sessions: a session link can be shared so multiple teammates can watch the live stream and optionally send messages; users may switch between Slack ↔ web mid-session and state must stay consistent.\n  - Users frequently send follow-up messages while the agent is executing a long command (tests, builds).\n\n  Task:\n  Propose a concrete architecture and execution flow for v1. Include:\n  1) Sandbox lifecycle strategy to minimize startup latency (how environments are prepared, updated, and reused).\n  2) API/state layer design that prevents cross-session interference and supports multi-client + multiplayer synchronization.\n  3) Real-time streaming approach for tokens + tool status + file changes.\n  4) GitHub auth/authorization + PR creation flow that achieves user-based attribution and prevents self-approval.\n  5) A policy for handling follow-up messages during execution (e.g., queue vs immediate insert) and how to support “stop/cancel”.\n  6) 3–5 metrics you would track to know the system is working.\n\n  Keep the answer implementation-oriented (components + flows + key tradeoffs), not a generic essay.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Judge whether the response demonstrates practical hosted-agent infrastructure design choices that materially improve latency, scalability, and collaboration.\n\n    Score 1.0 if the answer includes, in a coherent end-to-end design:\n\n    - A clear startup-latency mitigation that goes beyond “use faster machines”, such as:\n      * prebuilt/baked environment images (repo + deps + build/test caches) with periodic rebuilds, AND\n      * warm pools and/or predictive warm-up (pre-warmed sandboxes before prompt submission).\n      (Equivalent concepts like “golden images”, “baked snapshots”, “pre-provisioned runners” are acceptable.)\n    - Some form of snapshot/restore or reuse strategy for follow-up prompts (or otherwise explains how repeat sessions avoid re-running setup).\n    - An API/state layer plan that explicitly isolates per-session state (e.g., per-session DB/Durable Object/isolated storage) and explains how Slack + web share the same session state (server-first/thin clients is acceptable).\n    - Real-time streaming for both model tokens and tool/file events (WebSockets/SSE/event stream), not just polling; mentioning cost control like hibernation/idle suspension is a plus but not required.\n    - Multiplayer support details: session not hard-bound to a single author; per-message authorship is tracked; shared link/watchers; consistent state across clients.\n    - Correct Git/GitHub attribution and auth flow:\n      * cloning/build access can use an app/bot token, but commits set git user.name/email to the prompting user, AND\n      * PR creation uses the user’s GitHub auth (or an equivalent “on behalf of user” mechanism),\n      * includes at least one guardrail against self-approval (e.g., branch protections, review rules, preventing author approval).\n    - A thoughtful follow-up-message policy (queue vs insert) with justification and a “stop/cancel” mechanism.\n    - Metrics that include at least time-to-first-token/latency and a business-value metric such as “sessions resulting in merged PRs” (other reasonable value metrics accepted).\n\n    Score 0.5 if the answer is generally plausible but misses two or more key hosted-agent specifics above (e.g., suggests generic autoscaling without prebuilt images/warm pools; lacks per-session isolation; omits user-based attribution; or doesn’t address multiplayer synchronization), yet still provides some concrete components/flows.\n\n    Score 0.0 if the answer is mostly generic cloud architecture talk with no actionable plan for prebuilt images/warm environments, no clear session state isolation + multi-client sync, and no correct user-attributed GitHub/PR flow.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hosted-agent infrastructure by requiring concrete architectural decisions (prebuilt images, warm pools, per-session isolation, user attribution) rather than trivia. The grader is comprehensive and accepts multiple valid approaches while checking for genuine domain knowledge. Minor deduction: the test is somewhat lengthy and could be slightly more focused, and the grader criteria, while thorough, might be challenging to apply consistently across diverse valid solutions."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "Analysis:\n1. Domain knowledge: The skill teaches architectural patterns for hosted agent infrastructure—sandbox image management, warm pools, predictive warm-up, snapshot/restore flows, self-spawning agents, per-session state isolation, API synchronization, multiplayer collaboration, authentication strategies, and client integrations (Slack, web, Chrome). It emphasizes remote, sandboxed execution with prebuilt environments and server-first frameworks.\n2. Workflow decisions: Knowing when to pre-build images vs on-demand setup, when to allow file reads before git sync, choosing queue vs insert follow-up handling, deciding to spawn sub-agents, structuring server-first agent frameworks with plugin systems, and attributing commits per user. It guides tradeoffs between latency and consistency, concurrency scaling, and metrics to track.\n3. Practitioner struggle: Without this skill, one might fail to deliver low-latency hosted agents, neglect warm pools or predictive warm-up causing slow starts, forget per-session isolation leading to cross-session interference, mis-handle Git auth/commit attribution, or lack multiplayer-ready state syncing. Also might track wrong metrics or implement only single-client flows.\n\n```yaml\nname: low-latency-multiplayer-agent-architecture\nskills:\n  - hosted-agents\nprompt: |\n  Your startup’s hosted coding agent currently spins up ad-hoc cloud VMs per request, causing 90-second cold starts and preventing multiplayer Slack sessions. You’ve been asked to redesign the system so that a Slack-based background agent can:\n  • Respond in under 10 seconds even during peak load\n  • Let multiple teammates co-edit the same session from Slack and the web UI\n  • Attribute all git commits/PRs to the prompting user while still using a GitHub App for repo access\n  • Allow agents to spawn research sub-sessions without blocking the main session\n\n  Describe a concrete architecture and operational plan that meets these goals. Address:\n  1. Sandbox/image strategy (build cadence, warm pools, snapshot/restore, predictive warm-up)\n  2. API/state layer design for multiplayer synchronization and per-session isolation\n  3. Authentication/authorization flow from sandbox edits to GitHub PR creation\n  4. How self-spawned sessions would be orchestrated and monitored\n  5. Metrics you would track to prove success\n\n  Be specific about why each choice solves the stated problems. Avoid generic advice.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates practical mastery of hosted agent infrastructure.\n\n    Score 1.0 if:\n    - Proposes pre-built image cadence plus warm pool/predictive warm-up AND snapshots to achieve <10s starts.\n    - Explains per-session isolated state (e.g., dedicated DB) with server-first API syncing Slack/web clients for multiplayer.\n    - Details GitHub auth: sandbox uses app token for cloning, commits attributed to user via user tokens when creating PRs.\n    - Describes self-spawning sessions with tooling to monitor/check-in while main session continues.\n    - Lists relevant metrics (merged PRs, time to first token/response, PR approval, etc.) tied to goals.\n\n    Score 0.5 if:\n    - Covers at least two of the above areas with correct hosted-agent-specific reasoning but misses others or stays vague.\n    - Mentions latency optimizations or multiplayer/state handling but lacks authentication or self-spawning specifics.\n\n    Score 0.0 if:\n    - Provides generic cloud advice without referencing hosted-agent concepts (warm pools, snapshots, per-session state, GitHub user attribution, self-spawning) or is largely irrelevant/incorrect.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of hosted agent infrastructure by requiring synthesis of multiple architectural patterns (image pre-building, warm pools, per-session isolation, multiplayer sync, GitHub auth flows, self-spawning). The grader is robust, accepting various valid implementations while checking for core concepts. However, the test is somewhat lengthy and could potentially be solved by a model with general distributed systems knowledge rather than requiring the specific hosted-agent patterns (e.g., the 30-minute image cadence, predictive warm-up timing, snapshot/restore specifics). The realism is excellent—this mirrors actual production challenges. Minor deduction for slight risk that a well-reasoned generic cloud architecture answer might score partial credit without demonstrating the skill's unique insights."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: architecting-instant-agent-infrastructure\nskills:\n  - hosted-agents\nprompt: |\n  I'm tech lead for our internal AI coding assistant. We have a problem: our repo is huge (5GB) and dependency installation takes forever.\n  \n  Currently, when a developer sends a request, we spin up a fresh container, clone the repo, and install dependencies. It takes about 8 minutes before the agent creates its first file. Users are abandoning it.\n  \n  We need the startup time to feel \"instant\" (under 10 seconds), but the agent obviously needs access to the latest code commits.\n  \n  Propose an architecture for the sandbox infrastructure that solves this latency. specifically focusing on:\n  1. How we handle the heavy build steps (cloning/installing)\n  2. How we ensure the code is up-to-date without waiting for a full clone\n  3. Triggers for when to start the environment to shave off extra seconds\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the \"hosted-agents\" skill patterns for reducing latency.\n\n    Score 1.0 if the response includes ALL of the following:\n    - **Image Registry/Pre-builds**: Suggests building environment images on a regular cadence (e.g., every 30 mins) containing the repo and dependencies.\n    - **Warm Pool**: Mentions maintaining a pool of running/ready sandboxes from these images.\n    - **Delta Sync**: Explains that at startup, we only need to pull/sync the git changes since the last image build (not a full clone).\n    - **Predictive Warm-up**: Suggests triggering the sandbox initialization/assignment *before* the user submits the prompt (e.g., when they start typing).\n\n    Score 0.5 if the response includes:\n    - Pre-built images and Warm Pools.\n    - BUT misses the specific \"Predictive Warm-up\" (triggering on typing) or fails to explain how to handle the latest code changes (Delta Sync) effectively.\n\n    Score 0.0 if the response:\n    - Only suggests standard caching (e.g., \"cache node_modules\").\n    - Suggests running locally.\n    - Fails to address the architectural change to pre-warmed remote sandboxes.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively measures transferable knowledge about reducing latency in remote development environments through pre-building, warm pools, and predictive optimization—concepts applicable beyond this specific skill. The grader is robust and accepts multiple valid architectural approaches while focusing on core patterns rather than exact terminology. However, it loses some points on realism: while the scenario is plausible, the specific combination of requirements (8-minute startup, 5GB repo, need for <10s response) feels somewhat contrived to test the skill's specific patterns rather than emerging organically from common practice. The test would be stronger if framed as a more general 'reduce our agent startup time' problem without the precise numerical constraints."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill focuses on the architecture and infrastructure required to run LLM agents in remote, sandboxed environments. Key concepts include environment isolation (sandboxing), state management (API layers vs. clients), and latency optimization (pre-built images, snapshots, and warm pools). It moves beyond \"how to write a prompt\" into \"how to build the system that executes agent code.\"\n\n2.  **What workflow decisions does it enable?**\n    It enables decisions regarding system latency and scalability:\n    - Choosing **Image Registry Patterns** vs. on-the-fly environment setup.\n    - Implementing **Predictive Warm-up** based on user activity rather than submission.\n    - Selecting a **Server-First Architecture** to support multiple clients (Slack, Web, IDE) and multiplayer state synchronization.\n    - Deciding how to handle **Git configuration and authentication** for background agents (User-based commits vs. App-based).\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner would likely build a system that is slow (waiting minutes for `npm install` or `git clone` on every session), brittle (environments drifting from local state), and limited to a single user (no way for a team to collaborate on an agent's progress). They might also struggle with \"agent identity\" issues, where all PRs appear to come from a generic bot account rather than the user who triggered the task.\n\n---\n\n### Test Case\n\n```yaml\nname: hosted-agent-infra-optimization\nskills:\n- hosted-agents\nprompt: |\n  You are architecting a \"Background Coding Agent\" platform where agents autonomously fix bugs in large GitHub repositories. \n\n  Current prototype issues:\n  1. Startup Latency: It takes ~3 minutes for a sandbox to spin up because it has to clone the repo and install heavy node_modules/dependencies every time.\n  2. Collaboration: When a user shares a link to an active agent session with a teammate, the teammate cannot see the live logs or the agent's current filesystem state.\n  3. Git Identity: All Pull Requests created by the agent are attributed to a generic \"System Bot\" account, making it hard to track which human initiated the work.\n\n  Propose a technical architecture to solve these three specific problems. Your solution should focus on infrastructure patterns for hosted execution and state management.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on its technical approach to hosted agent infrastructure:\n\n    Score 1.0 if the response includes ALL of the following:\n    - Speed Optimization: Proposes an Image Registry pattern (pre-building images with dependencies) or Filesystem Snapshots/Restoration to bypass the setup phase.\n    - Latency Reduction: Mentions a Warm Pool strategy or Predictive Warm-up (starting the environment when the user starts typing/interacting).\n    - Architecture: Suggests a Server-First architecture where state is isolated per session and synchronized via an API/WebSockets to enable multiplayer/multi-client views.\n    - Identity: Recommends using the prompting user's GitHub identity/tokens for commits and PRs rather than a generic bot account.\n\n    Score 0.5 if the response:\n    - Suggests pre-building images/snapshots but misses the warm pool/predictive logic.\n    - OR provides a good speed optimization but fails to address the multiplayer/server-first state synchronization.\n    - OR addresses infrastructure but fails to handle the Git identity/attribution problem.\n\n    Score 0.0 if:\n    - The response suggests local execution instead of hosted infrastructure.\n    - The response only gives generic AI advice (e.g., \"use a better prompt\") without addressing the infrastructure/latency issues.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This is a strong test case that evaluates transferable infrastructure knowledge rather than skill-specific trivia. The test measures understanding of pre-building environments, state synchronization, and identity management—concepts that apply broadly to distributed systems. The grader is mostly robust, accepting multiple valid approaches (image registry OR snapshots, warm pools OR predictive warm-up). However, it loses some points on clarity because it requires ALL four elements for full credit when some combinations could reasonably solve the stated problems (e.g., a solution focusing heavily on speed optimization with warm pools might partially address collaboration through faster session sharing). The realism is excellent—these are genuine pain points practitioners face when building hosted agent systems. Minor deduction for the grader being slightly rigid in requiring all four specific patterns when alternative valid architectures exist."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "```yaml\nname: hosted-agent-infrastructure-decisions\nskills:\n  - hosted-agents\nprompt: |\n  You're the technical lead at a growing startup building AI-powered development tools. Your team wants to build a hosted coding agent that multiple engineers can use collaboratively, but you're struggling with infrastructure decisions.\n\n  Current situation:\n  - 8 engineers want to use it simultaneously\n  - Working on multiple microservices (each in separate repos)\n  - Need to handle commits and PR creation properly\n  - Users expect near-instant startup times\n  - Need real-time collaboration features\n\n  Your task: Design the infrastructure architecture for this hosted agent system. Specifically address:\n\n  1. **Session Management**: How will you handle multiple concurrent users without resource contention?\n  \n  2. **Authentication Flow**: How will the agent create commits and PRs that are attributed to the actual user, not a service account?\n  \n  3. **Speed Optimization**: What strategies will you use to ensure sessions start quickly, even when users haven't used the system before?\n  \n  4. **Multiplayer Features**: How will you enable real-time collaboration where multiple team members can see and interact with the same agent session?\n  \n  5. **Repository Handling**: What's your approach for managing work across multiple repositories with different dependency requirements?\n\n  Provide specific architectural decisions and explain the trade-offs you're making.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response for understanding of hosted agent infrastructure concepts. Score based on:\n\n    **Score 1.0 (Full Credit)** if the response demonstrates:\n    - Clear understanding of when hosted agents are appropriate vs local execution\n    - Specific architectural patterns for session isolation (per-session databases/sandboxes)\n    - Proper authentication flow using user tokens/GitHub apps for attribution\n    - Speed optimization strategies (image registry, predictive warm-up, parallel operations)\n    - Multiplayer architecture using state synchronization across clients\n    - Trade-off analysis between different approaches\n\n    **Score 0.5 (Partial Credit)** if the response:\n    - Shows basic understanding of some concepts but lacks specificity\n    - Misses key architectural patterns or authentication details\n    - Provides solutions but doesn't explain trade-offs\n    - Covers most areas but with gaps in implementation details\n\n    **Score 0.0 (No Credit)** if the response:\n    - Focuses on local agent execution without addressing hosted infrastructure\n    - Lacks understanding of authentication and attribution requirements\n    - Doesn't address scaling/concurrency challenges\n    - Missing key concepts like session isolation or state management\n    - Provides overly simplistic or infeasible solutions\n\n    Key concepts that should be present:\n    - Session isolation and per-session state management\n    - User-based authentication for commits/PRs\n    - Speed optimization through pre-building and predictive warm-up\n    - Real-time synchronization for multiplayer features\n    - Trade-offs between complexity and functionality\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable infrastructure knowledge about hosted agents, session isolation, authentication flows, and speed optimization strategies that apply broadly to distributed systems. The grader is robust and accepts multiple valid architectural approaches while checking for key concepts. However, it loses some points on realism - while the scenario is plausible, the prompt structure (numbered list of specific questions) feels somewhat contrived compared to how a technical lead would naturally approach this problem. A more realistic framing might be 'write an architecture doc' or 'present to the team' rather than answering five specific questions."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze the skill document:\n\n1. **What domain knowledge does this skill teach?** \n   - Architecture patterns for remote sandboxed agent execution vs local execution\n   - Strategies for achieving near-instant session starts despite complex environment setup\n   - Design patterns for multiplayer agent sessions with shared state\n   - Self-spawning agent architectures for parallel task execution\n   - State isolation and synchronization across multiple client interfaces\n\n2. **What workflow decisions does it enable?**\n   - When to use hosted vs local agent infrastructure\n   - How to optimize cold start times (image registry patterns, predictive warm-up, warm pools)\n   - When agents should spawn sub-agents vs handling everything sequentially\n   - How to structure agent frameworks for multi-client support\n   - Trade-offs in handling concurrent user messages during execution\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Designing a system where users don't wait minutes for environments to spin up\n   - Understanding how to enable true multiplayer collaboration in agent sessions\n   - Architecting systems where agents can intelligently parallelize work\n   - Building infrastructure that scales beyond single-machine constraints\n\n## Test Case\n\n```yaml\nname: hosted-agent-architecture-design\nskills:\n  - hosted-agents\nprompt: |\n  You're architecting a hosted coding agent system for a 500-person engineering organization. The system needs to:\n  \n  1. Allow engineers to start coding sessions from Slack that feel \"instant\" (under 5 seconds to first response)\n  2. Support multiple engineers collaborating in the same session simultaneously\n  3. Handle repositories with 20+ minute dependency installation times\n  4. Allow agents to break large refactoring tasks into parallel subtasks\n  \n  Current pain points with the existing local agent setup:\n  - Engineers wait 15-20 minutes for environments to initialize\n  - Only one person can use the agent at a time on their machine\n  - Large refactoring tasks take hours running sequentially\n  - Different engineers get different results due to local environment differences\n  \n  Describe your architectural approach addressing:\n  - How you'll achieve sub-5-second session start times despite long setup requirements\n  - The infrastructure pattern for enabling multiple engineers in one session\n  - How agents will spawn and coordinate parallel work\n  - Your strategy for maintaining environment consistency across sessions\n  \n  Be specific about the technical patterns and trade-offs in your design.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of hosted agent architecture patterns for production systems.\n    \n    Score 1.0 if the response includes ALL of:\n    - A pre-building/caching strategy (image registry, warm pools, or similar) to achieve fast starts\n    - A clear multiplayer architecture with state synchronization across clients\n    - A self-spawning pattern where agents can create sub-sessions for parallel work\n    - Environment consistency approach (sandboxes, containers, or similar isolation)\n    - Reasonable technical details showing understanding of the trade-offs\n    \n    Score 0.5 if the response includes SOME but not all of:\n    - Some form of environment pre-building or caching (but may be incomplete)\n    - Mentions multiplayer support but lacks architectural detail\n    - Discusses parallelization but without clear agent spawning patterns\n    - Shows partial understanding of the speed vs setup time trade-off\n    \n    Score 0.0 if the response:\n    - Suggests waiting for full environment setup on each session start\n    - Doesn't address the multiplayer requirement meaningfully\n    - Lacks understanding of how to parallelize agent work\n    - Proposes solutions that don't solve the core latency problem\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (non-obviousness: 8/10) by testing architectural patterns like pre-building, multiplayer state, and self-spawning rather than trivia. The grader is mostly robust (clarity: 7/10) and accepts multiple valid approaches, though it could be slightly more flexible about specific implementation details. The scenario is highly realistic (realism: 9/10) - architecting hosted agent infrastructure for a real organization is exactly what practitioners would do. However, the test could be improved by being more explicit about accepting alternative valid patterns (e.g., different caching strategies beyond image registries) to avoid potential brittleness."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\nThe hosted-agents skill teaches how to architect remote sandboxed agent infrastructure to overcome limitations of local execution. It covers domain knowledge such as image registry and snapshot strategies for fast session startup, per-session state isolation, API synchronization, multiplayer collaboration, authentication flows, client interfaces, and warm pool/predictive warm-up tactics. The workflow decisions it enables include when to pre-build images versus on-demand setups, how to structure agent frameworks (server-first with clients), when to warm sandboxes predictively, how to handle multiplayer synchronization, and when/where to attribute commits to prompting users. A practitioner lacking this skill would struggle with designing scalable hosted agent systems: they'd miss rapid session responsiveness, reliable sandbox state, coordinated client sync, multiplayer readiness, and proper authentication/PR workflows. \n\n**Test Case**\n\n```yaml\nname: design-hosted-agent-session-workflow\nskills:\n- hosted-agents\nprompt: |\n  You are an infrastructure architect tasked with designing the onboarding workflow for a hosted coding agent that multiple developers will use concurrently through Slack, a web client, and a Chrome extension. Describe how you would architect:\n  1. The sandbox provisioning process to ensure sub-30-second session startup while keeping environments reproducible.\n  2. How session state and git metadata stay isolated per client, yet allow commits on behalf of the user triggering the session.\n  3. The synchronization strategy that lets a web user, Slack user, and Chrome extension all observe the same agent progress in real time.\n  Include specific infrastructure layers, image/snapshot strategy, auth flows, and how you keep multiplayer sessions consistent.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria explaining what a correct answer looks like.\n\n    Score 1.0 if: The response details a multi-layered architecture with sandbox/image registry/pre-warm strategies for fast startup, per-session state isolation, GitHub authentication tokens for user commits, and synchronization across Slack, web, and Chrome clients (mentioning shared state, WebSocket updates, or similar). It should also reference multiplayer consistency (e.g., attributing actions, queueing or streaming updates).\n    Score 0.5 if: The response addresses some but not all areas (e.g., describes fast sandbox provisioning but omits user-authored commits or client sync) or remains high-level without linking layers to the provided prompts.\n    Score 0.0 if: The response fails to explain sandbox provisioning, session isolation, or client synchronization in a meaningful way.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong realism (8/10) by asking practitioners to design a multi-client hosted agent system, which is a genuine architectural task. Clarity is good (7/10) - the grader accepts multiple valid approaches and tests actual capability, though it could be more specific about what constitutes 'meaningful' explanation. Non-obviousness is moderate (6/10) - while it tests transferable knowledge about distributed systems, session isolation, and real-time sync, the prompt structure closely mirrors the skill document's organization (sandbox provisioning, state isolation, auth flows, client sync), making it somewhat derivative rather than testing whether the knowledge transfers to novel scenarios. A stronger test would present a different problem context that requires applying these patterns without directly echoing the skill's structure."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n\n## Analysis\n\n**Domain Knowledge:** This skill teaches the core principles of building hosted agent infrastructure - remote sandboxed execution environments that enable scalable, collaborative AI coding agents beyond local machine limitations.\n\n**Workflow Decisions:** The skill emphasizes critical architectural choices like server-first agent frameworks, pre-built environment images vs cold starts, queue vs immediate message handling, and user-based vs app-based authentication flows.\n\n**Practitioner Struggles:** Without this skill, practitioners would struggle with fundamental hosted agent challenges: slow session startup times, resource contention at scale, environment inconsistency, and building collaborative features that actually work across multiple clients.\n\n## Test Case\n\n```yaml\nname: hosted-agent-architecture-decisions\nskills:\n- hosted-agents\nprompt: |\n  You're a lead architect designing a hosted coding agent platform for a team of 50 developers. The platform needs to handle 100+ concurrent coding sessions with near-instant startup times, support collaborative debugging, and integrate with the team's existing GitHub workflows.\n\nYour team is debating three core architectural approaches:\n\n1. **Local Agent with Remote Storage**: Each developer runs agents locally, but all state is stored in a centralized cloud database\n2. **Cold-Start Sandboxes**: Spin up fresh environments for each session with on-demand dependency installation \n3. **Pre-Built Sandbox Pool**: Maintain a warm pool of pre-configured sandboxes that are ready to use\n\nFor the recommended approach, explain:\n- Why it's the best choice for this use case\n- How you'd handle session startup performance and user authentication\n- The key technical patterns you'd implement for multiplayer collaboration\n- How you'd measure success beyond just session counts\n\nFocus on the architectural reasoning and trade-offs, not implementation details.</arg_value>\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the response demonstrates understanding of hosted agent infrastructure principles:\n\n    Score 1.0 if: \n    - Correctly recommends Pre-Built Sandbox Pool (#3) as the optimal approach\n    - Explains key benefits: near-instant startup, environment consistency, unlimited concurrency\n    - Describes critical technical patterns: user-based authentication/commits, per-session state isolation, real-time streaming\n    - Mentions proper success metrics: merged PRs, approval rates, agent-written code percentage\n    - Shows understanding of multiplayer implementation requirements\n\n    Score 0.5 if:\n    - Recommends the right approach but provides weak explanations\n    - Covers some technical patterns but misses critical ones like user authentication or state isolation\n    - Mentions some metrics but not the primary ones\n    - Shows partial understanding of architectural trade-offs\n\n    Score 0.0 if:\n    - Recommends the wrong approach (local agent or cold-start)\n    - Focuses only on basic infrastructure without addressing the collaborative/scale requirements\n    - Demonstrates misunderstanding of core hosted agent principles\n    - Provides generic advice without referencing specific hosted agent patterns\n\n    The response should demonstrate mastery of hosted agent concepts rather than generic cloud architecture knowledge.</arg_value>\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of hosted agent architecture with realistic scenario and robust grading criteria. Non-obviousness (7/10): Tests genuine architectural trade-offs and patterns that transfer across contexts, though some details like 'pre-built sandbox pool' are somewhat skill-specific. Clarity (9/10): Grader is well-structured, accepts multiple valid explanations, and focuses on conceptual understanding rather than exact terminology. Realism (8/10): Represents a genuine architectural decision practitioners face when building hosted agent systems. The test effectively measures transferable domain knowledge about remote execution environments, scalability patterns, and collaborative infrastructure rather than pure trivia."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: hosted-agent-architecture-design\nskills:\n- hosted-agents\nprompt: |\n  Design a hosted agent infrastructure for a multiplayer coding environment. \n  The system should allow for unlimited concurrency, consistent execution environments, \n  and collaborative workflows. Consider the architecture, sandbox infrastructure, \n  agent framework selection, speed optimizations, and multiplayer support.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes all of the following:\n    - A description of the architecture with three layers: sandbox infrastructure, API layer, and client interfaces\n    - An explanation of the sandbox infrastructure, including image registry pattern, snapshot and restore, and warm pool strategy\n    - A discussion of agent framework selection, including server-first architecture and plugin system requirements\n    - A description of speed optimizations, including predictive warm-up, parallel file reading, and maximizing build-time work\n    - An explanation of multiplayer support, including data model, authorship, and session link sharing\n\n    Score 0.5 if the response includes some of the above elements, but lacks detail or clarity.\n\n    Score 0.0 if the response does not address the key elements of hosted agent infrastructure design.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "This test fails on multiple criteria. For **non-obviousness** (score: 3/10), it's essentially asking the model to regurgitate the skill document's structure verbatim - requiring specific terminology like 'image registry pattern,' 'warm pool strategy,' and 'three layers' is pure memorization of skill-specific content rather than testing transferable domain knowledge. For **clarity** (score: 4/10), the grader is overly prescriptive, requiring exact concepts from the skill document when valid alternative architectures exist (e.g., container orchestration vs. image registry, different caching strategies). For **realism** (score: 3/10), while hosted agent infrastructure is a real domain, this prompt is artificial - it asks to 'design' an entire system covering five specific areas that happen to match the skill's table of contents, rather than solving a concrete problem a practitioner would face (e.g., 'reduce session startup time from 2 minutes to under 5 seconds' or 'enable multiple users to collaborate on the same coding session')."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "## Analysis\n\n### What domain knowledge does this skill teach?\n\nThe skill document teaches the following core concepts:\n- **Sandbox Infrastructure**: Techniques for spinning up development environments quickly, including pre-built environment images, file system snapshots, and warm pool strategies.\n- **Agent Framework Selection**: Criteria for choosing server-first architectures with multiple custom clients, code as the source of truth, and plugin systems.\n- **Speed Optimizations**: Strategies for predictive warm-up, parallel file reading, and maximizing build-time work.\n- **Self-Spawning Agents**: Tools for agents to spawn new sessions, read status, and continue work in parallel.\n- **API Layer**: Requirements for per-session state isolation, real-time streaming, and synchronization across clients.\n- **Multiplayer Support**: Benefits and implementation requirements for multiplayer sessions.\n- **Authentication and Authorization**: Techniques for user-based commits and sandbox-to-API flows.\n- **Client Implementations**: Details for integrating with Slack, web interfaces, and Chrome extensions.\n\n### What workflow decisions does it enable?\n\nThe skill enables the following workflow decisions:\n- **When to use hosted agents** for background tasks, sandboxed execution, multiplayer sessions, and multi-client interfaces.\n- **How to design sandbox infrastructure** for fast and consistent environment setup.\n- **Which agent framework to choose** based on server-first architecture, code as the source of truth, and plugin systems.\n- **Optimizations for speed and efficiency** by pre-warming sandboxes, allowing parallel file reads, and maximizing build-time work.\n- **Implementing self-spawning agents** for parallel subtask execution and research across repositories.\n- **Designing the API layer** for real-time updates and state synchronization.\n- **Supporting multiplayer sessions** for collaborative work.\n- **Handling authentication and authorization** for secure and user-specific operations.\n- **Choosing and implementing client interfaces** like Slack, web, and Chrome extensions.\n\n### What would a practitioner actually struggle with without this skill?\n\nWithout this skill, a practitioner would struggle with:\n- **Slow and inconsistent environment setup** leading to delays and inconsistencies in development workflows.\n- **Inefficient resource utilization** due to lack of optimized sandbox and agent infrastructure.\n- **Limited scalability** in terms of concurrency and handling multiple users.\n- **Complexity in managing state and synchronization** across different clients and sessions.\n- **Security and authorization issues** when managing user-specific operations and commits.\n- **Poor user experience** in collaborative and multiplayer environments.\n- **Suboptimal client implementations** that do not leverage the full potential of the hosted agent infrastructure.\n\n## Test Case\n\n```yaml\nname: design-hosted-agent-infrastructure\nskills:\n  - hosted-agents\nprompt: |\n  You are tasked with designing a hosted agent infrastructure for a new collaborative coding platform. The platform should support:\n  - Fast and consistent environment setup\n  - Unlimited concurrency and scalability\n  - Real-time collaboration and multiplayer sessions\n  - Integration with multiple clients (Slack, web, Chrome extensions)\n  - Secure and user-specific operations\n\n  Describe the architecture you would implement, including:\n  - How you would handle environment setup and sandbox infrastructure\n  - The agent framework you would choose and why\n  - Speed optimizations you would implement\n  - How you would support self-spawning agents\n  - The design of the API layer for state management and client coordination\n  - How you would ensure secure and user-specific operations\n  - The client implementations you would prioritize and why\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if: The response comprehensively covers all aspects of the task, including sandbox infrastructure, agent framework selection, speed optimizations, self-spawning agents, API layer design, secure operations, and client implementations. The solution is well-structured and demonstrates a deep understanding of the domain.\n\n    Score 0.5 if: The response covers most aspects of the task but may lack depth or detail in one or two areas. The solution is generally sound but could be more comprehensive.\n\n    Score 0.0 if: The response is incomplete, missing critical components, or demonstrates a lack of understanding of the domain.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test case fails on multiple criteria. For **non-obviousness** (score: 3/10), it asks candidates to regurgitate the entire skill document rather than testing transferable domain knowledge - a practitioner would need to apply specific patterns, not describe everything. For **clarity** (score: 4/10), the grader is too vague ('comprehensively covers all aspects') and doesn't specify what constitutes valid solutions for each component, making it difficult to grade consistently. For **realism** (score: 4/10), while designing infrastructure is realistic, asking someone to describe an entire architecture covering 7+ distinct areas in one response is contrived - real practitioners would tackle these incrementally, not in a single comprehensive design document."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Git History Surgery",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 7,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Git History Surgery\n\n**SKILL.md Content:**\n```markdown\n---\nname: Git History Surgery\ndescription: Safely rewrite, clean up, and recover Git history using rebase/cherry-pick/revert and reflog without breaking shared branches.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Git History Surgery\n\n## Overview\n\nRewriting Git history is powerful and dangerous. The core safety rule is:\n\n**Do not rewrite commits that other people have based work on.**\n\nIf a commit is already on a shared branch (e.g., `main`), prefer **revert** over rebase/reset.\n\n## Decision Tree\n\n- Need to fix the last commit message/content? → `git commit --amend`\n- Need to reorder/squash/drop commits on a branch not yet shared? → `git rebase -i`\n- Need to move specific commits between branches? → `git cherry-pick`\n- Need to undo a bad merge/rewrite? → `git reflog` + `git reset --hard`\n\n## Workflow: Interactive Rebase (Safe Version)\n\n1. Ensure clean status: `git status` (no uncommitted changes).\n2. Create a backup pointer: `git branch backup/<name>`.\n3. Start interactive rebase: `git rebase -i <base>`.\n4. Use actions intentionally:\n   - `reword`: change message\n   - `edit`: modify content\n   - `squash`/`fixup`: combine commits\n   - `drop`: remove a commit\n5. Resolve conflicts:\n   - Fix files, `git add`, then `git rebase --continue`.\n6. Run tests and sanity checks.\n7. If the branch was pushed before, you’ll likely need:\n   - `git push --force-with-lease` (safer than `--force`).\n\n## Recovery Patterns\n\n- Abort an in-progress rebase: `git rebase --abort`\n- Find “where things were”: `git reflog`\n- Restore the branch: `git reset --hard <reflog_hash>`\n- Recover a dropped commit: find it in reflog and `git cherry-pick <hash>`\n\n## Common Pitfalls\n\n- **Force pushing over someone else’s work**: use `--force-with-lease` and coordinate.\n- **Rebasing merges incorrectly**: if you need to preserve merges, consider `--rebase-merges`.\n- **Losing local changes**: make a backup branch or stash before surgery.\n- **Accidentally rewriting `main`**: avoid history rewrites on protected branches.\n\n## Checklist\n\n- [ ] Confirm the branch is safe to rewrite (not shared)\n- [ ] Create a backup branch\n- [ ] Use `--force-with-lease` if pushing rewritten history\n- [ ] Use reflog for recovery\n- [ ] Run tests after rewriting\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Git History Surgery\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Git History Surgery\n\n**SKILL.md Content:**\n```markdown\n---\nname: Git History Surgery\ndescription: Safely rewrite, clean up, and recover Git history using rebase/cherry-pick/revert and reflog without breaking shared branches.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Git History Surgery\n\n## Overview\n\nRewriting Git history is powerful and dangerous. The core safety rule is:\n\n**Do not rewrite commits that other people have based work on.**\n\nIf a commit is already on a shared branch (e.g., `main`), prefer **revert** over rebase/reset.\n\n## Decision Tree\n\n- Need to fix the last commit message/content? → `git commit --amend`\n- Need to reorder/squash/drop commits on a branch not yet shared? → `git rebase -i`\n- Need to move specific commits between branches? → `git cherry-pick`\n- Need to undo a bad merge/rewrite? → `git reflog` + `git reset --hard`\n\n## Workflow: Interactive Rebase (Safe Version)\n\n1. Ensure clean status: `git status` (no uncommitted changes).\n2. Create a backup pointer: `git branch backup/<name>`.\n3. Start interactive rebase: `git rebase -i <base>`.\n4. Use actions intentionally:\n   - `reword`: change message\n   - `edit`: modify content\n   - `squash`/`fixup`: combine commits\n   - `drop`: remove a commit\n5. Resolve conflicts:\n   - Fix files, `git add`, then `git rebase --continue`.\n6. Run tests and sanity checks.\n7. If the branch was pushed before, you’ll likely need:\n   - `git push --force-with-lease` (safer than `--force`).\n\n## Recovery Patterns\n\n- Abort an in-progress rebase: `git rebase --abort`\n- Find “where things were”: `git reflog`\n- Restore the branch: `git reset --hard <reflog_hash>`\n- Recover a dropped commit: find it in reflog and `git cherry-pick <hash>`\n\n## Common Pitfalls\n\n- **Force pushing over someone else’s work**: use `--force-with-lease` and coordinate.\n- **Rebasing merges incorrectly**: if you need to preserve merges, consider `--rebase-merges`.\n- **Losing local changes**: make a backup branch or stash before surgery.\n- **Accidentally rewriting `main`**: avoid history rewrites on protected branches.\n\n## Checklist\n\n- [ ] Confirm the branch is safe to rewrite (not shared)\n- [ ] Create a backup branch\n- [ ] Use `--force-with-lease` if pushing rewritten history\n- [ ] Use reflog for recovery\n- [ ] Run tests after rewriting\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Git History Surgery\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Git History Surgery",
          "skill_dir": "git-history-surgery",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "Looking at this skill document, let me analyze what it teaches:\n\n## Analysis\n\n1. **Core Domain Knowledge:**\n   - SQL-based querying of Hugging Face datasets using DuckDB and the `hf://` protocol\n   - Understanding dataset structure: configs, splits, and parquet file access\n   - Data transformation workflows: filter, join, aggregate, and push results\n   - Template-based dataset creation for different ML task types\n\n2. **Key Workflow Decisions:**\n   - Using `data` as the table alias in SQL queries (gets replaced with actual HF path)\n   - Understanding the `hf://datasets/{id}@~parquet/{config}/{split}/*.parquet` path format\n   - When to query/transform existing datasets vs. create new ones from scratch\n   - How to push query results back to Hub as new datasets\n\n3. **What practitioners would struggle with:**\n   - Understanding how to use SQL to filter/transform HF datasets directly\n   - Knowing the correct path format and protocol for accessing datasets\n   - Understanding the workflow of creating dataset subsets from large public datasets\n\nA realistic test scenario would involve filtering a dataset and understanding how to create a subset - this is a common ML practitioner task.\n\n```yaml\nname: huggingface-dataset-sql-filtering\nskills:\n  - hugging-face-datasets\nprompt: |\n  I have a large Hugging Face dataset called \"squad\" and I want to create a filtered subset containing only examples where the context paragraph is longer than 1000 characters and the question contains the word \"why\". \n  \n  I want to use SQL to query the dataset directly without downloading it first, then push the filtered results to a new private dataset repository.\n  \n  Can you explain:\n  1. How can I query Hugging Face datasets directly with SQL without downloading them first?\n  2. What SQL query would filter for these criteria?\n  3. How would I push the results to a new Hub repository?\n  \n  Please provide a concrete approach with example commands or code.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates knowledge of SQL-based querying of Hugging Face datasets.\n    \n    Score 1.0 if the response:\n    - Explains that DuckDB can query HF datasets via the hf:// protocol (or similar direct access mechanism)\n    - Provides a reasonable SQL query with LENGTH() for context filtering and a string match (LIKE, regexp_matches, or similar) for the \"why\" keyword\n    - Shows how to push/export results to a new repository\n    \n    Score 0.5 if the response:\n    - Shows understanding of SQL-based dataset querying but misses some details\n    - Provides a query that addresses the filtering requirements but may have minor syntax issues\n    - Mentions pushing to Hub but lacks specifics\n    \n    Score 0.0 if the response:\n    - Only suggests downloading the dataset and processing locally with pandas/datasets library\n    - Does not mention SQL-based querying capabilities\n    - Provides generic advice without concrete SQL examples\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It tests transferable knowledge about SQL-based dataset querying (non-obviousness: 8/10) - practitioners genuinely need to understand how to filter and subset large datasets efficiently. The grader is mostly robust (clarity: 8/10), accepting various SQL approaches (LIKE, regexp_matches) and focusing on conceptual understanding rather than exact syntax. The task is highly realistic (realism: 9/10) - creating filtered subsets from large public datasets is a common ML workflow. Minor deductions: the grader could be slightly more explicit about accepting alternative valid approaches (e.g., using the Python API vs CLI), and the prompt could specify whether to use CLI or Python to reduce ambiguity."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze what this skill teaches:\n\n1. **What domain knowledge does this skill teach?**\n   - Dataset querying and transformation using SQL (via DuckDB)\n   - The `hf://` protocol for direct dataset access without full downloads\n   - Dataset lifecycle management (creation, configuration, content updates)\n   - Different dataset formats and when to use them (chat, QA, classification, etc.)\n   - Efficient data subsetting and filtering workflows\n\n2. **What workflow decisions does it enable?**\n   - **When to use SQL queries vs. downloading entire datasets** - Critical for large datasets\n   - How to explore dataset schema before transformations\n   - When to filter/transform data and push to Hub vs. export locally\n   - How to reshape data between formats (e.g., multiple-choice to QA format)\n   - Whether to create subsets based on specific criteria\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Not knowing they can query datasets directly with SQL instead of downloading\n   - Inefficiently downloading entire datasets just to extract a small subset\n   - Not understanding how to explore dataset structure before transformation\n   - Missing the ability to filter and reshape data in one step\n   - Not knowing about the `hf://` protocol for direct access\n\n**Test target**: The core value is understanding that **SQL-based querying allows efficient dataset exploration and subsetting without downloading**, and knowing the workflow for creating filtered/transformed subsets.\n\n```yaml\nname: dataset-filtering-workflow\nskills:\n  - hugging-face-datasets\nprompt: |\n  A data scientist wants to create a training dataset for a medical question-answering model. They've identified a large benchmark dataset on Hugging Face (approximately 15,000 questions across 50+ subjects) that includes medical topics, but they only need the medical-related questions.\n  \n  The dataset has columns: question, choices (array), answer (index), subject\n  \n  They want to:\n  1. First explore what medical subjects are available\n  2. Create a subset with only medical questions (subjects like 'anatomy', 'clinical_knowledge', 'medical_genetics', etc.)\n  3. Transform the format to extract the correct answer text (not just the index)\n  4. Push the result as a new private dataset to their Hugging Face account\n  \n  What approach would you recommend for this workflow? Consider efficiency, especially given the dataset size.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of efficient dataset querying and transformation workflows.\n    \n    Score 1.0 if the response:\n    - Recommends using SQL-based querying to filter the dataset (avoiding full download)\n    - Suggests first exploring the dataset schema/structure (e.g., checking available subjects, column types)\n    - Describes filtering for medical-related subjects using SQL WHERE/IN clauses\n    - Mentions transforming the data to extract correct answers (e.g., using array indexing like choices[answer])\n    - Indicates the result can be pushed directly to Hub as a new dataset\n    - Overall demonstrates understanding that large datasets can be efficiently queried and transformed without downloading\n    \n    Score 0.5 if the response:\n    - Suggests filtering and transformation but proposes downloading the entire dataset first\n    - OR mentions SQL querying but misses key steps like schema exploration or transformation\n    - OR provides a partially efficient workflow but with unnecessary intermediate steps\n    \n    Score 0.0 if the response:\n    - Suggests manually downloading and processing the dataset without mentioning SQL querying capabilities\n    - Shows no awareness of direct dataset querying methods\n    - Provides an inefficient workflow that doesn't leverage available tools for large datasets\n    \n    The response doesn't need to use exact command syntax or specific tool names, but should demonstrate understanding of the efficient workflow pattern.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of transferable domain knowledge (efficient dataset querying vs. downloading, SQL-based filtering workflows) rather than skill-specific trivia. The grader is robust, accepting multiple valid approaches without requiring exact syntax. The scenario is realistic - creating medical QA subsets from large benchmarks is a common ML task. Minor deduction: the grader could be slightly more explicit about accepting alternative efficient approaches (e.g., using dataset streaming APIs), though it does state 'doesn't need exact command syntax.' Overall, this is a high-quality test that measures genuine domain expertise."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this skill document, I need to identify the core knowledge it teaches:\n\n1. **What domain knowledge does this skill teach?**\n   - The existence and use of DuckDB's `hf://` protocol for direct SQL querying of Hugging Face datasets\n   - How to efficiently create dataset subsets without downloading entire datasets\n   - SQL-based data transformation and filtering strategies for ML datasets\n   - The workflow of exploring → filtering → pushing subsets to Hub\n\n2. **What workflow decisions does it enable?**\n   - When to use SQL queries vs downloading full datasets (efficiency for large datasets)\n   - How to create focused training subsets from larger datasets\n   - How to transform dataset formats using SQL operations\n   - When to push filtered results to a new Hub repo vs exporting locally\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - They wouldn't know about the `hf://` protocol capability at all\n   - They'd inefficiently download massive datasets just to extract small subsets\n   - They wouldn't know how to filter and reshape datasets for specific training needs\n   - They'd struggle to create domain-specific training subsets from general datasets\n\nThe most valuable and non-obvious knowledge here is the ability to use SQL directly on Hugging Face datasets via the `hf://` protocol, especially for creating training subsets.\n\n```yaml\nname: hugging-face-dataset-subset-creation\nskills:\n- hugging-face-datasets\nprompt: |\n  You need to create a specialized training dataset for a medical question-answering model. The MMLU dataset on Hugging Face (repository: \"cais/mmlu\") contains multiple-choice questions across many subjects, including medical topics.\n\n  Your task:\n  1. Create a subset containing only medical-related questions (subjects: \"anatomy\", \"clinical_knowledge\", \"medical_genetics\", \"professional_medicine\", \"college_medicine\")\n  2. Transform the data to include just the question text and the correct answer text (not the index)\n  3. Push this subset to a new Hugging Face dataset repository\n\n  The original MMLU dataset has columns: question, choices (array), answer (index), subject\n  \n  Explain your approach and provide the specific commands or code you would use to accomplish this efficiently without downloading the entire dataset.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates knowledge of efficient dataset subset creation from Hugging Face datasets.\n\n    Score 1.0 if the response:\n    - Uses SQL queries directly on the Hugging Face dataset (via hf:// protocol, DuckDB, or similar direct querying method)\n    - Correctly filters for the medical subjects using SQL WHERE clause or equivalent\n    - Transforms the data to extract the correct answer text from the choices array using the answer index\n    - Shows how to push the result to a new Hub repository\n    - Avoids downloading the entire dataset unnecessarily\n\n    Score 0.5 if the response:\n    - Shows a valid approach to create the subset but downloads the entire dataset first\n    - OR correctly identifies the need to filter and transform but has minor SQL syntax issues\n    - OR uses the right approach but misses one aspect (like extracting answer text vs index)\n\n    Score 0.0 if the response:\n    - Doesn't provide a concrete solution\n    - OR suggests only downloading and manually filtering in Python without any efficient method\n    - OR doesn't understand how to create dataset subsets from Hugging Face\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively evaluates transferable knowledge about efficient dataset querying and subset creation using SQL on Hugging Face datasets. It tests genuine domain knowledge (using SQL queries via hf:// protocol to avoid downloading entire datasets) rather than trivia. The grader is mostly robust, accepting multiple valid approaches and providing clear scoring criteria. However, there's minor brittleness in requiring the specific hf:// protocol mention when other SQL-based approaches (like the sql_manager.py tool) would be equally valid. The task is highly realistic - creating domain-specific training subsets is a common ML workflow. The test loses points for slight grader inflexibility around implementation details, but overall it's a strong test of practical skill knowledge."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: duckdb-sql-subset-transform-hf-dataset\nskills:\n- hugging-face-datasets\nprompt: |\n  You need to build a small evaluation dataset on the Hugging Face Hub by subsetting and transforming an existing public dataset using SQL (DuckDB).\n\n  Scenario:\n  - Source dataset: \"cais/mmlu\"\n  - You only want the \"test\" split.\n  - Goal A (exploration): inspect the schema and compute the top 5 `subject` values by row count.\n  - Goal B (transformation): create a QA-style subset containing only rows where `subject` matches the regex pattern `nutrition|anatomy` (case-insensitive is fine).\n    The subset should have exactly these columns:\n      - subject\n      - question\n      - correct_answer (derived from the multiple-choice fields; the dataset has `choices` and an `answer` index)\n  - Goal C (artifact): export 200 random rows from the transformed subset to a local Parquet file.\n  - Goal D (publishing): push the full transformed subset to a NEW Hub dataset repo \"username/mmlu-nutrition-anatomy-qa\" (assume you have write access; it should be private).\n\n  Task:\n  Provide a concrete, command-oriented plan with example DuckDB SQL for each goal. Your answer must:\n  - Show how you would reference the dataset via the `hf://` protocol (including the Parquet-backed revision) OR explain equivalently how your tooling resolves `data` to the underlying `hf://.../*.parquet` path.\n  - Demonstrate correct handling of dataset config/split selection (even if you assume the default config).\n  - Include the SQL that derives `correct_answer` from `choices` and `answer` (be explicit about indexing assumptions).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates practical competence with SQL-based Hugging Face dataset querying/transformation via DuckDB and the hf:// protocol.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Uses DuckDB-style SQL to operate on a Hugging Face dataset via hf:// (either explicitly shows an hf://datasets/... path OR clearly explains that a placeholder like `data` maps to an hf://datasets/...@~parquet/.../*.parquet path).\n    - Correctly incorporates the Parquet-backed revision concept (e.g., @~parquet) or an equivalent explanation of querying auto-converted Parquet files.\n    - Shows schema inspection (e.g., DESCRIBE/PRAGMA table_info/describe command) and a query that computes top subjects by COUNT(*) with GROUP BY, ORDER BY, LIMIT 5 restricted to the test split.\n    - Demonstrates config/split handling (explicitly specifies split=test and acknowledges config/default behavior, or uses an explicit hf:// path containing /default/test/ or equivalent).\n    - Provides a transformation query filtering subjects by a regex pattern (regexp_matches or similar) and produces exactly subject, question, correct_answer.\n    - Correct_answer derivation is technically plausible and explicit about indexing:\n      * Accept `choices[answer]` with a note about indexing assumptions, OR\n      * Accept `list_extract(choices, answer+1)` with a note about DuckDB list indexing, OR another correct equivalent.\n    - Includes both: exporting a random sample of 200 rows to Parquet AND pushing the full transformed subset to a new private Hub dataset repo.\n\n    Score 0.5 if the answer is mostly correct but misses ONE or TWO key elements above (e.g., no @~parquet mention, or no explicit split/config handling, or no export/push step), while still clearly describing an hf:// + DuckDB SQL workflow.\n\n    Score 0.0 if it is generic (no hf:// + DuckDB SQL workflow), omits the transformation logic for correct_answer, or does not provide command-oriented steps that would realistically accomplish the goals.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill by requiring practical knowledge of DuckDB SQL with HF datasets, hf:// protocol, Parquet-backed revisions, split/config handling, array indexing for answer extraction, and both export/push workflows. The grader is comprehensive and accepts multiple valid approaches (e.g., different array indexing methods). However, the test is somewhat contrived in requiring all four goals (A-D) simultaneously, which makes it slightly less realistic than a focused single-purpose task. The non-obviousness is high (tests transferable SQL/dataset transformation patterns), clarity is excellent (robust grader with clear acceptance criteria), and realism is good but not perfect (practitioners would typically do these steps separately)."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "## Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill teaches the abstraction of Hugging Face datasets as queryable SQL tables via DuckDB and the `hf://` protocol. It moves beyond simple \"download and load\" patterns to server-side-like filtering, schema discovery, and structured data transformation. It also covers the lifecycle of creating and configuring specialized training datasets using templates (chat, QA, etc.).\n\n2.  **What workflow decisions does it enable?**\n    It enables practitioners to perform complex data engineering (filtering, joining, and reshaping) directly on the Hub without local storage overhead. It allows for the decision to \"push-to-hub\" directly from a SQL query result, significantly streamlining the creation of fine-tuning subsets from large-scale benchmarks like MMLU or SQuAD.\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    Without this skill, a practitioner would likely download gigabytes of data to local memory/disk, write boilerplate Python code with `pandas` or `datasets.map()`, struggle with indexing into nested choice arrays, and manually handle the authentication and repository initialization for the output. They would likely miss the efficiency of DuckDB's SQL functions (like `choices[answer]` for array indexing) and the templated validation for specific NLP tasks.\n\n---\n\n```yaml\nname: hf-dataset-sql-transformation\nskills:\n- hugging-face-datasets\nprompt: |\n  I need to create a high-quality medical subset from the `cais/mmlu` dataset for a specialized fine-tuning project. \n  \n  Please provide the command (or Python code using the skill's API) to perform the following:\n  1. Filter the `cais/mmlu` dataset for only the following subjects: 'anatomy', 'clinical_knowledge', and 'college_medicine'.\n  2. Further filter the results to only include rows where the `question` length is greater than 150 characters to ensure high-context examples.\n  3. Transform the output so that instead of the integer `answer` column (which is an index), you create a new column called `ground_truth` which contains the actual string text from the `choices` array at that index.\n  4. Push this resulting subset directly to a new private repository on my Hugging Face account named `med-pro/mmlu-medical-long`.\n\n  Assume I am using the `sql_manager.py` tool provided in the skill.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The assistant should provide either a CLI command using `sql_manager.py` or equivalent Python code using the `HFDatasetSQL` class.\n\n    Score 1.0 if the solution correctly addresses all four requirements:\n    1. Filters for the three specific subjects ('anatomy', 'clinical_knowledge', 'college_medicine').\n    2. Filters for question length > 150 (using `LENGTH(question) > 150`).\n    3. Corrects the transformation logic using DuckDB SQL array indexing: `choices[answer]` (or `choices[answer+1]` if they assume 1-based, though the skill doc specifies 0-based `choices[answer]`).\n    4. Includes the `--push-to` argument (or `push_to_hub` method) with the correct repo ID and the `--private` flag.\n\n    Score 0.5 if:\n    - The SQL logic for filtering and the array transformation is correct, but the push/repository logic is missing or incorrect.\n    - The push logic is correct, but the SQL transformation for `choices[answer]` is missing or logically flawed.\n\n    Score 0.0 if:\n    - The assistant fails to use SQL-based querying.\n    - The assistant suggests downloading the whole dataset and using manual Python loops instead of the skill's optimized SQL tools.\n    - The filters for subjects or length are entirely missing.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core value proposition - SQL-based dataset transformation and Hub integration. It tests genuine domain knowledge (DuckDB array indexing, filtering patterns, push workflows) rather than trivia. The grader is mostly robust, accepting both CLI and Python API approaches. However, there's a minor clarity issue: the grader mentions uncertainty about 0-based vs 1-based indexing (`choices[answer]` vs `choices[answer+1]`), when the skill documentation clearly specifies 0-based indexing in the DuckDB SQL Functions section. This ambiguity could lead to inconsistent grading. The task is highly realistic - creating specialized training subsets from large benchmarks is a common ML workflow. Overall, this is a high-quality test with only minor grader precision issues."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Let me analyze this skill document before creating the test case.\n\n## Analysis\n\n**What domain knowledge does this skill teach?**\n\nThis skill teaches two complementary capabilities:\n1. **SQL-based dataset querying** - Using DuckDB to explore, filter, and transform existing Hugging Face datasets\n2. **Dataset creation and management** - Structuring different data types (chat, QA, classification, etc.) for training data\n\nThe core domain concepts are:\n- Understanding different data structure templates for various tasks (conversational, QA, classification, etc.)\n- Data transformation workflows (discover → query → filter → format → push)\n- When to use SQL querying vs creating new datasets\n- Structuring data appropriately for its intended use\n\n**What workflow decisions does it enable?**\n\nPractitioners need to decide:\n- Should I work with existing datasets (SQL querying) or create new ones?\n- What template best fits my data type and use case?\n- How do I filter, aggregate, or join datasets to create subsets?\n- What's the right way to structure my data for my specific domain?\n\n**What would practitioners struggle with without this skill?**\n\n- Not knowing how to explore and filter existing datasets programmatically\n- Not understanding which data structure to use for different domains\n- Not knowing what transformations are possible (filtering, aggregation, joining, reshaping)\n- Confusion about when to query existing data vs creating new datasets\n\n**What to avoid testing:**\n- Exact script syntax or command names\n- Specific dataset IDs \n- Memorized exact JSON schemas\n\n**What to test:**\n- Understanding when to use SQL for data exploration and transformation\n- Knowledge of appropriate data templates and structures\n- Ability to plan a realistic data workflow\n- Understanding possible transformations and their purposes\n\n```yaml\nname: hf-dataset-workflow-design\nskills:\n  - hugging-face-datasets\nprompt: |\n  You are helping a team create a new training dataset for fine-tuning a customer support AI system. You have access to a large existing public Q&A dataset on the Hugging Face Hub, but it contains a mix of technical support, billing questions, and general inquiries. Your team needs training data specifically for handling technical issues.\n  \n  Design a complete workflow to:\n  1. Understand what data is available in the existing dataset (schema, content types)\n  2. Filter and extract only technical support questions\n  3. Reformat the data appropriately for conversation/dialogue training\n  4. Push the resulting curated dataset to a new Hub repository for your team to use\n  \n  Explain your workflow in detail, including:\n  - What type of exploration/analysis you would do first\n  - How you would identify and filter the technical support examples\n  - How you would restructure the data for conversational fine-tuning\n  - Why you chose this approach over alternatives\n  \n  Focus on the domain knowledge and decision-making, not the exact command syntax.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of the Hugging Face Datasets skill workflow.\n    \n    The response should show:\n    1. Understanding of dataset exploration - recognizes the need to first understand schema, examine samples, and analyze data distribution\n    2. Understanding of SQL-based filtering - identifies that querying/filtering capabilities exist to extract relevant subsets based on content criteria\n    3. Understanding of data restructuring - knows that data must be reshaped into appropriate templates/structures for the target task (in this case, conversational)\n    4. Workflow integration - understands the progression from discovery → exploration → filtering → restructuring → publishing\n    5. Appropriate reasoning about data templates - demonstrates knowledge that different domains need different structures\n    \n    The response should NOT focus on:\n    - Exact script names or command syntax (e.g., \"uv run scripts/sql_manager.py\")\n    - Specific dataset identifiers\n    - Exact JSON formatting details\n    - Implementation commands\n    \n    Score 1.0 if: The response demonstrates all five points above. It shows clear understanding of the workflow stages, appropriate use of querying/filtering for data exploration, understanding of template-based restructuring, and logical reasoning about why this approach works.\n    \n    Score 0.5 if: The response shows understanding of most points (3-4) but is missing important elements. For example: understands exploration and filtering but doesn't address restructuring for conversation format, OR explains the workflow but shows unclear understanding of why SQL querying is useful for this task, OR describes steps but without clear reasoning about templates.\n    \n    Score 0.0 if: The response lacks understanding of the core workflow, shows confusion about how to approach the task, doesn't address key stages, or demonstrates no knowledge of the skill's capabilities (e.g., treating it purely as a manual data processing task without leveraging dataset tools).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of workflow-based evaluation and avoids trivia, scoring well on non-obviousness (8/10) and realism (8/10). However, the grader has moderate clarity issues (6/10): it uses subjective criteria like 'clear understanding' and 'logical reasoning' without concrete indicators, making it difficult to consistently distinguish between scores. The five-point checklist helps but lacks specificity about what constitutes 'understanding' versus 'mentioning' each element. A more robust grader would specify concrete evidence (e.g., 'mentions examining schema/samples AND explains why', 'identifies specific filtering criteria like keywords/categories', 'names a conversational template type or describes multi-turn structure')."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain analysis (what the skill teaches and what’s hard without it)\n\n1. **Core domain knowledge**\n   - Hugging Face Hub datasets can be queried *in place* via DuckDB using the `hf://` protocol (often via the `@~parquet` auto-converted revision), enabling SQL exploration/transforms without downloading full datasets.\n   - Practical SQL patterns for HF dataset structures: schema discovery, sampling, filtering by split/config, and transforming nested fields (e.g., arrays like `choices` with an index like `answer`).\n   - Operational outputs: exporting query results to files (Parquet/JSONL) and/or pushing a derived subset back to the Hub (optionally private), requiring HF authentication.\n\n2. **Workflow decisions it enables**\n   - When to choose **SQL-over-Hub** (fast exploration/aggregation/subsetting; minimal local storage) vs. downloading into Python for offline processing.\n   - How to correctly target **config + split** and avoid confusion about dataset layouts.\n   - How to validate a transformation (describe/sample) before pushing a new derived dataset.\n\n3. **What practitioners struggle with without it (test target)**\n   - Knowing that `hf://...@~parquet/.../*.parquet` exists and how to use it for DuckDB SQL.\n   - Writing correct SQL for nested/list fields (extracting the correct choice text from `choices` using `answer`).\n   - Remembering to include schema checks and a reproducible “export/push” flow rather than vague advice.\n\n```yaml\nname: duckdb-hf-subset-transform-and-push\nskills:\n- hugging-face-datasets\nprompt: |\n  You need to create a small *derived* dataset on the Hugging Face Hub from an existing public dataset, but you are storage-constrained and should avoid downloading the full dataset.\n\n  Task:\n  - Source dataset: `cais/mmlu`\n  - Use ONLY the `test` split.\n  - Keep ONLY rows where `subject` is either `nutrition` or `anatomy`.\n  - Transform each row into a QA-like schema with columns:\n      - `question_clean`: the question text with newlines removed\n      - `answer_text`: the *text* of the correct answer option (derived from the `choices` list using the `answer` index)\n      - `subject`\n  - Then (a) export the result to a local Parquet file and (b) push the result to a new private dataset repo on the Hub under your username (you can use a placeholder like `YOURNAME/mmlu-med-qa`).\n\n  Assume you have access to a tool/workflow that can run DuckDB SQL directly against Hugging Face datasets via the `hf://` protocol (including the auto-parquet revision) and can export/push query results.\n\n  Provide:\n  1) The schema/discovery step(s) you would run to confirm column names/types.\n  2) The SQL query to perform the filtering + transformation (be explicit about how `answer_text` is computed).\n  3) Example command(s) or pseudocode showing how you would export to Parquet and push to a new private Hub dataset.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates the key practical workflow for querying/transformation of HF Hub datasets via DuckDB SQL (hf://), including correct handling of splits and list/array fields, and a realistic export+push flow.\n\n    Score 1.0 if the response includes ALL of the following:\n    - Explicit use of DuckDB-over-HF via the hf:// protocol (or an equivalent direct parquet-on-Hub reference), ideally mentioning the @~parquet auto-conversion and/or a path like hf://datasets/{id}@~parquet/{config}/{split}/*.parquet.\n    - A concrete schema discovery step (e.g., DESCRIBE, a \"describe dataset\" command, or a minimal SELECT LIMIT/sample) to confirm columns such as subject/question/choices/answer and split selection.\n    - An SQL query that:\n        * restricts to the test split (either via explicit path containing /test/ or by specifying split=test in the invocation),\n        * filters subject IN ('nutrition','anatomy'),\n        * produces question_clean (using replace/regexp_replace to remove newlines),\n        * derives answer_text from the choices list using the answer index (accept choices[answer], choices[answer+1], or list_extract with an offset IF the response notes indexing and/or validation via sampling),\n        * outputs exactly (or at least) question_clean, answer_text, subject.\n    - A realistic export step to Parquet AND a push-to-Hub step to a new private repo, including mention of authentication/token requirements (HF_TOKEN or equivalent) and naming a target repo (placeholder acceptable).\n\n    Score 0.5 if the response is partially correct, such as:\n    - Provides a correct transformation idea and subject/split filtering but is missing either (a) explicit hf:///@~parquet usage, (b) a clear schema discovery step, or (c) either export or push details; OR\n    - Uses a non-SQL approach (e.g., datasets streaming) but still gives a concrete, correct transformation and push plan.\n\n    Score 0.0 if the response lacks a concrete SQL query and workflow, fails to show how to compute answer_text from choices+answer, or is generic/high-level without actionable steps.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong domain knowledge by testing DuckDB SQL querying over HF Hub datasets, array indexing for nested fields, and the export/push workflow. However, it has moderate clarity issues: the grader accepts 'choices[answer+1]' as valid despite DuckDB using 0-based indexing (making this incorrect), and the complex multi-part grading criteria may be brittle. The realism is good—practitioners do create derived datasets—though the specific transformation requirements are somewhat contrived. The test appropriately focuses on transferable concepts (SQL-over-Hub, handling nested data) rather than trivia."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis\n# 1) Domain knowledge this skill teaches:\n#    - How to work with Hugging Face Hub datasets as queryable data sources via DuckDB SQL,\n#      including the hf:// protocol, Parquet-backed access (@~parquet), and config/split addressing.\n#    - Practical SQL patterns for dataset exploration (schema, sampling, distributions),\n#      filtering/transforming, handling arrays/lists, and exporting results.\n#    - Publishing transformed subsets back to the Hub (new repo, private/public), without\n#      downloading the full source dataset.\n#\n# 2) Workflow decisions it enables:\n#    - When to do remote SQL querying/transforms (DuckDB + hf://) vs. local downloads/processing.\n#    - How to correctly target the intended dataset config/split and avoid mistakes like querying\n#      the wrong split or requiring full materialization.\n#    - How to structure a reproducible “explore → filter/transform → export → push” pipeline.\n#\n# 3) What practitioners struggle with without this skill:\n#    - Discovering the hf:// path format and @~parquet approach to query Hub datasets directly.\n#    - Correctly writing DuckDB SQL over Hub-hosted Parquet (including list/array extraction).\n#    - Producing a hub-published subset reliably (and privately) while keeping the workflow efficient.\n\nname: duckdb-hf-parquet-subset-export-and-push\nskills:\n  - hugging-face-datasets\nprompt: |\n  You need to create a small evaluation subset on the Hugging Face Hub from a much larger source dataset, and you are not allowed to download the full dataset locally.\n\n  Source dataset (on the Hub):\n  - dataset_id: \"acme/exam-bank\"\n  - config: \"default\"\n  - split: \"test\"\n  - columns:\n    - id (string)\n    - question (string)\n    - choices (list<string>)\n    - answer_index (int)   # 0-based index into choices\n    - subject (string)\n    - rationale (string, nullable)\n\n  Target dataset to publish (new repo):\n  - repo_id: \"acme/biology-qa-subset\"\n  - must be PRIVATE\n\n  Requirements:\n  1) Show how you would explore the dataset remotely (schema + a subject distribution).\n  2) Write the DuckDB SQL to produce the subset where:\n     - subject IN ('biology', 'anatomy')\n     - LENGTH(question) > 50\n     - output columns are: id, question, answer, subject\n       where answer is extracted from the choices list using answer_index\n  3) Explain (with concrete example paths/commands/code) how you would query the dataset via the\n     `hf://` protocol including the Parquet conversion revision, and how you would specify config/split.\n  4) Export the query results locally to BOTH Parquet and JSONL.\n  5) Push the transformed subset to the new private Hub dataset repo.\n\n  Provide either:\n  - a CLI-oriented solution (commands + SQL), OR\n  - a Python-oriented solution (code + SQL),\n  but it must be detailed enough that an engineer could implement it directly.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the assistant response for correctness and practicality of a Hugging Face Hub → DuckDB SQL → export → push workflow.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Remote querying via DuckDB using Hugging Face Hub access, explicitly referencing the `hf://` protocol\n      AND the Parquet conversion revision (e.g., `@~parquet`) OR an equivalent clearly-correct mechanism\n      for querying Hub-hosted Parquet without downloading the full dataset.\n    - Correct handling of config/split targeting (default + test), either via the hf:// path structure\n      (`.../{config}/{split}/*.parquet`) or via clearly described parameters/flags.\n    - At least one concrete exploration step:\n      * schema discovery (e.g., DESCRIBE / a \"describe\" action), AND\n      * a subject distribution (e.g., GROUP BY subject count, histogram/unique, etc.).\n    - Provides the actual SQL for the subset that:\n      * filters subject IN ('biology','anatomy')\n      * filters question length > 50 (LENGTH/len equivalent acceptable in DuckDB context)\n      * produces columns id, question, answer, subject\n      * extracts `answer` from `choices` using `answer_index` (accept `choices[answer_index]` OR\n        an offset form like `choices[answer_index+1]` if the assistant explicitly notes indexing\n        conventions and why the offset is needed).\n    - Describes exporting results to BOTH Parquet and JSONL with concrete commands or code.\n    - Describes pushing the resulting subset to a NEW Hub dataset repo and making it PRIVATE\n      (e.g., mentions creating the repo / push_to_hub and `private=True` or equivalent).\n\n    Score 0.5 if the answer is mostly correct but misses 1–2 key elements above, such as:\n    - Mentions SQL and filtering but fails to include hf:// + @~parquet (or otherwise still implies full download),\n    - Omits either exploration (schema/distribution) or one of the export formats,\n    - Does not clearly address config/split,\n    - Push step is vague (e.g., \"upload it\") without indicating Hub dataset repo creation/push and privacy.\n\n    Score 0.0 if:\n    - The answer does not provide an actionable DuckDB+HF approach (no hf:// pathing or equivalent),\n    - Or it primarily relies on fully downloading the dataset without addressing the constraint,\n    - Or it lacks the required SQL transformation/extraction and publishing steps.\n\n    Be flexible about exact command names and tooling (CLI vs Python), but require the core technical\n    substance (hf:// + parquet revision access, correct SQL filtering/transformation, export formats, and private push).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong technical depth by requiring knowledge of DuckDB's hf:// protocol, @~parquet revision, array indexing, and Hub publishing workflows—all transferable concepts. However, it has moderate clarity issues: the grader is overly complex with many conditional branches that could lead to inconsistent scoring, and the test requires knowing specific dataset structure details (exact column names like 'answer_index', 'choices') that make it somewhat brittle. The realism is good as creating filtered subsets and publishing them is a common workflow, though the specific constraint of 'not downloading' feels slightly contrived to force use of the hf:// approach."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "Before constructing the test, here is the required analysis of the skill document:\n\n1. **Domain Knowledge**  \n   - Managing Hugging Face Hub datasets end-to-end: initializing repositories, configuring metadata/system prompts, and maintaining structured data using templates.  \n   - SQL-based querying and transformation of Hub datasets through DuckDB via the `hf://` protocol, including filtering, aggregation, joining, sampling, and exporting/pushing results.  \n   - Handling dataset formats (chat, classification, QA, completion, tabular) and ensuring data validity via templates and JSON validation.\n\n2. **Workflow Decisions Enabled**  \n   - Choosing when to rely on the HF MCP server (discovery/metadata) vs. this skill (creation, editing, SQL operations).  \n   - Selecting between template-driven dataset_manager commands for new data vs. sql_manager for querying/transformation of existing datasets.  \n   - Determining when to push transformed subsets directly to the Hub versus exporting locally for further processing.  \n   - Deciding how to manage configs (system prompts, metadata) after creating or transforming datasets.\n\n3. **Likely Practitioner Struggles Without the Skill**  \n   - Unsure how to efficiently filter or reshape large Hub datasets without downloading everything.  \n   - Confusion about generating private subsets or reformatting data into a specific template (e.g., QA) and pushing them as new datasets.  \n   - Difficulty coordinating dataset configuration steps (system prompts, templates) after transformations.  \n   - Not leveraging SQL operations (describe, histogram, query, push) properly, or mixing responsibilities between HF MCP server and this skill.\n\nWith that understanding, here is the requested test case:\n\n```yaml\nname: plan-private-squad-longform-subset\nskills:\n  - hugging-face-datasets\nprompt: |\n  You need to brief a teammate on how to build and publish a private, long-form QA subset of the public \"squad\" dataset on Hugging Face Hub. The subset should only include rows where the context is over 500 characters and the question is over 30 characters. The teammate also wants to (a) inspect the current context length distribution before filtering, (b) push the filtered result directly to a new private repo called \"your-org/squad-longform\", and (c) ensure the new dataset advertises a QA system prompt aligned with the provided QA template structure.\n\n  Write a step-by-step plan that explains exactly which commands/scripts from the hugging-face-datasets skill they should run, what key arguments or SQL clauses to use, and how to handle the configuration step after pushing. Be explicit about how to accomplish each requirement without downloading the entire dataset locally.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the answer:\n      - Clearly distinguishes that sql_manager.py should be used to explore (e.g., describe/histogram) and filter the squad dataset via SQL using the hf:// data source.\n      - Specifies an appropriate SQL query or equivalent command that filters by context length > 500 and question length > 30, and mentions pushing directly to \"your-org/squad-longform\" as a private repo via the --push-to / --private options.\n      - Mentions configuring the resulting dataset (e.g., with dataset_manager.py config or similar) to set a QA-style system prompt/template after the push.\n\n    Score 0.5 if the answer covers some but not all of the above (e.g., filters correctly but omits either the exploratory step or the configuration step), or if tool usage is vaguely correct but missing critical arguments.\n\n    Score 0.0 if the answer does not reference the proper scripts/workflows from the skill, fails to mention SQL-based filtering/pushing, or is otherwise irrelevant/incorrect.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core capabilities (SQL querying, filtering, pushing to Hub, configuration) and tests genuine workflow knowledge rather than trivia. However, it has moderate clarity issues: the grader accepts vague answers ('or similar', 'or equivalent command') and doesn't clearly specify what constitutes partial credit scenarios. The realism is good - creating filtered subsets is a common task - but the multi-step planning format is somewhat contrived compared to directly asking the agent to execute the task. The non-obviousness is solid as it tests transferable concepts about dataset transformation workflows."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: hf-dataset-sql-transform\nskills:\n  - hugging-face-datasets\nprompt: |\n  I need to create a new dataset repository `my-org/mmlu-math-subset` on the Hugging Face Hub. \n  It should contain data from the existing `cais/mmlu` dataset, but only the rows where the subject is 'elementary_mathematics'.\n  \n  Additionally, I need to transform the data structure during this process:\n  1. Rename the `question` column to `instruction`.\n  2. Create a `solution` column that contains the actual text string of the correct answer. The original dataset provides a `choices` list and an `answer` integer index, so you'll need to extract the correct string from the list.\n  \n  Provide the specific command to run this query and push the result to the new repository directly.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The user wants to filter and transform a Hugging Face dataset using the provided SQL manager tool.\n    \n    Evaluation Criteria:\n    \n    Score 1.0 if:\n    - Uses `scripts/sql_manager.py` with the `query` command.\n    - Defines a SQL query that filters by subject: `WHERE subject = 'elementary_mathematics'` (or similar quoting).\n    - Renames the question column in SQL: `question AS instruction`.\n    - correctly extracts the answer text using the array syntax shown in the docs: `choices[answer] AS solution` (or `choices[answer]`).\n    - Includes the `--push-to \"my-org/mmlu-math-subset\"` argument.\n\n    Score 0.5 if:\n    - Correctly identifies the tool and generally constructs the command.\n    - Misses the specific array indexing syntax (e.g., tries to join or uses Python logic instead of SQL for the answer extraction) OR forgets to rename the column.\n    \n    Score 0.0 if:\n    - Writes a standard Python script using the `datasets` library instead of using the specific `sql_manager` tool provided by the skill.\n    - Fails to provide a command.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case evaluates genuine transferable knowledge about SQL-based dataset transformation (filtering, column renaming, array indexing) rather than memorizing script names. However, it has moderate clarity issues: the grader accepts only the `query` command when `transform` with `--select` would also be valid per the docs, and it doesn't specify whether `--dataset` argument is required. The task is realistic - practitioners commonly need to filter and reshape datasets before pushing subsets to new repositories. The non-obviousness is strong (tests SQL transformation patterns), but the brittleness in accepting only one valid approach reduces the clarity score."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "Looking at this skill document, let me analyze the core knowledge and workflow capabilities it provides:\n\n## Skill Analysis\n\n**Domain Knowledge:**\n- SQL-based querying and transformation of Hugging Face datasets using DuckDB\n- Dataset lifecycle management (init, configure, stream updates)\n- Multi-format dataset structures (chat, classification, QA, completion, tabular)\n- The `hf://` protocol for direct dataset access without downloading\n\n**Workflow Decisions Enabled:**\n- When to query directly with SQL vs download datasets\n- How to efficiently create subsets, filter, and transform large datasets\n- How to convert between data formats (e.g., multiple-choice to QA format)\n- How to push query results back to the Hub as new datasets\n\n**Practitioner Struggles (Test Target):**\nWithout this skill, practitioners wouldn't know:\n- The correct SQL syntax when using DuckDB with HF datasets\n- How to efficiently work with array fields like `choices[answer]`\n- The workflow for exploring, filtering, and repurposing existing datasets\n- How to use the `hf://` protocol correctly for different configs/splits\n\n## Test Case Design\n\nI'll create a test that measures the model's ability to provide a complete, practical workflow for repurposing an existing Hugging Face dataset into a new format and pushing it to the Hub.\n\n```yaml\nname: hf-dataset-query-and-transform\nskills:\n- hugging-face-datasets\nprompt: |\n  You are working on a machine learning project and need to create a custom training dataset.\n\n  The MMLU dataset (cais/mmlu) contains multiple-choice questions across many academic subjects. Your goal is to create a new subset that:\n  1. Contains only questions from the medical-related subjects: \"nutrition\", \"anatomy\", and \"clinical_knowledge\"\n  2. Converts the format from multiple-choice to simple Question-Answer pairs (extracting the correct answer from the choices array using the answer index)\n  3. Includes the subject field for categorization\n  4. Pushes this transformed dataset to a new Hugging Face repository\n\n  Using the hugging-face-datasets skill (which provides sql_manager.py and dataset_manager.py), provide the complete commands and workflow to accomplish this. Include any preliminary exploration commands you'd want to run first.\n\n  Describe your complete solution with the exact commands that would be executed.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the model's response for a complete and correct workflow to transform the MMLU dataset.\n\n    Score 1.0 (Full Credit) if:\n    - The response includes preliminary exploration commands (describe or histogram to understand the dataset structure)\n    - Correctly uses sql_manager.py with appropriate SQL filtering for the three medical subjects\n    - Correctly extracts the correct answer using SQL array syntax like `choices[answer]`\n    - Provides a command to push the results to a new hub repository with `--push-to`\n    - Demonstrates understanding of using `data` as the table name in SQL queries\n    - The commands follow the correct syntax using `uv run scripts/sql_manager.py`\n\n    Score 0.5 (Partial Credit) if:\n    - Provides correct SQL for filtering subjects but misses the answer extraction logic\n    - OR provides correct answer extraction but misses subject filtering\n    - OR includes correct commands but missing exploration/validation steps\n    - OR uses incorrect script syntax/naming but shows understanding of the approach\n\n    Score 0.0 (No Credit) if:\n    - Suggests downloading the entire dataset rather than using SQL queries\n    - Shows no understanding of the hf:// protocol or DuckDB integration\n    - Provides commands that don't match the skill's tools or syntax\n    - The workflow is fundamentally incorrect or would not accomplish the stated goal\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core capabilities and tests genuine workflow knowledge (SQL querying, array indexing, dataset transformation). However, it has moderate clarity issues: the grader accepts solutions that 'show understanding of the approach' even with 'incorrect script syntax/naming,' which could allow fundamentally broken commands to pass. The test is realistic and measures transferable knowledge about dataset transformation workflows rather than trivia, but the grading rubric needs tighter constraints around what constitutes a working solution versus conceptual understanding."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis:**\n- **Domain knowledge taught:** The skill covers how to manage datasets on the Hugging Face Hub, especially creating and configuring repositories with structured templates, validating/streaming data, and using DuckDB SQL via the `hf://` protocol to query, sample, aggregate, transform, and export Hugging Face datasets. It also explains how to set up system prompts and metadata for dataset configurations, and how to push transformed subsets back to the Hub.\n\n- **Workflow decisions enabled:** It clarifies when to rely on the MCP server (discovery, metadata, search) versus when to use the `hugging-face-datasets` capability (creation, editing, SQL-based exploration and transformation). Practitioners need to decide when to use SQL queries (for filtering, aggregating, reshaping) before dataset creation, and how to sequence operations: explore with `sql_manager.py`, transform/subset, export or push, then initialize/configure a new dataset repo with `dataset_manager.py` and chosen templates/system prompts before adding rows.\n\n- **Struggles without the skill:** Without this knowledge, someone would struggle to efficiently derive a focused dataset subset (e.g., filtering by subject/length) using DuckDB SQL and the `hf://` access pattern, or to orchestrate pushing transformed data into a new Hugging Face repo with proper configuration and templated schema. They’d miss how to combine SQL-driven filtering with dataset creation commands and how to enforce QA data structure through templates/system prompts.\n\n```yaml\nname: transform-and-publish-hf-dataset-subset\nskills:\n  - hugging-face-datasets\nprompt: |\n  You need to create a focused QA training dataset from a large public resource on the Hugging Face Hub. Your task is to:\n  1. Explore and describe the schema or column types of the source dataset to understand the fields available for filtering.\n  2. Use the SQL-based tooling to extract only QA examples whose context length exceeds 400 characters and whose answer type is \"explanatory\", while also returning the subject/topic field and the correctly aligned answer (e.g., `choices[answer] AS correct_answer` if applicable).\n  3. Push that filtered subset to a new private Hugging Face dataset repository under your username with a name like `username/long-explanatory-qa`.\n  4. Initialize and configure the new repository to use a QA template with a system prompt that enforces detailed answers, and explain how you would validate the new rows.\n  5. Mention how you would add new QA rows using the templated schema.\n\n  Describe the commands and steps you would take with both `scripts/sql_manager.py` and `scripts/dataset_manager.py` to accomplish this workflow, and note why each step is necessary.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria explaining what a correct answer looks like.\n\n    Score 1.0 if: The response outlines using sql_manager to describe the source schema, then runs an SQL query filtering by context length and answer_type, selecting the correct answer column (e.g., `choices[answer]`). It should mention pushing the resulting subset to a new private repository (`--push-to \"username/long-explanatory-qa\" --private`). The answer then explains dataset_manager commands to init the repo, configure a QA template/system prompt for detailed answers, and add or validate rows with that template. Each step’s purpose in the workflow should be clear.\n\n    Score 0.5 if: The response includes either the SQL filtering and pushing steps or the dataset creation/configuration flow but not both, or if it describes both flows but omits key filters (context length or answer_type) or template/configuration details.\n\n    Score 0.0 if: The response fails to describe SQL filtering/subsetting with the required conditions, or does not explain how to initialize/configure the new repository and add templated QA rows.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of the skill's core workflow (SQL querying → filtering → pushing → dataset initialization → configuration), testing genuine domain knowledge about orchestrating HF dataset transformations. However, it suffers from moderate clarity issues: the grader requires specific implementation details (exact filter conditions, template names, command flags) that could reject valid alternative approaches (e.g., using `transform` instead of `query`, different filter thresholds, or alternative validation methods). The realism is solid as this represents a common data engineering task, but the non-obviousness is somewhat diminished by requiring memorization of specific command patterns rather than testing the underlying conceptual workflow of explore→filter→create→configure."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "```yaml\nname: hf-dataset-sql-querying-transformation\nskills:\n  - hugging-face-datasets\nprompt: |\n  You are working on a machine learning project and need to analyze and prepare a subset of the MMLU (Massive Multitask Language Understanding) dataset for training a specialized model focused on medical and scientific domains.\n\n  Your task:\n  1. First, examine the structure of the \"cais/mmlu\" dataset using SQL to understand its schema and data distribution\n  2. Write a SQL query to extract only the medical and science-related subjects (anatomy, clinical_knowledge, nutrition, biochemistry, etc.)\n  3. Transform the data to extract the correct answers from the choices array and create a clean question-answer format suitable for training\n  4. Filter out any low-quality entries (very short questions or answers)\n  5. Provide the final SQL query that would create this filtered dataset\n\n  Show your SQL queries using the proper `data` table reference and DuckDB syntax for accessing the dataset via the hf:// protocol. Focus on using appropriate string functions, array indexing, and filtering techniques.\n  \n  The goal is to prepare a high-quality subset that could be used to train a specialized medical/scientific QA model.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of SQL-based dataset querying for Hugging Face datasets using DuckDB and the hf:// protocol.\n\n    Score 1.0 if:\n    - Uses correct `data` table reference in SQL queries\n    - Demonstrates understanding of hf:// protocol access pattern\n    - Shows knowledge of DuckDB string functions (LENGTH, regexp_matches, regexp_replace)\n    - Uses proper array indexing syntax (choices[answer]) \n    - Includes appropriate filtering and aggregation queries\n    - Understands how to extract and transform data structure\n    - Shows awareness of data quality considerations (length filtering)\n    - Demonstrates progression from exploration to final transformation\n\n    Score 0.5 if:\n    - Shows partial understanding of SQL querying but has minor syntax errors\n    - Understands basic filtering but misses some advanced transformations\n    - Attempts proper structure but with some gaps in DuckDB-specific syntax\n\n    Score 0.0 if:\n    - Does not use SQL querying approach\n    - Major misunderstanding of dataset access or table structure\n    - Provides no actual SQL queries or completely incorrect syntax\n    - Does not demonstrate understanding of the hf:// protocol or DuckDB functions\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test demonstrates good understanding of SQL querying concepts (array indexing, filtering, transformations) which are transferable skills, earning moderate non-obviousness (6/10). However, it has clarity issues (5/10) as the grader is overly prescriptive about specific DuckDB syntax details (e.g., 'choices[answer]', 'regexp_matches') that may reject valid alternative approaches or SQL dialects that accomplish the same goal. The task is realistic (7/10) as creating domain-specific dataset subsets is a common ML workflow, though the specific MMLU dataset reference and medical domain focus make it somewhat contrived. The grader should focus more on whether the approach correctly filters and transforms data rather than exact syntax matching."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-subset-from-dataset\nskills:\n  - hugging-face-datasets\nprompt: |\n  You have been given a large dataset from the Hugging Face Hub named \"cais/mmlu\" which contains a variety of questions and answers on different subjects. Your task is to create a subset of this dataset that includes only questions related to the subjects \"nutrition\" and \"anatomy\". Additionally, you need to push this subset to a new private dataset on the Hugging Face Hub with the name \"your-username/mmlu-medical-subset\".\n\n  Provide the SQL query you would use to create this subset and the command to push the subset to the new dataset.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    Score 1.0 if:\n    - The SQL query correctly filters the dataset to include only questions related to \"nutrition\" and \"anatomy\".\n    - The command to push the subset to the new private dataset is correct and includes the necessary parameters.\n\n    Score 0.5 if:\n    - The SQL query is partially correct but may have minor issues or is missing a subject filter.\n    - The command to push the subset is partially correct but may have minor issues.\n\n    Score 0.0 if:\n    - The SQL query is incorrect or does not filter the dataset correctly.\n    - The command to push the subset is incorrect or missing.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues across all dimensions. Non-obviousness (3/10): While it tests SQL filtering concepts, it requires knowing the specific dataset name 'cais/mmlu' and exact subject names from the skill examples, making it more trivia-focused than transferable. Clarity (4/10): The grader is brittle - it only accepts the specific subjects 'nutrition' and 'anatomy' and the exact dataset name from the skill doc, rejecting valid alternatives like filtering other subjects or using different datasets. The grader also doesn't specify what constitutes a 'correct' SQL query structure. Realism (5/10): Creating dataset subsets is realistic, but the task is overly prescriptive about which specific dataset and subjects to use rather than testing the general capability of filtering and pushing datasets."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: create-and-query-dataset\nskills:\n- hugging-face-datasets\nprompt: |\n  You are tasked with creating a new dataset on the Hugging Face Hub for storing information about books, including their titles, authors, and publication years. After creating the dataset, you need to query it to find all books published after 2010. Please provide the steps and commands to achieve this using the hugging-face-datasets skill.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes the correct commands for creating a dataset and querying it to filter books published after 2010.\n    Score 0.5 if the response includes the correct command for creating a dataset but fails to provide the correct query command.\n    Score 0.0 if the response does not include the correct commands for either creating the dataset or querying it.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. For non-obviousness (3/10): While it tests dataset creation and querying concepts, the task is overly simplistic and doesn't test genuine gotchas or transferable patterns - it's essentially asking to demonstrate basic commands from the documentation. For clarity (2/10): The grader is fundamentally broken - it asks for 'commands' but doesn't specify what constitutes correct commands, doesn't account for the fact that the model would need to use `uv run scripts/dataset_manager.py` and `uv run scripts/sql_manager.py` with specific arguments, and doesn't validate that the SQL query is actually correct (e.g., `SELECT * FROM data WHERE publication_year > 2010`). The grader would likely accept vague descriptions rather than executable commands. For realism (4/10): The scenario is somewhat plausible but contrived - creating a books dataset just to immediately query it is artificial; real practitioners would either work with existing datasets or have more complex workflows. The test doesn't reflect realistic dataset management challenges like schema design, data validation, or handling edge cases."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Hook Development",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 8,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Hook Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Hook Development\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse/PostToolUse/Stop hook\", \"validate tool use\", \"implement prompt-based hooks\", \"use ${CLAUDE_PLUGIN_ROOT}\", \"set up event-driven automation\", \"block dangerous commands\", or mentions hook events (PreToolUse, PostToolUse, Stop, SubagentStop, SessionStart, SessionEnd, UserPromptSubmit, PreCompact, Notification). Provides comprehensive guidance for creating and implementing Claude Code plugin hooks with focus on advanced prompt-based hooks API.\nversion: 0.1.0\n---\n\n# Hook Development for Claude Code Plugins\n\n## Overview\n\nHooks are event-driven automation scripts that execute in response to Claude Code events. Use hooks to validate operations, enforce policies, add context, and integrate external tools into workflows.\n\n**Key capabilities:**\n- Validate tool calls before execution (PreToolUse)\n- React to tool results (PostToolUse)\n- Enforce completion standards (Stop, SubagentStop)\n- Load project context (SessionStart)\n- Automate workflows across the development lifecycle\n\n## Hook Types\n\n### Prompt-Based Hooks (Recommended)\n\nUse LLM-driven decision making for context-aware validation:\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if this tool use is appropriate: $TOOL_INPUT\",\n  \"timeout\": 30\n}\n```\n\n**Supported events:** Stop, SubagentStop, UserPromptSubmit, PreToolUse\n\n**Benefits:**\n- Context-aware decisions based on natural language reasoning\n- Flexible evaluation logic without bash scripting\n- Better edge case handling\n- Easier to maintain and extend\n\n### Command Hooks\n\nExecute bash commands for deterministic checks:\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\",\n  \"timeout\": 60\n}\n```\n\n**Use for:**\n- Fast deterministic validations\n- File system operations\n- External tool integrations\n- Performance-critical checks\n\n## Hook Configuration Formats\n\n### Plugin hooks.json Format\n\n**For plugin hooks** in `hooks/hooks.json`, use wrapper format:\n\n```json\n{\n  \"description\": \"Brief explanation of hooks (optional)\",\n  \"hooks\": {\n    \"PreToolUse\": [...],\n    \"Stop\": [...],\n    \"SessionStart\": [...]\n  }\n}\n```\n\n**Key points:**\n- `description` field is optional\n- `hooks` field is required wrapper containing actual hook events\n- This is the **plugin-specific format**\n\n**Example:**\n```json\n{\n  \"description\": \"Validation hooks for code quality\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/validate.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Settings Format (Direct)\n\n**For user settings** in `.claude/settings.json`, use direct format:\n\n```json\n{\n  \"PreToolUse\": [...],\n  \"Stop\": [...],\n  \"SessionStart\": [...]\n}\n```\n\n**Key points:**\n- No wrapper - events directly at top level\n- No description field\n- This is the **settings format**\n\n**Important:** The examples below show the hook event structure that goes inside either format. For plugin hooks.json, wrap these in `{\"hooks\": {...}}`.\n\n## Hook Events\n\n### PreToolUse\n\nExecute before any tool runs. Use to approve, deny, or modify tool calls.\n\n**Example (prompt-based):**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety. Check: system paths, credentials, path traversal, sensitive content. Return 'approve' or 'deny'.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output for PreToolUse:**\n```json\n{\n  \"hookSpecificOutput\": {\n    \"permissionDecision\": \"allow|deny|ask\",\n    \"updatedInput\": {\"field\": \"modified_value\"}\n  },\n  \"systemMessage\": \"Explanation for Claude\"\n}\n```\n\n### PostToolUse\n\nExecute after tool completes. Use to react to results, provide feedback, or log.\n\n**Example:**\n```json\n{\n  \"PostToolUse\": [\n    {\n      \"matcher\": \"Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze edit result for potential issues: syntax errors, security vulnerabilities, breaking changes. Provide feedback.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output behavior:**\n- Exit 0: stdout shown in transcript\n- Exit 2: stderr fed back to Claude\n- systemMessage included in context\n\n### Stop\n\nExecute when main agent considers stopping. Use to validate completeness.\n\n**Example:**\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion: tests run, build succeeded, questions answered. Return 'approve' to stop or 'block' with reason to continue.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Decision output:**\n```json\n{\n  \"decision\": \"approve|block\",\n  \"reason\": \"Explanation\",\n  \"systemMessage\": \"Additional context\"\n}\n```\n\n### SubagentStop\n\nExecute when subagent considers stopping. Use to ensure subagent completed its task.\n\nSimilar to Stop hook, but for subagents.\n\n### UserPromptSubmit\n\nExecute when user submits a prompt. Use to add context, validate, or block prompts.\n\n**Example:**\n```json\n{\n  \"UserPromptSubmit\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Check if prompt requires security guidance. If discussing auth, permissions, or API security, return relevant warnings.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### SessionStart\n\nExecute when Claude Code session begins. Use to load context and set environment.\n\n**Example:**\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Special capability:** Persist environment variables using `$CLAUDE_ENV_FILE`:\n```bash\necho \"export PROJECT_TYPE=nodejs\" >> \"$CLAUDE_ENV_FILE\"\n```\n\nSee `examples/load-context.sh` for complete example.\n\n### SessionEnd\n\nExecute when session ends. Use for cleanup, logging, and state preservation.\n\n### PreCompact\n\nExecute before context compaction. Use to add critical information to preserve.\n\n### Notification\n\nExecute when Claude sends notifications. Use to react to user notifications.\n\n## Hook Output Format\n\n### Standard Output (All Hooks)\n\n```json\n{\n  \"continue\": true,\n  \"suppressOutput\": false,\n  \"systemMessage\": \"Message for Claude\"\n}\n```\n\n- `continue`: If false, halt processing (default true)\n- `suppressOutput`: Hide output from transcript (default false)\n- `systemMessage`: Message shown to Claude\n\n### Exit Codes\n\n- `0` - Success (stdout shown in transcript)\n- `2` - Blocking error (stderr fed back to Claude)\n- Other - Non-blocking error\n\n## Hook Input Format\n\nAll hooks receive JSON via stdin with common fields:\n\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.txt\",\n  \"cwd\": \"/current/working/dir\",\n  \"permission_mode\": \"ask|allow\",\n  \"hook_event_name\": \"PreToolUse\"\n}\n```\n\n**Event-specific fields:**\n\n- **PreToolUse/PostToolUse:** `tool_name`, `tool_input`, `tool_result`\n- **UserPromptSubmit:** `user_prompt`\n- **Stop/SubagentStop:** `reason`\n\nAccess fields in prompts using `$TOOL_INPUT`, `$TOOL_RESULT`, `$USER_PROMPT`, etc.\n\n## Environment Variables\n\nAvailable in all command hooks:\n\n- `$CLAUDE_PROJECT_DIR` - Project root path\n- `$CLAUDE_PLUGIN_ROOT` - Plugin directory (use for portable paths)\n- `$CLAUDE_ENV_FILE` - SessionStart only: persist env vars here\n- `$CLAUDE_CODE_REMOTE` - Set if running in remote context\n\n**Always use ${CLAUDE_PLUGIN_ROOT} in hook commands for portability:**\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\"\n}\n```\n\n## Plugin Hook Configuration\n\nIn plugins, define hooks in `hooks/hooks.json`:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety\"\n        }\n      ]\n    }\n  ],\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion\"\n        }\n      ]\n    }\n  ],\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\",\n          \"timeout\": 10\n        }\n      ]\n    }\n  ]\n}\n```\n\nPlugin hooks merge with user's hooks and run in parallel.\n\n## Matchers\n\n### Tool Name Matching\n\n**Exact match:**\n```json\n\"matcher\": \"Write\"\n```\n\n**Multiple tools:**\n```json\n\"matcher\": \"Read|Write|Edit\"\n```\n\n**Wildcard (all tools):**\n```json\n\"matcher\": \"*\"\n```\n\n**Regex patterns:**\n```json\n\"matcher\": \"mcp__.*__delete.*\"  // All MCP delete tools\n```\n\n**Note:** Matchers are case-sensitive.\n\n### Common Patterns\n\n```json\n// All MCP tools\n\"matcher\": \"mcp__.*\"\n\n// Specific plugin's MCP tools\n\"matcher\": \"mcp__plugin_asana_.*\"\n\n// All file operations\n\"matcher\": \"Read|Write|Edit\"\n\n// Bash commands only\n\"matcher\": \"Bash\"\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate inputs in command hooks:\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\n\n# Validate tool name format\nif [[ ! \"$tool_name\" =~ ^[a-zA-Z0-9_]+$ ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Invalid tool name\"}' >&2\n  exit 2\nfi\n```\n\n### Path Safety\n\nCheck for path traversal and sensitive files:\n\n```bash\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Deny path traversal\nif [[ \"$file_path\" == *\"..\"* ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Path traversal detected\"}' >&2\n  exit 2\nfi\n\n# Deny sensitive files\nif [[ \"$file_path\" == *\".env\"* ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Sensitive file\"}' >&2\n  exit 2\nfi\n```\n\nSee `examples/validate-write.sh` and `examples/validate-bash.sh` for complete examples.\n\n### Quote All Variables\n\n```bash\n# GOOD: Quoted\necho \"$file_path\"\ncd \"$CLAUDE_PROJECT_DIR\"\n\n# BAD: Unquoted (injection risk)\necho $file_path\ncd $CLAUDE_PROJECT_DIR\n```\n\n### Set Appropriate Timeouts\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash script.sh\",\n  \"timeout\": 10\n}\n```\n\n**Defaults:** Command hooks (60s), Prompt hooks (30s)\n\n## Performance Considerations\n\n### Parallel Execution\n\nAll matching hooks run **in parallel**:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write\",\n      \"hooks\": [\n        {\"type\": \"command\", \"command\": \"check1.sh\"},  // Parallel\n        {\"type\": \"command\", \"command\": \"check2.sh\"},  // Parallel\n        {\"type\": \"prompt\", \"prompt\": \"Validate...\"}   // Parallel\n      ]\n    }\n  ]\n}\n```\n\n**Design implications:**\n- Hooks don't see each other's output\n- Non-deterministic ordering\n- Design for independence\n\n### Optimization\n\n1. Use command hooks for quick deterministic checks\n2. Use prompt hooks for complex reasoning\n3. Cache validation results in temp files\n4. Minimize I/O in hot paths\n\n## Temporarily Active Hooks\n\nCreate hooks that activate conditionally by checking for a flag file or configuration:\n\n**Pattern: Flag file activation**\n```bash\n#!/bin/bash\n# Only active when flag file exists\nFLAG_FILE=\"$CLAUDE_PROJECT_DIR/.enable-strict-validation\"\n\nif [ ! -f \"$FLAG_FILE\" ]; then\n  # Flag not present, skip validation\n  exit 0\nfi\n\n# Flag present, run validation\ninput=$(cat)\n# ... validation logic ...\n```\n\n**Pattern: Configuration-based activation**\n```bash\n#!/bin/bash\n# Check configuration for activation\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/plugin-config.json\"\n\nif [ -f \"$CONFIG_FILE\" ]; then\n  enabled=$(jq -r '.strictMode // false' \"$CONFIG_FILE\")\n  if [ \"$enabled\" != \"true\" ]; then\n    exit 0  # Not enabled, skip\n  fi\nfi\n\n# Enabled, run hook logic\ninput=$(cat)\n# ... hook logic ...\n```\n\n**Use cases:**\n- Enable strict validation only when needed\n- Temporary debugging hooks\n- Project-specific hook behavior\n- Feature flags for hooks\n\n**Best practice:** Document activation mechanism in plugin README so users know how to enable/disable temporary hooks.\n\n## Hook Lifecycle and Limitations\n\n### Hooks Load at Session Start\n\n**Important:** Hooks are loaded when Claude Code session starts. Changes to hook configuration require restarting Claude Code.\n\n**Cannot hot-swap hooks:**\n- Editing `hooks/hooks.json` won't affect current session\n- Adding new hook scripts won't be recognized\n- Changing hook commands/prompts won't update\n- Must restart Claude Code: exit and run `claude` again\n\n**To test hook changes:**\n1. Edit hook configuration or scripts\n2. Exit Claude Code session\n3. Restart: `claude` or `cc`\n4. New hook configuration loads\n5. Test hooks with `claude --debug`\n\n### Hook Validation at Startup\n\nHooks are validated when Claude Code starts:\n- Invalid JSON in hooks.json causes loading failure\n- Missing scripts cause warnings\n- Syntax errors reported in debug mode\n\nUse `/hooks` command to review loaded hooks in current session.\n\n## Debugging Hooks\n\n### Enable Debug Mode\n\n```bash\nclaude --debug\n```\n\nLook for hook registration, execution logs, input/output JSON, and timing information.\n\n### Test Hook Scripts\n\nTest command hooks directly:\n\n```bash\necho '{\"tool_name\": \"Write\", \"tool_input\": {\"file_path\": \"/test\"}}' | \\\n  bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\n\necho \"Exit code: $?\"\n```\n\n### Validate JSON Output\n\nEnsure hooks output valid JSON:\n\n```bash\noutput=$(./your-hook.sh < test-input.json)\necho \"$output\" | jq .\n```\n\n## Quick Reference\n\n### Hook Events Summary\n\n| Event | When | Use For |\n|-------|------|---------|\n| PreToolUse | Before tool | Validation, modification |\n| PostToolUse | After tool | Feedback, logging |\n| UserPromptSubmit | User input | Context, validation |\n| Stop | Agent stopping | Completeness check |\n| SubagentStop | Subagent done | Task validation |\n| SessionStart | Session begins | Context loading |\n| SessionEnd | Session ends | Cleanup, logging |\n| PreCompact | Before compact | Preserve context |\n| Notification | User notified | Logging, reactions |\n\n### Best Practices\n\n**DO:**\n- ✅ Use prompt-based hooks for complex logic\n- ✅ Use ${CLAUDE_PLUGIN_ROOT} for portability\n- ✅ Validate all inputs in command hooks\n- ✅ Quote all bash variables\n- ✅ Set appropriate timeouts\n- ✅ Return structured JSON output\n- ✅ Test hooks thoroughly\n\n**DON'T:**\n- ❌ Use hardcoded paths\n- ❌ Trust user input without validation\n- ❌ Create long-running hooks\n- ❌ Rely on hook execution order\n- ❌ Modify global state unpredictably\n- ❌ Log sensitive information\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and advanced techniques, consult:\n\n- **`references/patterns.md`** - Common hook patterns (8+ proven patterns)\n- **`references/migration.md`** - Migrating from basic to advanced hooks\n- **`references/advanced.md`** - Advanced use cases and techniques\n\n### Example Hook Scripts\n\nWorking examples in `examples/`:\n\n- **`validate-write.sh`** - File write validation example\n- **`validate-bash.sh`** - Bash command validation example\n- **`load-context.sh`** - SessionStart context loading example\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-hook-schema.sh`** - Validate hooks.json structure and syntax\n- **`test-hook.sh`** - Test hooks with sample input before deployment\n- **`hook-linter.sh`** - Check hook scripts for common issues and best practices\n\n### External Resources\n\n- **Official Docs**: https://docs.claude.com/en/docs/claude-code/hooks\n- **Examples**: See security-guidance plugin in marketplace\n- **Testing**: Use `claude --debug` for detailed logs\n- **Validation**: Use `jq` to validate hook JSON output\n\n## Implementation Workflow\n\nTo implement hooks in a plugin:\n\n1. Identify events to hook into (PreToolUse, Stop, SessionStart, etc.)\n2. Decide between prompt-based (flexible) or command (deterministic) hooks\n3. Write hook configuration in `hooks/hooks.json`\n4. For command hooks, create hook scripts\n5. Use ${CLAUDE_PLUGIN_ROOT} for all file references\n6. Validate configuration with `scripts/validate-hook-schema.sh hooks/hooks.json`\n7. Test hooks with `scripts/test-hook.sh` before deployment\n8. Test in Claude Code with `claude --debug`\n9. Document hooks in plugin README\n\nFocus on prompt-based hooks for most use cases. Reserve command hooks for performance-critical or deterministic checks.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Hook Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Hook Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Hook Development\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse/PostToolUse/Stop hook\", \"validate tool use\", \"implement prompt-based hooks\", \"use ${CLAUDE_PLUGIN_ROOT}\", \"set up event-driven automation\", \"block dangerous commands\", or mentions hook events (PreToolUse, PostToolUse, Stop, SubagentStop, SessionStart, SessionEnd, UserPromptSubmit, PreCompact, Notification). Provides comprehensive guidance for creating and implementing Claude Code plugin hooks with focus on advanced prompt-based hooks API.\nversion: 0.1.0\n---\n\n# Hook Development for Claude Code Plugins\n\n## Overview\n\nHooks are event-driven automation scripts that execute in response to Claude Code events. Use hooks to validate operations, enforce policies, add context, and integrate external tools into workflows.\n\n**Key capabilities:**\n- Validate tool calls before execution (PreToolUse)\n- React to tool results (PostToolUse)\n- Enforce completion standards (Stop, SubagentStop)\n- Load project context (SessionStart)\n- Automate workflows across the development lifecycle\n\n## Hook Types\n\n### Prompt-Based Hooks (Recommended)\n\nUse LLM-driven decision making for context-aware validation:\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if this tool use is appropriate: $TOOL_INPUT\",\n  \"timeout\": 30\n}\n```\n\n**Supported events:** Stop, SubagentStop, UserPromptSubmit, PreToolUse\n\n**Benefits:**\n- Context-aware decisions based on natural language reasoning\n- Flexible evaluation logic without bash scripting\n- Better edge case handling\n- Easier to maintain and extend\n\n### Command Hooks\n\nExecute bash commands for deterministic checks:\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\",\n  \"timeout\": 60\n}\n```\n\n**Use for:**\n- Fast deterministic validations\n- File system operations\n- External tool integrations\n- Performance-critical checks\n\n## Hook Configuration Formats\n\n### Plugin hooks.json Format\n\n**For plugin hooks** in `hooks/hooks.json`, use wrapper format:\n\n```json\n{\n  \"description\": \"Brief explanation of hooks (optional)\",\n  \"hooks\": {\n    \"PreToolUse\": [...],\n    \"Stop\": [...],\n    \"SessionStart\": [...]\n  }\n}\n```\n\n**Key points:**\n- `description` field is optional\n- `hooks` field is required wrapper containing actual hook events\n- This is the **plugin-specific format**\n\n**Example:**\n```json\n{\n  \"description\": \"Validation hooks for code quality\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/validate.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Settings Format (Direct)\n\n**For user settings** in `.claude/settings.json`, use direct format:\n\n```json\n{\n  \"PreToolUse\": [...],\n  \"Stop\": [...],\n  \"SessionStart\": [...]\n}\n```\n\n**Key points:**\n- No wrapper - events directly at top level\n- No description field\n- This is the **settings format**\n\n**Important:** The examples below show the hook event structure that goes inside either format. For plugin hooks.json, wrap these in `{\"hooks\": {...}}`.\n\n## Hook Events\n\n### PreToolUse\n\nExecute before any tool runs. Use to approve, deny, or modify tool calls.\n\n**Example (prompt-based):**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety. Check: system paths, credentials, path traversal, sensitive content. Return 'approve' or 'deny'.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output for PreToolUse:**\n```json\n{\n  \"hookSpecificOutput\": {\n    \"permissionDecision\": \"allow|deny|ask\",\n    \"updatedInput\": {\"field\": \"modified_value\"}\n  },\n  \"systemMessage\": \"Explanation for Claude\"\n}\n```\n\n### PostToolUse\n\nExecute after tool completes. Use to react to results, provide feedback, or log.\n\n**Example:**\n```json\n{\n  \"PostToolUse\": [\n    {\n      \"matcher\": \"Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze edit result for potential issues: syntax errors, security vulnerabilities, breaking changes. Provide feedback.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output behavior:**\n- Exit 0: stdout shown in transcript\n- Exit 2: stderr fed back to Claude\n- systemMessage included in context\n\n### Stop\n\nExecute when main agent considers stopping. Use to validate completeness.\n\n**Example:**\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion: tests run, build succeeded, questions answered. Return 'approve' to stop or 'block' with reason to continue.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Decision output:**\n```json\n{\n  \"decision\": \"approve|block\",\n  \"reason\": \"Explanation\",\n  \"systemMessage\": \"Additional context\"\n}\n```\n\n### SubagentStop\n\nExecute when subagent considers stopping. Use to ensure subagent completed its task.\n\nSimilar to Stop hook, but for subagents.\n\n### UserPromptSubmit\n\nExecute when user submits a prompt. Use to add context, validate, or block prompts.\n\n**Example:**\n```json\n{\n  \"UserPromptSubmit\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Check if prompt requires security guidance. If discussing auth, permissions, or API security, return relevant warnings.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### SessionStart\n\nExecute when Claude Code session begins. Use to load context and set environment.\n\n**Example:**\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Special capability:** Persist environment variables using `$CLAUDE_ENV_FILE`:\n```bash\necho \"export PROJECT_TYPE=nodejs\" >> \"$CLAUDE_ENV_FILE\"\n```\n\nSee `examples/load-context.sh` for complete example.\n\n### SessionEnd\n\nExecute when session ends. Use for cleanup, logging, and state preservation.\n\n### PreCompact\n\nExecute before context compaction. Use to add critical information to preserve.\n\n### Notification\n\nExecute when Claude sends notifications. Use to react to user notifications.\n\n## Hook Output Format\n\n### Standard Output (All Hooks)\n\n```json\n{\n  \"continue\": true,\n  \"suppressOutput\": false,\n  \"systemMessage\": \"Message for Claude\"\n}\n```\n\n- `continue`: If false, halt processing (default true)\n- `suppressOutput`: Hide output from transcript (default false)\n- `systemMessage`: Message shown to Claude\n\n### Exit Codes\n\n- `0` - Success (stdout shown in transcript)\n- `2` - Blocking error (stderr fed back to Claude)\n- Other - Non-blocking error\n\n## Hook Input Format\n\nAll hooks receive JSON via stdin with common fields:\n\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.txt\",\n  \"cwd\": \"/current/working/dir\",\n  \"permission_mode\": \"ask|allow\",\n  \"hook_event_name\": \"PreToolUse\"\n}\n```\n\n**Event-specific fields:**\n\n- **PreToolUse/PostToolUse:** `tool_name`, `tool_input`, `tool_result`\n- **UserPromptSubmit:** `user_prompt`\n- **Stop/SubagentStop:** `reason`\n\nAccess fields in prompts using `$TOOL_INPUT`, `$TOOL_RESULT`, `$USER_PROMPT`, etc.\n\n## Environment Variables\n\nAvailable in all command hooks:\n\n- `$CLAUDE_PROJECT_DIR` - Project root path\n- `$CLAUDE_PLUGIN_ROOT` - Plugin directory (use for portable paths)\n- `$CLAUDE_ENV_FILE` - SessionStart only: persist env vars here\n- `$CLAUDE_CODE_REMOTE` - Set if running in remote context\n\n**Always use ${CLAUDE_PLUGIN_ROOT} in hook commands for portability:**\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\"\n}\n```\n\n## Plugin Hook Configuration\n\nIn plugins, define hooks in `hooks/hooks.json`:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety\"\n        }\n      ]\n    }\n  ],\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion\"\n        }\n      ]\n    }\n  ],\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\",\n          \"timeout\": 10\n        }\n      ]\n    }\n  ]\n}\n```\n\nPlugin hooks merge with user's hooks and run in parallel.\n\n## Matchers\n\n### Tool Name Matching\n\n**Exact match:**\n```json\n\"matcher\": \"Write\"\n```\n\n**Multiple tools:**\n```json\n\"matcher\": \"Read|Write|Edit\"\n```\n\n**Wildcard (all tools):**\n```json\n\"matcher\": \"*\"\n```\n\n**Regex patterns:**\n```json\n\"matcher\": \"mcp__.*__delete.*\"  // All MCP delete tools\n```\n\n**Note:** Matchers are case-sensitive.\n\n### Common Patterns\n\n```json\n// All MCP tools\n\"matcher\": \"mcp__.*\"\n\n// Specific plugin's MCP tools\n\"matcher\": \"mcp__plugin_asana_.*\"\n\n// All file operations\n\"matcher\": \"Read|Write|Edit\"\n\n// Bash commands only\n\"matcher\": \"Bash\"\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate inputs in command hooks:\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\n\n# Validate tool name format\nif [[ ! \"$tool_name\" =~ ^[a-zA-Z0-9_]+$ ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Invalid tool name\"}' >&2\n  exit 2\nfi\n```\n\n### Path Safety\n\nCheck for path traversal and sensitive files:\n\n```bash\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Deny path traversal\nif [[ \"$file_path\" == *\"..\"* ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Path traversal detected\"}' >&2\n  exit 2\nfi\n\n# Deny sensitive files\nif [[ \"$file_path\" == *\".env\"* ]]; then\n  echo '{\"decision\": \"deny\", \"reason\": \"Sensitive file\"}' >&2\n  exit 2\nfi\n```\n\nSee `examples/validate-write.sh` and `examples/validate-bash.sh` for complete examples.\n\n### Quote All Variables\n\n```bash\n# GOOD: Quoted\necho \"$file_path\"\ncd \"$CLAUDE_PROJECT_DIR\"\n\n# BAD: Unquoted (injection risk)\necho $file_path\ncd $CLAUDE_PROJECT_DIR\n```\n\n### Set Appropriate Timeouts\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash script.sh\",\n  \"timeout\": 10\n}\n```\n\n**Defaults:** Command hooks (60s), Prompt hooks (30s)\n\n## Performance Considerations\n\n### Parallel Execution\n\nAll matching hooks run **in parallel**:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write\",\n      \"hooks\": [\n        {\"type\": \"command\", \"command\": \"check1.sh\"},  // Parallel\n        {\"type\": \"command\", \"command\": \"check2.sh\"},  // Parallel\n        {\"type\": \"prompt\", \"prompt\": \"Validate...\"}   // Parallel\n      ]\n    }\n  ]\n}\n```\n\n**Design implications:**\n- Hooks don't see each other's output\n- Non-deterministic ordering\n- Design for independence\n\n### Optimization\n\n1. Use command hooks for quick deterministic checks\n2. Use prompt hooks for complex reasoning\n3. Cache validation results in temp files\n4. Minimize I/O in hot paths\n\n## Temporarily Active Hooks\n\nCreate hooks that activate conditionally by checking for a flag file or configuration:\n\n**Pattern: Flag file activation**\n```bash\n#!/bin/bash\n# Only active when flag file exists\nFLAG_FILE=\"$CLAUDE_PROJECT_DIR/.enable-strict-validation\"\n\nif [ ! -f \"$FLAG_FILE\" ]; then\n  # Flag not present, skip validation\n  exit 0\nfi\n\n# Flag present, run validation\ninput=$(cat)\n# ... validation logic ...\n```\n\n**Pattern: Configuration-based activation**\n```bash\n#!/bin/bash\n# Check configuration for activation\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/plugin-config.json\"\n\nif [ -f \"$CONFIG_FILE\" ]; then\n  enabled=$(jq -r '.strictMode // false' \"$CONFIG_FILE\")\n  if [ \"$enabled\" != \"true\" ]; then\n    exit 0  # Not enabled, skip\n  fi\nfi\n\n# Enabled, run hook logic\ninput=$(cat)\n# ... hook logic ...\n```\n\n**Use cases:**\n- Enable strict validation only when needed\n- Temporary debugging hooks\n- Project-specific hook behavior\n- Feature flags for hooks\n\n**Best practice:** Document activation mechanism in plugin README so users know how to enable/disable temporary hooks.\n\n## Hook Lifecycle and Limitations\n\n### Hooks Load at Session Start\n\n**Important:** Hooks are loaded when Claude Code session starts. Changes to hook configuration require restarting Claude Code.\n\n**Cannot hot-swap hooks:**\n- Editing `hooks/hooks.json` won't affect current session\n- Adding new hook scripts won't be recognized\n- Changing hook commands/prompts won't update\n- Must restart Claude Code: exit and run `claude` again\n\n**To test hook changes:**\n1. Edit hook configuration or scripts\n2. Exit Claude Code session\n3. Restart: `claude` or `cc`\n4. New hook configuration loads\n5. Test hooks with `claude --debug`\n\n### Hook Validation at Startup\n\nHooks are validated when Claude Code starts:\n- Invalid JSON in hooks.json causes loading failure\n- Missing scripts cause warnings\n- Syntax errors reported in debug mode\n\nUse `/hooks` command to review loaded hooks in current session.\n\n## Debugging Hooks\n\n### Enable Debug Mode\n\n```bash\nclaude --debug\n```\n\nLook for hook registration, execution logs, input/output JSON, and timing information.\n\n### Test Hook Scripts\n\nTest command hooks directly:\n\n```bash\necho '{\"tool_name\": \"Write\", \"tool_input\": {\"file_path\": \"/test\"}}' | \\\n  bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\n\necho \"Exit code: $?\"\n```\n\n### Validate JSON Output\n\nEnsure hooks output valid JSON:\n\n```bash\noutput=$(./your-hook.sh < test-input.json)\necho \"$output\" | jq .\n```\n\n## Quick Reference\n\n### Hook Events Summary\n\n| Event | When | Use For |\n|-------|------|---------|\n| PreToolUse | Before tool | Validation, modification |\n| PostToolUse | After tool | Feedback, logging |\n| UserPromptSubmit | User input | Context, validation |\n| Stop | Agent stopping | Completeness check |\n| SubagentStop | Subagent done | Task validation |\n| SessionStart | Session begins | Context loading |\n| SessionEnd | Session ends | Cleanup, logging |\n| PreCompact | Before compact | Preserve context |\n| Notification | User notified | Logging, reactions |\n\n### Best Practices\n\n**DO:**\n- ✅ Use prompt-based hooks for complex logic\n- ✅ Use ${CLAUDE_PLUGIN_ROOT} for portability\n- ✅ Validate all inputs in command hooks\n- ✅ Quote all bash variables\n- ✅ Set appropriate timeouts\n- ✅ Return structured JSON output\n- ✅ Test hooks thoroughly\n\n**DON'T:**\n- ❌ Use hardcoded paths\n- ❌ Trust user input without validation\n- ❌ Create long-running hooks\n- ❌ Rely on hook execution order\n- ❌ Modify global state unpredictably\n- ❌ Log sensitive information\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and advanced techniques, consult:\n\n- **`references/patterns.md`** - Common hook patterns (8+ proven patterns)\n- **`references/migration.md`** - Migrating from basic to advanced hooks\n- **`references/advanced.md`** - Advanced use cases and techniques\n\n### Example Hook Scripts\n\nWorking examples in `examples/`:\n\n- **`validate-write.sh`** - File write validation example\n- **`validate-bash.sh`** - Bash command validation example\n- **`load-context.sh`** - SessionStart context loading example\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-hook-schema.sh`** - Validate hooks.json structure and syntax\n- **`test-hook.sh`** - Test hooks with sample input before deployment\n- **`hook-linter.sh`** - Check hook scripts for common issues and best practices\n\n### External Resources\n\n- **Official Docs**: https://docs.claude.com/en/docs/claude-code/hooks\n- **Examples**: See security-guidance plugin in marketplace\n- **Testing**: Use `claude --debug` for detailed logs\n- **Validation**: Use `jq` to validate hook JSON output\n\n## Implementation Workflow\n\nTo implement hooks in a plugin:\n\n1. Identify events to hook into (PreToolUse, Stop, SessionStart, etc.)\n2. Decide between prompt-based (flexible) or command (deterministic) hooks\n3. Write hook configuration in `hooks/hooks.json`\n4. For command hooks, create hook scripts\n5. Use ${CLAUDE_PLUGIN_ROOT} for all file references\n6. Validate configuration with `scripts/validate-hook-schema.sh hooks/hooks.json`\n7. Test hooks with `scripts/test-hook.sh` before deployment\n8. Test in Claude Code with `claude --debug`\n9. Document hooks in plugin README\n\nFocus on prompt-based hooks for most use cases. Reserve command hooks for performance-critical or deterministic checks.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Hook Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Hook Development",
          "skill_dir": "hook-development",
          "skill_file_tree": "SKILL.md\nexamples/load-context.sh\nexamples/validate-bash.sh\nexamples/validate-write.sh\nreferences/advanced.md\nreferences/migration.md\nreferences/patterns.md\nscripts/README.md\nscripts/hook-linter.sh\nscripts/test-hook.sh\nscripts/validate-hook-schema.sh",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n### 1. What domain knowledge does this skill teach?\n\nThe skill teaches cloud-based LLM fine-tuning using TRL on Hugging Face Jobs infrastructure. Core concepts include:\n- Training methods (SFT, DPO, GRPO) and when to use each\n- Critical workflow requirements: ephemeral environments require Hub push, dataset format validation, timeout management\n- Hardware selection based on model size\n- Authentication flow for remote training (secrets passing)\n\n### 2. What workflow decisions does it enable?\n\n- Understanding that the training environment is ephemeral and ALL results are lost without Hub push\n- Knowing dataset format requirements differ by training method (DPO is especially strict)\n- Setting appropriate timeouts (default 30min is insufficient)\n- Choosing hardware based on model size and using LoRA for larger models\n\n### 3. What would a practitioner actually struggle with without this skill?\n\nThe #1 failure mode is **losing all training results** because practitioners don't realize:\n1. The environment is ephemeral\n2. They must explicitly configure Hub push with proper authentication\n3. Both the training config AND the job submission need specific settings\n\nA secondary struggle is **DPO dataset format mismatches** - most datasets need column mapping.\n\n---\n\n```yaml\nname: cloud-training-persistence-configuration\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  I want to fine-tune a 3B parameter language model on a cloud training service where the training environment is temporary/ephemeral (like Hugging Face Jobs). The training will take about 2 hours.\n\n  What are the critical configurations I need to ensure my trained model isn't lost when the job completes? Please explain both what needs to be configured in the training code/config AND in the job submission itself.\n\n  Also, what's a common pitfall with timeout settings for cloud training jobs?\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly identifies the critical configurations needed to persist training results from an ephemeral cloud training environment.\n\n    Key concepts that should be covered:\n\n    1. **Hub Push Configuration** (in training config/code):\n       - Must enable push_to_hub (True or equivalent)\n       - Must specify a target repository/model ID\n\n    2. **Authentication for Job** (in job submission):\n       - Must pass authentication token/credentials as a secret or environment variable to the job\n       - The job needs write access to push results\n\n    3. **Timeout Pitfall**:\n       - Default timeouts are often too short for real training\n       - Should set timeout longer than expected training time (with buffer)\n       - If timeout is exceeded, job is killed and progress is lost\n\n    Score 1.0 if: Response correctly explains ALL THREE concepts - (1) training config needs hub push enabled with target repo, (2) job submission needs authentication token passed as secret/env var, AND (3) mentions timeout defaults being too short and needing to set adequate timeout with buffer.\n\n    Score 0.5 if: Response covers 2 of the 3 concepts correctly, OR covers all 3 but with significant gaps in explanation.\n\n    Score 0.0 if: Response only covers 1 concept, misses the ephemeral nature of the environment, or gives incorrect advice about persisting training results.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about ephemeral cloud training environments - the concepts of persistence configuration, authentication passing, and timeout management apply broadly across cloud training platforms, not just Hugging Face Jobs. The grader is robust and accepts multiple valid explanations of the core concepts. However, the test loses some points on realism: while the underlying problem is genuine, the phrasing 'explain what configurations are needed' is somewhat educational/tutorial-like rather than a concrete practitioner task (e.g., 'configure this training job' or 'debug why my model wasn't saved'). The test would be stronger if framed as a specific scenario requiring action rather than explanation."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore writing the test case, let me identify the core domain knowledge:\n\n1. **What domain knowledge does this skill teach?**\n   - Ephemeral training environments and the critical need for explicit persistence\n   - Resource allocation trade-offs (hardware selection, memory management, cost vs. time)\n   - Dataset format compatibility across different training methods\n   - Asynchronous job lifecycle management\n\n2. **What workflow decisions does it enable?**\n   - When to validate datasets before expensive GPU training\n   - How to prevent catastrophic data loss in ephemeral environments\n   - When to use LoRA vs. full fine-tuning based on model size and memory\n   - How to estimate and set appropriate timeouts for training jobs\n   - Memory management strategies when hitting OOM errors\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - **Most critical**: Losing all training results because they didn't push to Hub (ephemeral environment not understood)\n   - Wasting GPU time/money on dataset format mismatches\n   - Jobs timing out because default settings are insufficient\n   - Not knowing the standard OOM mitigation hierarchy\n   - Understanding the async nature of cloud training\n\nThe most unique and critical knowledge is understanding **ephemeral training environments** and the absolute requirement for Hub persistence. This is a real-world failure mode that wastes hours of work and significant money.\n\n## Test Case\n\n```yaml\nname: cloud-training-environment-understanding\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A researcher asks you: \"I just finished a 4-hour training job on Hugging Face Jobs for my 7B parameter model. The job completed successfully according to the logs, but when I try to find my trained model on the Hub, it's not there. The only thing I can find is the base model I started with. What happened to my trained model, and how do I prevent this in the future?\"\n  \n  Explain what likely went wrong and provide specific guidance on how to configure future training jobs to avoid this problem.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should demonstrate understanding of ephemeral training environments and Hub persistence requirements.\n    \n    Score 1.0 if the response:\n    - Correctly identifies that the training environment is ephemeral/temporary and files are deleted after job completion\n    - Explains that the model was not pushed to the Hub, so it was lost\n    - Provides specific configuration requirements: push_to_hub=True, hub_model_id setting, AND HF_TOKEN in secrets\n    - Emphasizes that ALL THREE elements (push_to_hub config, hub_model_id, and token secrets) are required\n    \n    Score 0.5 if the response:\n    - Identifies that the model wasn't saved/pushed to Hub\n    - Mentions at least one or two of the required configuration elements (push_to_hub, hub_model_id, or secrets)\n    - Shows partial understanding that cloud environments don't persist data automatically\n    \n    Score 0.0 if the response:\n    - Suggests the model is saved locally somewhere or can be retrieved\n    - Doesn't identify the ephemeral nature of the training environment\n    - Provides incorrect or irrelevant troubleshooting (network issues, permissions without mentioning configuration)\n    - Misses the core concept that explicit Hub push configuration is required\n  \n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of transferable domain knowledge about ephemeral cloud environments and data persistence requirements. It tests a genuine practitioner problem (losing training results) that applies across cloud training platforms, not just HF Jobs. The grader is robust, accepting explanations that cover the core concepts (ephemeral environment, Hub push requirements) with clear scoring tiers. However, it loses some points on realism - while the scenario is plausible, the phrasing \"researcher asks you\" is slightly artificial. A more realistic framing would be a direct troubleshooting task or a planning scenario before training begins. The test is non-obvious (8/10), clear (9/10), and realistic (7/10)."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what it teaches:\n\n## Analysis\n\n**Domain Knowledge:**\n1. **Training method selection** (SFT vs DPO vs GRPO) - when to use each based on data type\n2. **Critical infrastructure requirements** - Hub persistence (ephemeral environment!), authentication tokens, timeouts\n3. **Dataset validation** - preventing costly failures by validating format first\n4. **Hardware-model sizing** - matching GPU flavor to model parameters and batch requirements\n5. **Cost-conscious planning** - timeout and resource estimation\n\n**Workflow Decisions Practitioners Face:**\n- Which training method fits my data and goals?\n- Do I need to validate my dataset first? (When is it worth the $0.01 CPU cost?)\n- What timeout should I set? (Default 30 min is too short)\n- Will my results survive? (Must push to Hub)\n- Can I afford this training? (Hardware selection, optimization)\n\n**What Fails Without This Skill:**\n- Choosing wrong method → wrong outputs or failed training\n- Skipping dataset validation → $5-10 GPU job fails on format error\n- Forgetting Hub push → all training lost forever\n- Insufficient timeout → job killed mid-training, wasted money\n- Wrong hardware → OOM or unnecessarily expensive\n\nI'll create a test that presents a realistic scenario requiring multiple correct decisions about method selection, validation, and critical setup.\n\n```yaml\nname: hugging-face-training-workflow-planning\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  You're helping a team plan a model training job. They want to fine-tune `mistralai/Mistral-7B` on preference data where they have pairs of model outputs labeled as \"better\" and \"worse\" (not instruction-completion pairs). They estimate ~5,000 examples and want to train on a100-large for 2 hours.\n\n  Identify the critical decisions and potential pitfalls for this job:\n  1. Which TRL training method should they use and why?\n  2. What dataset validation step is critical BEFORE launching the GPU training, and why?\n  3. What configuration is absolutely required to prevent losing all training results?\n  4. What timeout value would you recommend and why?\n  5. What's the key memory/parameter consideration for a 7B model on a100-large?\n\n  Provide your reasoning for each decision.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of the skill's core workflow decisions.\n\n    Score 1.0 if the response:\n    - Correctly identifies DPO as the method (preference/chosen-rejected data requires DPO, not SFT)\n    - Explains that dataset validation prevents costly GPU failures and should check for \"prompt\", \"chosen\", \"rejected\" columns (or equivalent that needs mapping)\n    - Identifies Hub pushing (push_to_hub=True, hub_model_id, and HF_TOKEN secret) as critical to prevent loss in ephemeral environment\n    - Recommends timeout >2 hours (e.g., 3h or 3.5h) with reasoning about buffer for loading/saving/network\n    - Mentions LoRA/PEFT for memory efficiency with 7B models to avoid OOM on a100-large\n    - Demonstrates understanding that these are interdependent decisions (method → dataset format → hardware → timeout)\n\n    Score 0.5 if the response:\n    - Correctly identifies DPO but reasoning is incomplete\n    - Mentions dataset validation but doesn't explain why it saves money/time\n    - Mentions Hub pushing but missing either push_to_hub, hub_model_id, or HF_TOKEN\n    - Recommends timeout at least 2 hours but without clear buffer reasoning\n    - Mentions LoRA or memory considerations but not both\n    - Shows awareness of 2-3 critical steps but misses connections\n\n    Score 0.0 if the response:\n    - Suggests SFT instead of DPO for preference data\n    - Omits dataset validation step entirely\n    - Doesn't mention Hub persistence or shows it's optional\n    - Suggests using default 30-min timeout without questioning it\n    - Shows no awareness of memory constraints with 7B models\n    - Treats these as isolated concerns rather than interconnected workflow\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core workflow decisions and tests transferable domain knowledge about training infrastructure, method selection, and failure prevention. It correctly identifies DPO for preference data, dataset validation to prevent costly failures, Hub persistence requirements, timeout planning, and memory considerations for 7B models. The grader is robust and accepts multiple valid approaches while checking for conceptual understanding rather than exact terminology. However, the test loses some points for realism - while the scenario is plausible, asking someone to \"identify critical decisions and potential pitfalls\" is somewhat artificial compared to \"help me set up this training job\" or \"review my training configuration.\" The test would be more realistic if framed as a practical task (e.g., \"create a training plan\" or \"what's wrong with this configuration\") rather than an explicit knowledge check."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this skill document, I can see it teaches critical knowledge about training language models on Hugging Face's cloud infrastructure. Let me analyze the core competencies:\n\n## Analysis\n\n1. **What domain knowledge does this skill teach?** \n   - Cloud-based model training workflows (ephemeral environments, Hub persistence)\n   - Different training methods (SFT, DPO, GRPO) and their data requirements\n   - Hardware selection and cost optimization for different model sizes\n   - Dataset validation and format requirements, especially DPO's strict column names\n   - Timeout management and checkpointing strategies\n\n2. **What workflow decisions does it enable?**\n   - When to validate datasets before expensive GPU training\n   - How to handle dataset format mismatches (mapping columns)\n   - Choosing appropriate hardware and timeouts based on model/dataset size\n   - Understanding the critical need for Hub persistence in ephemeral environments\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - DPO training failures due to dataset column mismatches (90% of datasets need mapping)\n   - Lost training results from not pushing to Hub (ephemeral environment)\n   - Wasted money from jobs timing out with default 30-minute limit\n   - Failed training runs from incorrect dataset formats\n\nBased on this analysis, I'll create a test case focusing on a realistic DPO training scenario with a preference dataset that has non-standard column names - this is the most common failure mode mentioned in the skill.\n\n```yaml\nname: dpo-training-dataset-format\nskills:\n- hugging-face-model-trainer\nprompt: |\n  I want to fine-tune a small language model using Direct Preference Optimization (DPO) on Hugging Face's cloud infrastructure. I have a preference dataset on the Hub at \"myuser/customer-preferences\" with the following columns:\n  - \"instruction\": the user query\n  - \"preferred_response\": the better response \n  - \"rejected_response\": the worse response\n  \n  The dataset has about 5,000 examples. I want to use Qwen/Qwen2.5-0.5B as the base model and save the final model to \"myuser/customer-assistant\".\n  \n  What are the key issues I need to address before launching the training job, and how would I prepare and submit this training job to avoid common failures? Please be specific about any dataset preprocessing needed and critical configuration settings.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly identifies and addresses the critical issues for DPO training on Hugging Face Jobs.\n\n    Score 1.0 if the response:\n    - Identifies that the dataset columns don't match DPO requirements (needs \"prompt\", \"chosen\", \"rejected\" not \"instruction\", \"preferred_response\", \"rejected_response\")\n    - Provides correct mapping/preprocessing code to transform the columns\n    - Mentions the ephemeral environment and need for push_to_hub=True with hub_model_id\n    - Recognizes that the default 30-minute timeout is insufficient and suggests 1-2+ hours\n    - Includes proper authentication setup (HF_TOKEN in secrets)\n    - Suggests appropriate hardware (t4-medium, l4x1, or a10g-small for this size)\n\n    Score 0.5 if the response:\n    - Identifies some but not all critical issues (e.g., mentions dataset format OR timeout OR Hub persistence)\n    - Provides a training setup that would work but misses important optimizations\n    - Gives generic DPO training advice without addressing the specific column mismatch\n\n    Score 0.0 if the response:\n    - Fails to identify the dataset column mismatch issue\n    - Doesn't mention the need for Hub persistence in the ephemeral environment\n    - Provides a configuration that would fail (wrong columns, no Hub push, inadequate timeout)\n    - Only gives generic fine-tuning advice without DPO-specific considerations\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable domain knowledge about DPO training workflows, particularly the critical dataset format mismatch issue that affects 90% of datasets. The grader is robust, accepting multiple valid approaches (different hardware choices, various timeout durations) while requiring demonstration of core competencies: dataset column mapping, Hub persistence, timeout management, and authentication. The scenario is highly realistic - practitioners frequently encounter preference datasets with non-standard column names. Minor deduction: the grader could be slightly more explicit about accepting alternative valid solutions (e.g., using dataset validation tools first, different mapping approaches), though it does avoid brittle string matching and focuses on conceptual understanding."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (skill → test design)\n# 1) Domain knowledge: How to run TRL training (esp. DPO) on Hugging Face Jobs with correct persistence\n#    (push_to_hub + HF_TOKEN secrets), monitoring (Trackio), and the \"ephemeral environment\" constraint.\n#    Also: dataset schema requirements (DPO needs prompt/chosen/rejected) and the importance of validating/mapping\n#    before spending GPU money; plus basic hardware/timeout selection for model size and asynchronous job behavior.\n# 2) Workflow decisions: Validate on cheap CPU first vs launching GPU job; choose DPO vs SFT; LoRA/PEFT usage;\n#    set timeout > default; submit via hf_jobs with inline UV/PEP723 script rather than local paths/terminal commands.\n# 3) Likely struggles without skill: forgetting Hub push + HF_TOKEN (losing the model), using wrong dataset columns,\n#    omitting Trackio, using too-short default timeout, or giving an approach that assumes local GPU / wrong job submission.\n\nname: dpo-hf-jobs-with-dataset-validation-and-hub-push\nskills:\n- hugging-face-model-trainer\nprompt: |\n  I don’t have a local GPU. I want to run a DPO alignment run on Hugging Face Jobs using TRL.\n\n  Details:\n  - Base model: Qwen/Qwen2.5-1.5B\n  - Dataset on the Hub: myorg/math-preference\n  - Dataset columns are NOT TRL-standard: it has\n    - instruction (string)\n    - accepted (string)\n    - rejected (string)\n  - I want the final model saved to the Hub as: myuser/qwen2.5-1.5b-math-dpo\n  - I want live monitoring (Trackio)\n  - I’m okay with LoRA/PEFT\n  - Please recommend a reasonable HF Jobs GPU flavor and timeout\n\n  Produce a practical plan AND the actual hf_jobs() submissions you would run (include the training script inline).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical competence running TRL DPO on Hugging Face Jobs.\n\n    Score 1.0 if the answer includes ALL of the following (allowing for equivalent wording/choices):\n    - Proposes validating the dataset format before GPU training (ideally via the HF dataset inspector script),\n      and shows (or clearly explains) how to map nonstandard columns (instruction/accepted/rejected) into\n      DPO-required fields (prompt/chosen/rejected).\n    - Provides at least one concrete hf_jobs(\"uv\", {...}) submission for the DPO training job that:\n      * passes an inline Python script (not a local file path),\n      * includes a PEP 723 dependency header (or an equivalent clear mechanism) with trl + datasets + peft + trackio,\n      * sets push_to_hub=True and hub_model_id=\"myuser/qwen2.5-1.5b-math-dpo\" (or an equivalent Hub target),\n      * includes HF auth via job secrets (secrets={\"HF_TOKEN\": \"$HF_TOKEN\"} or equivalent),\n      * includes Trackio reporting configuration (e.g., report_to=\"trackio\" plus a run/project name),\n      * uses LoRA/PEFT (e.g., LoraConfig) in a reasonable way.\n    - Recommends a GPU flavor appropriate for a ~1.5B model and gives a non-default timeout that is plausibly\n      >30 minutes (e.g., 1–2h+), with reasoning or at least a warning that the default timeout is too short.\n    - Notes that Jobs are asynchronous / provides monitoring guidance (e.g., where to watch logs / avoid polling),\n      and mentions the ephemeral environment risk (results lost without Hub push).\n\n    Score 0.5 if the answer is mostly correct but misses ONE major element above (e.g., no dataset validation step,\n    or no Trackio, or no HF_TOKEN secrets, or no mapping explanation), while still providing a usable HF Jobs plan.\n\n    Score 0.0 if the answer lacks multiple critical items (e.g., no Hub push/HF_TOKEN, no hf_jobs submission,\n    assumes local training, ignores dataset schema mismatch) such that following it would likely fail or lose results.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong domain knowledge by requiring dataset validation/mapping, Hub persistence with HF_TOKEN, Trackio monitoring, LoRA/PEFT usage, appropriate hardware selection, and asynchronous job handling—all transferable TRL/HF Jobs concepts. The grader is robust, accepting equivalent approaches while checking for critical elements. However, the test loses points on realism (score: 7/10) because the specific dataset column mismatch scenario (instruction/accepted/rejected → prompt/chosen/rejected) feels somewhat contrived for testing purposes rather than a naturally occurring task. A practitioner would more likely ask 'how do I run DPO training' without pre-specifying the exact column names. The test is still valuable and well-constructed, but the artificial precision of the dataset schema reduces its realism slightly."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (skill → what it enables → what people struggle with without it)\n# - Domain knowledge: Running TRL fine-tuning (esp. DPO) on Hugging Face Jobs using UV/PEP723 scripts, with correct\n#   dataset formatting, authentication, monitoring, and persistence to the Hub in an ephemeral environment.\n# - Workflow decisions: validate datasets on cheap CPU before burning GPU; choose HF Jobs submission style (inline UV script);\n#   pick hardware/LoRA/accumulation to avoid OOM; set timeouts; ensure push_to_hub + HF_TOKEN; include Trackio.\n# - Practitioner pain without this skill: jobs failing due to dataset column mismatch (DPO is strict), lost training artifacts\n#   due to missing Hub push/token, job timeouts, lack of monitoring, and broken Jobs submissions (e.g., local file paths).\n\nname: dpo-hf-jobs-dataset-mapping-hub-push-trackio\nskills:\n- hugging-face-model-trainer\nprompt: |\n  I have a Hugging Face Pro account and an HF token with write access stored in my environment as HF_TOKEN.\n\n  I want to run DPO fine-tuning on Hugging Face Jobs (no local GPU) using:\n  - Base model: Qwen/Qwen2.5-3B-Instruct\n  - Dataset on the Hub: myname/math-pref-dpo\n  - Dataset columns are: instruction, chosen_response, rejected_response (not TRL's default DPO column names)\n\n  Requirements:\n  1) Avoid wasting GPU money on dataset-format failures: do the cheap validation step first.\n  2) Then launch the actual DPO training job on a cost-conscious but realistic GPU flavor for a 3B model (use LoRA if appropriate to reduce VRAM).\n  3) Include Trackio monitoring in the training script.\n  4) Ensure the trained model is NOT lost when the job ends: push it to the Hub (use a hub_model_id like myname/qwen2.5-3b-math-dpo-lora).\n  5) Set a timeout that won’t kill the job mid-run (don’t use the too-short default).\n\n  Please output the exact code I should paste to submit the job(s). Use hf_jobs(\"uv\", ...) and include the scripts inline (not local file paths).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant response demonstrates correct, practitioner-useful knowledge for running TRL DPO training on Hugging Face Jobs.\n\n    Score 1.0 if the answer includes ALL (or functionally equivalent) elements:\n    - Uses Hugging Face Jobs submission with `hf_jobs(\"uv\", {...})` (not bash-only commands) and provides paste-ready code.\n    - Includes a cheap dataset validation/inspection step BEFORE GPU training (e.g., running a dataset inspector script on CPU),\n      OR an explicit explanation plus an equivalent concrete validation approach.\n    - Correctly handles the DPO dataset schema mismatch by mapping columns from\n      (instruction, chosen_response, rejected_response) → (prompt, chosen, rejected), shown in code.\n    - Training job script is inline (or URL) and NOT a local file path; includes PEP 723 dependency header (or otherwise clearly specifies deps).\n    - Includes Trackio integration (dependency + `report_to=\"trackio\"` and sensible run/project naming, or equivalent Trackio setup).\n    - Ensures persistence to the Hub: sets `push_to_hub=True`, sets a `hub_model_id` like `myname/qwen2.5-3b-math-dpo-lora`,\n      and passes the token into the job via `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}` (or equivalent secure injection).\n    - Sets a non-default timeout that is realistically >30 minutes (e.g., 1–2 hours or more) and mentions why.\n    - Chooses a reasonable GPU flavor for ~3B DPO and cost-conscious training, and uses a memory-conscious setup (e.g., LoRA/PEFT and/or small batch + grad accumulation).\n\n    Score 0.5 if the answer is mostly correct but is missing ONE or TWO of the critical operational safeguards above\n    (e.g., forgets the explicit dataset validation step, or forgets Trackio, or omits secrets injection), while still giving a viable HF Jobs plan.\n\n    Score 0.0 if the answer:\n    - Does not provide hf_jobs(\"uv\", ...) submission code, OR\n    - Fails to address Hub persistence/token injection, OR\n    - Ignores the DPO schema mismatch (no mapping/validation), OR\n    - Suggests using local file paths for Jobs scripts, OR\n    - Is generic advice without actionable, paste-ready job submission content.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong domain knowledge by testing critical DPO workflow elements: dataset validation before GPU training, column mapping for DPO format, Hub persistence with token injection, Trackio monitoring, timeout management, and hardware selection with LoRA. The grader is robust and accepts functionally equivalent solutions. However, it loses points on non-obviousness because it requires knowing specific implementation details like the exact `hf_jobs('uv', {...})` syntax, `secrets={'HF_TOKEN': '$HF_TOKEN'}` pattern, and PEP 723 dependency headers—these are skill-specific conventions rather than transferable domain concepts. A practitioner could understand DPO training principles without knowing Hugging Face's specific Jobs API syntax. The test is realistic and measures genuine workflow knowledge, but the grading criteria are somewhat brittle around exact API patterns."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Skill analysis:\n# - Domain knowledge: How to run TRL fine-tuning (esp. DPO) on Hugging Face Jobs with managed GPUs, including\n#   dataset schema requirements, ephemeral-job persistence via Hub push + HF_TOKEN secrets, Trackio monitoring,\n#   and practical timeout/hardware considerations.\n# - Workflow decisions: Validate dataset format on cheap CPU before GPU spend; choose inline UV scripts vs URL scripts;\n#   pick GPU flavor/timeout to avoid OOM/timeouts; ensure outputs are pushed to Hub; avoid local script paths.\n# - Practitioner struggle without skill: Wasting money/time on failed jobs from schema mismatch, losing models due to no\n#   Hub push, hitting default 30-min timeout, choosing underpowered hardware, or submitting non-runnable local scripts.\n\nname: hf-jobs-dpo-dataset-validation-hub-push\nskills:\n- hugging-face-model-trainer\nprompt: |\n  You are helping an ML engineer fine-tune a model using Hugging Face Jobs (no local GPU).\n\n  They want to do DPO alignment on the Hub dataset `acme/math-preferences`, whose columns are:\n  - instruction\n  - chosen_response\n  - rejected_response\n\n  Constraints / preferences:\n  - Use the `hf_jobs()` MCP tool with the \"uv\" runtime (do not give bash CLI commands).\n  - Keep the training script inline in the job submission (no local file paths).\n  - Log to Trackio so they can monitor training.\n  - Ensure the trained model is not lost when the job ends: push outputs to the Hub repo `myuser/qwen2.5-1.5b-math-dpo`.\n  - Pick a sensible single-GPU flavor for a ~1–2B parameter model and set a timeout that won’t hit the default 30‑minute limit.\n\n  Task:\n  1) Provide an `hf_jobs()` submission to quickly validate/inspect the dataset format on CPU (or cheapest option), and\n     explain what you are checking for in DPO.\n  2) Provide an `hf_jobs()` submission for the actual DPO training job (UV script). Include LoRA/PEFT, the dataset\n     column mapping to the DPO-required fields, Trackio config, and Hub push configuration (including secrets).\n  3) Briefly list 3 common failure modes/pitfalls on HF Jobs for this workflow and how your setup avoids them.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical competence with running TRL DPO fine-tuning on Hugging Face Jobs.\n\n    Score 1.0 if the answer includes ALL of the following (allowing reasonable variations):\n    - Uses the `hf_jobs(\"uv\", {...})` style job submission (not only bash/CLI), with the training script provided inline\n      or via an accessible URL (explicitly not a local file path).\n    - Includes a clear dataset validation/inspection step BEFORE spending GPU time (either calling a known dataset\n      inspector script on HF Hub OR an equivalent CPU job that loads the dataset and prints/validates columns), and\n      explains that DPO expects `prompt`, `chosen`, `rejected` (or equivalent) and why schema mismatches cause failures.\n    - Provides a DPO training job submission that:\n      * Loads `acme/math-preferences` and maps `instruction/chosen_response/rejected_response` into the DPO-required fields.\n      * Uses TRL's DPO trainer/config (e.g., DPOTrainer + DPOConfig or an official TRL DPO script with correct args).\n      * Enables PEFT/LoRA (or another memory-saving fine-tuning approach) in a plausible way.\n      * Configures Trackio reporting (e.g., `report_to=\"trackio\"` plus a project/run name or equivalent).\n      * Ensures persistence by pushing to the Hub repo `myuser/qwen2.5-1.5b-math-dpo` (e.g., `push_to_hub=True`,\n        `hub_model_id=...`) AND passes the HF token via job secrets (e.g., `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}` or\n        equivalent secure mechanism).\n      * Sets a non-default timeout meaningfully > 30 minutes (≈1h+ or otherwise justified) and chooses a reasonable GPU\n        flavor for a 1–2B model (e.g., t4-medium/l4x1/a10g-small; must not be CPU for training).\n    - Lists at least 3 realistic HF Jobs pitfalls/failure modes and mitigations, with at least two of these themes present:\n      ephemeral filesystem requiring hub push, dataset format mismatch, OOM mitigation (batch size/grad accumulation/\n      checkpointing/LoRA/hardware), timeout too short, missing dependencies, auth/token issues, local path script misuse.\n\n    Score 0.5 if the answer is mostly correct but misses ONE major required element above (e.g., no dataset validation step,\n    or no HF_TOKEN secrets, or no Trackio), while still showing a coherent HF Jobs + DPO plan.\n\n    Score 0.0 if the answer does not provide a plausible HF Jobs submission for DPO training (e.g., only generic local\n    training advice, only CLI commands despite the constraint, no hub push/auth handling, or no understanding of DPO\n    dataset requirements).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong domain knowledge by testing practical HF Jobs workflow decisions (dataset validation before GPU spend, Hub push for ephemeral environments, timeout/hardware selection, inline UV scripts). It avoids trivia by focusing on transferable concepts (DPO schema requirements, auth patterns, cost optimization) rather than memorizing exact script names. The grader is mostly robust, accepting variations in implementation while checking for core competencies. However, it loses points for clarity because the grader is somewhat complex with many nested conditions that could potentially reject valid alternative approaches (e.g., different but valid GPU choices, alternative validation methods), and the prompt's multi-part structure may make it harder to grade consistently. The realism is excellent—this represents an authentic practitioner workflow combining dataset validation, DPO training, and production concerns."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "The skill centers on planning and executing TRL-based fine-tuning jobs on Hugging Face Jobs infrastructure. Core knowledge includes when to choose SFT/DPO/GRPO, how to prepare datasets (especially validation/mapping), configuring cloud GPU jobs (hardware sizing, timeouts, cost), ensuring Trackio monitoring and Hub persistence, and managing asynchronous job submissions via `hf_jobs()` with inline UV scripts. Practitioners must decide when to validate datasets, how to structure training scripts (LoRA, dependencies, push-to-hub), and how to provide job details post-submission. Without this skill, they’d struggle with dataset compatibility, proper job submission (secrets, timeouts, hardware), Trackio integration, and ensuring results persist.\n\n```yaml\nname: plan-hf-jobs-dpo-training\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A teammate wants to run Direct Preference Optimization (DPO) fine-tuning on a 7B base model using a private dataset they just uploaded to the Hugging Face Hub. They have no local GPUs and insist everything runs on Hugging Face Jobs. They are unsure about dataset readiness, monitoring, hardware/timeouts, or how to make sure the resulting weights aren’t lost.\n\n  Draft a concrete action plan (no code execution needed) that gets them from dataset validation to job submission. Your plan must address:\n  - How to validate and, if necessary, remap the dataset before touching GPUs.\n  - The exact job submission approach they should use (tool, script style, dependency declaration).\n  - How Trackio is incorporated and why it matters.\n  - Hardware flavor and timeout choices suitable for a 7B model with LoRA, plus any cost/time considerations.\n  - Ensuring the model pushes to the Hub and the required secrets/auth configuration.\n  - Key reminders about asynchronous job behavior and what information to report back after submission.\n\n  Be specific enough that the teammate could follow your steps without guessing.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of the hugging-face-model-trainer skill.\n\n    Score 1.0 if:\n    - Explains validating the dataset with the recommended inspector workflow before GPU use (including handling remapping if needed).\n    - Recommends submitting the training via `hf_jobs(\"uv\", {...})` (or equivalent) with inline/URL script, mentions PEP 723 dependencies, and notes that Trackio must be included in the script.\n    - Specifies LoRA-friendly hardware (e.g., a10g-large or similar) and a timeout exceeding one hour, with some rationale about cost/time.\n    - Covers Hub persistence requirements (push_to_hub=True, hub_model_id, secrets with HF_TOKEN) and why this prevents data loss.\n    - Mentions asynchronous nature of Jobs, advising to share job ID/monitoring links and not to poll automatically.\n\n    Score 0.5 if two or three of the above elements are clearly satisfied but key directives are missing or incorrect.\n\n    Score 0.0 if the response omits most directives, gives unsafe/wrong instructions (e.g., local GPU training, no dataset validation, no Hub push), or is unrelated.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core knowledge areas. It effectively tests dataset validation workflows, job submission mechanics (hf_jobs with UV scripts, PEP 723), Trackio integration, hardware/timeout planning for 7B models with LoRA, Hub persistence requirements (secrets, push_to_hub), and asynchronous job behavior. The grader is robust and accepts multiple valid approaches while checking for key directives. Minor deductions: (1) The test is somewhat lengthy and could be more focused, (2) The grader could be slightly more specific about acceptable hardware alternatives beyond a10g-large, and (3) While realistic, the scenario bundles many concerns together which might make partial credit scoring less granular. Overall, this is a high-quality test that measures transferable domain knowledge rather than trivia."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill teaches the mechanics of cloud-based LLM orchestration. It focuses on the lifecycle of a training job: environment setup (UV/PEP 723), resource allocation (GPU flavors), persistence (Hugging Face Hub integration), monitoring (Trackio), and the specific configuration requirements of the TRL (Transformer Reinforcement Learning) library.\n\n2.  **What workflow decisions does it enable?**\n    It enables decisions on how to move from local experimentation to managed cloud infrastructure. Key decisions include:\n    - Choosing appropriate hardware based on model parameters (e.g., A10G for 7B models).\n    - Setting realistic timeouts to avoid job termination.\n    - Ensuring \"ephemeral-to-permanent\" data flow (using secrets and Hub push).\n    - Selecting the right training method (SFT vs. DPO vs. GRPO) and configuring library-specific parameters correctly (e.g., `max_length` vs the common mistake `max_seq_length`).\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner would likely experience \"silent\" or \"frustrating\" failures:\n    - Jobs timing out at the default 30-minute mark just before finishing.\n    - Losing hours of training work because they forgot to pass the `HF_TOKEN` secret or set `push_to_hub=True`.\n    - Script crashes due to `max_seq_length` being an invalid argument in TRL's config classes.\n    - Inefficient resource usage (e.g., trying to train a 7B model on a T4-small).\n\n---\n\n```yaml\nname: hf-jobs-dpo-orchestration\nskills:\n- hugging-face-model-trainer\nprompt: |\n  I want to fine-tune the `Qwen/Qwen2.5-7B` model using Direct Preference Optimization (DPO). \n  Please use the `trl-lib/ultrafeedback_binarized` dataset. \n  \n  I need to run this on Hugging Face Jobs. Please:\n  1. Write the training script and submit it immediately using the correct tool.\n  2. Ensure the model is saved to my profile at `my-user/qwen-7b-dpo-tuned`.\n  3. Include Trackio monitoring.\n  4. Use appropriate hardware for a 7B model.\n  5. The training will likely take about 3 hours.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The assistant must submit a training job using the `hf_jobs` tool with an inline UV script.\n\n    Score 1.0 if the following criteria are met:\n    1. Tool Usage: Uses `hf_jobs(\"uv\", ...)` with the script content passed as an inline string.\n    2. Persistence: The script sets `push_to_hub=True` (and specifies `hub_model_id`), AND the `hf_jobs` call includes `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}`.\n    3. Timeout: The `timeout` in the `hf_jobs` call is set to at least \"3h\" or \"180m\" (as requested by the user).\n    4. Hardware: Selects an appropriate GPU for a 7B model (e.g., `a10g-large` or `a100-large`).\n    5. TRL Logic: Uses `DPOConfig` and correctly uses `max_length` (or omits it to use defaults), but specifically does NOT use the invalid `max_seq_length`.\n    6. Monitoring: Includes `trackio` in the dependencies and sets `report_to=\"trackio\"` in the config.\n\n    Score 0.5 if:\n    - The job is submitted but misses one critical component like the `HF_TOKEN` secret, the specific timeout requested, or uses the wrong hardware flavor.\n    - Or if it saves the script to a local file first instead of passing it inline (contrary to directives).\n\n    Score 0.0 if:\n    - It suggests running bash commands instead of using the tool.\n    - It fails to include Hub push logic (meaning training results would be lost).\n    - It uses the wrong training method (e.g., SFT instead of DPO).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable orchestration knowledge (timeout management, persistence patterns, hardware selection) rather than trivia. The grader is mostly robust, accepting valid DPO configurations while checking critical requirements. However, it loses points for slight brittleness: it requires exact tool syntax (`hf_jobs(\"uv\", ...)`) when the skill also documents CLI alternatives, and the `max_seq_length` check, while testing real knowledge, is somewhat specific to TRL's API quirks. The scenario is highly realistic—practitioners frequently need to orchestrate cloud training jobs with proper resource allocation and persistence."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\n1. **Domain knowledge taught:** The skill explains how to run TRL-based training (SFT, DPO, GRPO, reward modeling) on Hugging Face Jobs, covering dataset validation, timeout/cost estimation, Trackio monitoring, hardware guidance, mandatory hub push, GH tokens, GGUF conversion, and inline UV scripts. The key domain concepts include understanding asynchronous cloud GPU jobs, ensuring persistence via hub pushes, embedding Trackio reporting, dataset formatting requirements (especially for DPO), and the nuance of using inline inline scripts with `hf_jobs()` instead of local scripts.\n\n2. **Workflow decisions enabled:** The skill helps decide when to validate a dataset (e.g., unknown/custom datasets, DPO); when to pick UV inline scripts versus maintained TRL scripts or CLI commands; choosing hardware based on model size and cost; when to include LoRA; how to ensure hub persistence (TOKEN + push settings); how to handle timeouts; when to use dataset inspector before launching expensive GPU jobs; and how to include Trackio monitoring appropriately.\n\n3. **Practitioner struggle without the skill:** Without this skill, practitioners might launch GPU jobs without dataset validation (leading to failures), forget to push to the hub (losing work), set insufficient timeouts, omit Trackio monitoring, use incorrect script submission methods, fail to map dataset columns for DPO, or misconfigure job submission (missing secrets, inline scripts). They’d also struggle to handle asynchronous job reporting and cost/time estimation.\n\n```yaml\nname: dpo-training-with-validation-and-trackio\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A researcher wants to run a DPO fine-tuning job on Hugging Face Jobs using the custom dataset `username/pref-data-2025` (split \"train\"), which currently has columns `instruction`, `chosen_response`, and `rejected_response`. They need to:\n  \n  1. Confirm the dataset is compatible before spending on GPUs and map the columns into the format DPO expects (prompt/chosen/rejected) if necessary.\n  2. Launch a DPO job with `hf_jobs(\"uv\", …)` using inline UV script (PEP 723 dependencies) that includes Trackio logging, LoRA, proper timeout, `push_to_hub=True`, and LoRA/PEFT config suited for a 3B+ model on `a10g-large`.\n  3. Ensure secrets/HF_TOKEN are passed for hub push, mention estimated runtime (~2h), and report the job ID, monitoring URL, and Trackio dashboard in the response.\n\n  Draft what you would respond to the researcher, including the dataset validation step, the inline script description (key elements only), the job submission parameters, and the follow-up information they should get once the job is submitted.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria:\n    \n    Score 1.0 if the answer:\n    - Includes a dataset validation step (hf_jobs call or equivalent) that checks the dataset and maps its columns to prompt/chosen/rejected if needed.\n    - Shows a UV-style inline script submission via hf_jobs with Trackio reporting, DPOTrainer (or equivalent), LoRA/PEFT settings, push_to_hub enabled with hub_model_id, timeout ≥1 hour, secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}, and hardware a10g-large.\n    - Mentions estimated runtime (~2h or similar buffer), provides job ID/monitor URL placeholders, and explains how the user can request status later.\n    \n    Score 0.5 if the answer includes most bullet points but omits one critical requirement (e.g., missing Trackio, hub push, or dataset validation detail).\n    \n    Score 0.0 if the response fails to describe a correct job submission workflow (e.g., no hf_jobs usage, no Trackio/hub push, or lacks validation step).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong domain knowledge by requiring dataset validation, DPO-specific column mapping, inline UV script submission with Trackio, and proper hub persistence configuration. However, it has moderate clarity issues: the grader accepts responses that merely 'describe' or 'mention' elements rather than requiring actual executable code, making it unclear whether a conceptual explanation or working implementation is needed. The realism is good (DPO training with custom datasets is common), but the prompt's request to 'draft what you would respond' creates ambiguity about whether the agent should produce runnable code or documentation. The non-obviousness is solid—it tests transferable concepts like async job management, dataset format validation, and persistence patterns rather than memorizing specific script names."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n\nLet me analyze this skill document thoroughly to create a high-quality test case.\n\n## Analysis of the Skill Document\n\n**1. Domain Knowledge:**\n- Training language models using TRL (Transformer Reinforcement Learning) on Hugging Face Jobs\n- Understanding different training methods: SFT, DPO, GRPO, Reward Modeling\n- Knowledge of cloud GPU infrastructure and hardware selection\n- Dataset preparation and validation workflows\n- Model persistence and Hub authentication\n- Cost estimation and monitoring with Trackio\n- GGUF conversion for local deployment\n\n**2. Workflow Decisions:**\n- When to use UV scripts vs TRL maintained scripts vs CLI\n- How to handle asynchronous job submission and monitoring\n- When to validate datasets vs when to skip validation\n- Hardware selection based on model size and use case\n- Timeout management and cost optimization strategies\n- When and how to push models to Hub to avoid data loss\n\n**3. Practitioner Pain Points:**\n- Choosing the right training method for their specific use case\n- Avoiding common failures like OOM, dataset format mismatches, timeout issues\n- Understanding the ephemeral nature of Jobs environment and necessity of Hub push\n- Estimating costs and appropriate timeouts before launching expensive jobs\n- Setting up proper authentication and monitoring\n\n**Test Target:** The key pain point practitioners face is planning a training job correctly - choosing hardware, estimating costs, setting appropriate timeouts, and ensuring proper configuration to avoid common failures. This requires understanding the relationships between model size, dataset size, hardware capabilities, and critical configuration parameters.\n\nNow I'll create a test case that measures whether the model can apply this integrated knowledge:\n```yaml\nname: hugging-face-training-job-planning\nskills:\n- hugging-face-model-trainer\nprompt: |\n  I want to fine-tune a 7B parameter language model using SFT (Supervised Fine-Tuning) on Hugging Face Jobs. I have a dataset with 8,000 examples and want to train for 3 epochs. I need help planning this properly to avoid common issues.\n\nPlease provide:\n1. Recommended hardware choice and justification\n2. Estimated training time and cost\n3. Appropriate timeout setting with buffer\n4. Critical configuration parameters to prevent failures\n5. Whether I need to validate my dataset format and why\n\nThis is for production use, so I want to ensure the job completes successfully and the model is saved permanently.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on understanding of Hugging Face Jobs training planning principles from the skill.\n\n    Score 1.0 if: Response demonstrates comprehensive planning knowledge including:\n    - Correct hardware recommendation for 7B model (a10g-large/a100-large with LoRA)\n    - Realistic time estimate (3-6 hours) and cost estimate ($15-30)\n    - Appropriate timeout setting (4-6 hours with buffer)\n    - Lists critical configurations (push_to_hub=True, hub_model_id, HF_TOKEN secrets)\n    - Mentions dataset validation necessity and reasons\n    - Covers all key planning aspects to prevent failures\n\n    Score 0.5 if: Response shows partial understanding with:\n    - Correct hardware but missing justification or LoRA recommendation\n    - Time/cost estimates that are plausible but not well-reasoned\n    - Mentions some critical configuration but missing Hub push or timeout\n    - Briefly mentions dataset validation without explaining why\n    - Covers 2-3 planning aspects but lacks comprehensive approach\n\n    Score 0.0 if: Response lacks fundamental planning knowledge:\n    - Recommends inadequate hardware (t4-small/medium for 7B model)\n    - No timeout consideration or default 30min timeout\n    - Missing critical Hub push configuration\n    - No mention of dataset validation or why it's important\n    - Fails to address key failure prevention aspects\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of domain knowledge by testing integrated planning decisions (hardware selection, timeout management, Hub configuration) rather than trivia. However, it has moderate clarity issues: the grader accepts only specific hardware recommendations (a10g-large/a100-large) when the skill document shows a10g-small could also work for 7B models with LoRA, and the time/cost estimates are quite narrow ranges that may reject reasonable alternatives. The realism is strong - planning a training job to avoid failures is a genuine practitioner task. The test would benefit from more flexible acceptance criteria for valid alternative approaches."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this skill document, I can see it teaches critical workflow knowledge for TRL training on Hugging Face Jobs infrastructure. Let me analyze the core domain knowledge and create a test case.\n\n## Domain Knowledge Analysis\n\nThis skill teaches practitioners:\n1. **TRL method selection** - When to use SFT, DPO, GRPO, etc. based on training goals\n2. **Critical workflow decisions**:\n   - Dataset validation before GPU training (prevents 50%+ of training failures)\n   - Hub push configuration (ephemeral environment = all results lost without push)\n   - Timeout management (default 30min is insufficient)\n   - Hardware selection based on model size\n3. **Tool usage** - Using `hf_jobs()` MCP tool vs bash commands\n4. **Cost/time estimation** workflows\n\n## Test Case Focus\n\nThe most unique and critical knowledge here is the **dataset validation workflow** - practitioners routinely waste $1-10 and 30-60 minutes on GPU training failures due to dataset format issues. The skill teaches checking format compatibility BEFORE launching expensive training jobs.\n\n```yaml\nname: dataset-validation-before-training\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A user wants to train a DPO model using this preference dataset:\n  \n  Dataset: \"argilla/distilabel-math-preference-dpo\"\n  Format: instruction, chosen_response, rejected_response\n  \n  The user plans to use an a10g-large GPU for 3 hours and wants to fine-tune a 7B parameter model.\n  \n  What should you do before launching this training job? Provide specific next steps.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response:\n    - Identifies that dataset format validation is needed BEFORE training\n    - Mentions using the dataset inspector tool/script to check compatibility\n    - Explains that DPO requires specific column names (prompt, chosen, rejected)\n    - Warns about potential format mismatch with the given dataset columns\n    - Provides specific validation command or approach\n    \n    Score 0.5 if the response:\n    - Mentions dataset validation but lacks specificity about DPO format requirements\n    - Provides some validation approach but misses the critical timing (before GPU training)\n    \n    Score 0.0 if the response:\n    - Proceeds directly to training without validation\n    - Uses bash commands instead of proper MCP tools\n    - Ignores dataset format compatibility issues\n  extractor: last_assistant\n```\n",
          "score": 0.75,
          "rationale": "The test case demonstrates strong domain knowledge by focusing on dataset validation workflow - a genuine practitioner concern that prevents costly GPU training failures. It tests transferable knowledge about format compatibility checking rather than memorizing specific tool names. However, the grader has moderate brittleness: it requires mentioning 'dataset inspector tool/script' specifically, which could reject valid alternatives like manual format checking or using other validation approaches. The test is realistic and measures useful knowledge, but the grader's specificity about exact validation methods (rather than the underlying concept of pre-validation) reduces robustness slightly."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: hf-jobs-dpo-training-setup\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  I want to run a DPO training job on Hugging Face Jobs.\n  \n  Model: Qwen/Qwen2.5-0.5B\n  Dataset: argilla/distilabel-math-preference-dpo\n  \n  Please submit the job.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The user wants to run a DPO (Direct Preference Optimization) training job on Hugging Face infrastructure.\n    \n    Evaluate the assistant's tool call or response based on the safety and reliability guidelines in the skill.\n    \n    Score 1.0 if:\n    - The Assistant calls `hf_jobs` (or `uv run` via jobs) with the dataset inspector script to validate the dataset format FIRST (this is a key directive for DPO).\n    - OR, The Assistant submits a training job via `hf_jobs` that meets ALL these criteria:\n      1. **Script Source:** Uses an INLINE python script (PEP 723) or a public URL. It must NOT use a local file path (e.g., \"train.py\") which would fail in the container.\n      2. **Timeout:** Sets a `timeout` significantly > 30 minutes (e.g., \"2h\", \"120m\"). The skill warns 30m is too short.\n      3. **Persistence:** Configures `push_to_hub=True` in the script AND passes `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}` in the job config.\n      4. **Method:** Uses `DPOTrainer` logic.\n    \n    Score 0.5 if:\n    - The Assistant submits the job using `hf_jobs` but fails ONE of the critical checks (e.g., leaves default timeout, forgets secrets, or references a local file).\n    \n    Score 0.0 if:\n    - The Assistant attempts to run the training locally on the user's machine.\n    - The Assistant provides a bash script without using the `hf_jobs` tool.\n    - The Assistant misses multiple critical configuration steps (e.g., no timeout extension AND no hub push).\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues. For **non-obviousness** (3/10): It tests specific tool usage (`hf_jobs`) and exact parameter names (`secrets`, `timeout`) rather than transferable domain knowledge about DPO training or distributed training concepts. For **clarity** (4/10): The grader is brittle - it requires dataset validation OR a perfect job submission, but the skill shows validation is recommended for unknown datasets, not mandatory. The grader also accepts inline scripts OR URLs but penalizes local paths, creating confusion about what's actually being tested. For **realism** (6/10): Running DPO training is realistic, but the prompt is too minimal - a real user would provide more context about their goals, making this feel like a contrived test of whether the model remembers specific tool syntax rather than solving an actual problem."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: fine-tune-model-on-hugging-face-jobs\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  You have been asked to fine-tune a 7B parameter language model using the SFT (Supervised Fine-Tuning) method on Hugging Face Jobs. The dataset is available on the Hugging Face Hub and contains 10,000 examples. The user wants to use LoRA for efficient fine-tuning and has provided an Hugging Face API token with write permissions. Write the necessary script to submit the training job, ensuring that the model is saved to the Hugging Face Hub after training. Provide the user with the job ID, monitoring URL, and estimated time to completion.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n    \n    Score 1.0 if:\n    - The response includes a complete and correct UV script for submitting the training job using `hf_jobs(\"uv\", {...})`.\n    - The script correctly uses the `SFTTrainer` and `SFTConfig` from the `trl` library.\n    - The script includes the necessary LoRA configuration.\n    - The script sets `push_to_hub=True` and specifies a valid `hub_model_id`.\n    - The script includes Trackio for real-time monitoring.\n    - The response provides the user with the job ID, monitoring URL, and an estimated time to completion.\n    \n    Score 0.5 if:\n    - The response includes a UV script but is missing one or more of the required elements (LoRA configuration, hub push, Trackio, etc.).\n    - The response provides partial information about the job ID, monitoring URL, or estimated time.\n    \n    Score 0.0 if:\n    - The response does not include a UV script or is incorrect.\n    - The response fails to provide the job ID, monitoring URL, or estimated time to completion.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant clarity issues. It requires the model to 'write the necessary script' and 'provide the user with job ID, monitoring URL, and estimated time' - but the model cannot actually submit jobs or receive real job IDs. The grader expects actual job IDs and monitoring URLs that would only exist after real submission. This creates a broken evaluation where valid responses (showing how to submit a job) would fail because they can't provide real job IDs. The test also lacks specificity about the dataset name and model name, making it impossible to write a complete script. While it tests relevant knowledge about SFT training with LoRA and hub pushing, the grader's expectations are unrealistic for a text-based evaluation context."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: test-training-models-with-hugging-face-jobs\nskills:\n- hugging-face-model-trainer\nprompt: |\n  Train a language model using the Hugging Face Jobs infrastructure with the SFT method on a dataset of your choice. Ensure that the model is saved to the Hugging Face Hub and that Trackio is used for real-time monitoring. Provide the job ID, monitoring URL, and estimated completion time.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes: \n    1. A job ID, \n    2. a monitoring URL, \n    3. an estimated completion time, \n    4. confirmation that the model will be saved to the Hugging Face Hub, \n    5. and Trackio will be used for monitoring.\n    Score 0.5 if the response is mostly correct but lacks one or two details.\n    Score 0.0 if the response does not demonstrate understanding of training models with Hugging Face Jobs.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "This test has significant issues across all dimensions. For non-obviousness (score: 3/10), it tests basic workflow execution rather than transferable domain knowledge - it's essentially 'follow the tutorial' without testing understanding of when/why to use specific configurations, hardware choices, or troubleshooting. For clarity (score: 4/10), the grader is overly permissive and doesn't verify the actual submission was correct (checking for job ID/URL doesn't validate the script had proper Hub push configuration, Trackio setup, or appropriate timeout). The prompt says 'dataset of your choice' but doesn't verify format compatibility, and accepts any response claiming to use Trackio without checking implementation. For realism (score: 5/10), while training models is realistic, the task is too vague ('dataset of your choice') and doesn't represent a genuine practitioner scenario - real users have specific requirements, constraints, or problems to solve, not just 'train something and report back.'"
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-hosted-agents",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 9,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hosted-agents\n\n**SKILL.md Content:**\n```markdown\n---\nname: hosted-agents\ndescription: This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.\n---\n\n# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hosted-agents\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hosted-agents\n\n**SKILL.md Content:**\n```markdown\n---\nname: hosted-agents\ndescription: This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.\n---\n\n# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hosted-agents\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "hosted-agents",
          "skill_dir": "hosted-agents",
          "skill_file_tree": "SKILL.md\nreferences/infrastructure-patterns.md\nscripts/sandbox_manager.py",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n1. **What domain knowledge does this skill teach?**\n   - PDF manipulation using Python libraries (pypdf for basic operations, pdfplumber for text/table extraction, reportlab for creation)\n   - The critical distinction between fillable and non-fillable PDF forms, and different workflows for each\n   - Command-line tools as alternatives (qpdf, pdftotext)\n   - OCR workflow for scanned PDFs (convert to images first, then use pytesseract)\n\n2. **What workflow decisions does it enable?**\n   - Choosing pypdf for merge/split/rotate vs pdfplumber for text/table extraction vs reportlab for creation\n   - Understanding that PDF form filling requires first checking if fields are fillable before choosing an approach\n   - Knowing when OCR is needed (scanned PDFs) vs direct text extraction\n\n3. **What would a practitioner struggle with without this skill?**\n   - Selecting the appropriate library for table extraction (pdfplumber is best, not pypdf)\n   - Understanding the two-path workflow for PDF forms (fillable vs non-fillable)\n   - Knowing that scanned PDFs require image conversion + OCR rather than direct text extraction\n\nA good test should focus on a realistic scenario where library/tool selection matters. The table extraction use case is particularly good because it's common and the skill document clearly differentiates which tool is best for this task (pdfplumber over pypdf).\n\n```yaml\nname: pdf-table-extraction-approach\nskills:\n  - pdf\nprompt: |\n  I have a financial report PDF that contains several data tables with quarterly revenue figures. I need to extract these tables and convert them into Excel spreadsheets for further analysis in pandas.\n  \n  What Python library should I use for extracting tables from PDFs, and can you show me how to extract all tables from a multi-page PDF and save them to an Excel file?\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly identifies the best approach for extracting tables from PDFs.\n    \n    Score 1.0 if:\n    - The response recommends pdfplumber as the library for table extraction (this is the correct choice for structured table extraction)\n    - The code demonstrates using page.extract_tables() or similar pdfplumber table extraction method\n    - The solution shows converting extracted tables to pandas DataFrames and saving to Excel\n    \n    Score 0.5 if:\n    - The response recommends pdfplumber but the code has minor issues or is incomplete\n    - OR the response uses a reasonable alternative (like tabula-py or camelot) with working code\n    - OR the response correctly explains the approach but code is missing key steps\n    \n    Score 0.0 if:\n    - The response recommends pypdf for table extraction (pypdf is for basic operations, not table extraction)\n    - OR the response only shows basic text extraction without table-specific handling\n    - OR the response doesn't address the table extraction task at all\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable domain knowledge about choosing the right PDF library for table extraction (pdfplumber vs pypdf), which is a genuine practitioner concern. The grader is mostly robust, accepting pdfplumber and reasonable alternatives like tabula-py/camelot. The task is highly realistic - extracting financial tables to Excel is a common real-world need. Minor deduction: the grader could be slightly more explicit about what constitutes 'working code' and the 0.5 criteria is somewhat broad, but overall this is a strong test that measures actual skill application rather than trivia."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore creating the test case, let me analyze what this skill teaches:\n\n**1. Core domain knowledge:**\n- Different Python libraries have different strengths: pypdf for basic operations, pdfplumber for table/text extraction, reportlab for creation\n- PDF forms come in two types (fillable fields vs. non-fillable) requiring completely different approaches\n- Text extraction from scanned PDFs requires OCR (separate from native PDF text extraction)\n- Table extraction requires specialized tools and often needs conversion to structured formats\n\n**2. Key workflow decisions:**\n- When to use pdfplumber vs. pypdf for text extraction (layout preservation, table extraction)\n- How to properly extract and structure tabular data from PDFs\n- The multi-step process for form filling (check fillability → extract fields → analyze → populate)\n- When command-line tools are more appropriate than Python libraries\n\n**3. What practitioners struggle with:**\n- Choosing the wrong library for table extraction (pypdf can't extract tables well, but pdfplumber can)\n- Not knowing that table extraction requires special handling beyond simple text extraction\n- Understanding that PDF tables need to be converted to structured formats (DataFrames, Excel, CSV)\n\n**The test target:** Understanding when and how to extract structured tabular data from PDFs, which requires knowing that tables need specialized tools (pdfplumber) and conversion to structured formats.\n\n---\n\n```yaml\nname: extract-tables-from-pdf\nskills:\n  - pdf\nprompt: |\n  A client has sent you a PDF report containing several data tables that you need to analyze. The PDF is a native (not scanned) document with multiple pages, and at least some pages contain structured tables with rows and columns.\n  \n  Explain your approach for extracting these tables from the PDF into a format suitable for data analysis (like a spreadsheet or CSV). Your answer should cover:\n  1. Which Python library or tool you would use and why\n  2. The general code approach for extracting tables from multiple pages\n  3. How you would handle converting the extracted tables into a structured data format\n  \n  Focus on the methodology and tool selection rather than perfect syntax.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should demonstrate understanding that PDF table extraction requires specialized tools and workflows.\n    \n    Score 1.0 if the response:\n    - Identifies pdfplumber (or similar table-aware library like camelot/tabula) as appropriate for table extraction\n    - Explains that basic text extraction tools (like simple pypdf text extraction) are insufficient for tables\n    - Describes iterating through pages and extracting tables\n    - Mentions converting extracted tables to structured format (pandas DataFrame, CSV, Excel, etc.)\n    - Shows understanding that tables require special handling beyond plain text extraction\n    \n    Score 0.5 if the response:\n    - Identifies a reasonable approach but with gaps (e.g., mentions pdfplumber but doesn't explain conversion to structured format)\n    - OR uses a suboptimal but workable approach (like using OCR when not needed, or complex text parsing)\n    - Shows partial understanding of table extraction workflow\n    \n    Score 0.0 if the response:\n    - Suggests using basic text extraction (pypdf extract_text()) without acknowledging tables need special handling\n    - Proposes manual/visual approaches without mentioning programmatic table extraction libraries\n    - Shows no understanding that PDF tables require specialized tools\n    - Focuses entirely on irrelevant aspects (like merging/splitting PDFs, form filling, etc.)\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of evaluation principles. It tests genuine domain knowledge (choosing appropriate tools for table extraction vs. basic text extraction) rather than trivia, uses a robust judge-based grader that accepts multiple valid approaches (pdfplumber, camelot, tabula), and represents a realistic practitioner task. The grading criteria clearly distinguish between full understanding (1.0), partial understanding (0.5), and fundamental misunderstanding (0.0). Minor deduction because the test could be slightly more specific about what constitutes 'structured format' in the partial credit scenario, and the prompt could be more concrete (e.g., specifying a use case like 'financial data' or 'survey results'), but overall this is a high-quality test that effectively measures transferable PDF processing knowledge."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this PDF skill document, let me first analyze what practitioners actually need to know:\n\n## Skill Analysis\n\n**Domain Knowledge:**\n- When to use different PDF libraries based on task type (pypdf for structure manipulation, pdfplumber for data extraction, reportlab for creation)\n- How different PDF content requires different extraction approaches (structured tables vs flowing text vs scanned images)\n- When to use command-line tools vs Python libraries\n\n**Workflow Decisions:**\n- Table extraction requires pdfplumber, not pypdf\n- Scanned PDFs need OCR (pytesseract + pdf2image), not simple text extraction\n- Different content types in the same batch may need different handling strategies\n\n**Practitioner Struggles (without this skill):**\n- Assuming one library works for all PDF tasks (e.g., trying pypdf for table extraction)\n- Not recognizing when a PDF is scanned and requires OCR\n- Not understanding how to design workflows that handle mixed PDF content\n\n**What NOT to test:**\n- Exact API syntax or specific script names\n- Memorization of the exact tool comparison table\n- Trivia about library licenses\n\n**What TO test:**\n- Can the model recommend the right tool for a realistic multi-step task?\n- Does it understand when different PDF types need different approaches?\n- Can it design a practical workflow for consolidated data extraction?\n\n```yaml\nname: pdf-data-extraction-workflow-design\nskills:\n  - pdf\nprompt: |\n  You need to process a batch of 50 financial PDFs for data consolidation. The PDFs vary in format:\n  \n  - 30 PDFs contain structured tables showing quarterly revenue by product line (native PDF tables)\n  - 15 PDFs are scanned images of printed financial reports with table data\n  - 5 PDFs have both - some pages with native tables, some with scanned table images\n  \n  Your goal is to extract all tabular data and consolidate it into a single CSV file that can be analyzed. \n  \n  Design a workflow that handles all three PDF types appropriately. Explain:\n  1. Which Python libraries or tools you would use for each PDF type\n  2. Why you chose those specific tools\n  3. How you would handle the variation across your batch\n  4. What potential challenges you'd need to address\n  \ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of PDF processing tool selection and workflow design.\n    \n    The response should show:\n    - Recognition that different PDF types require different extraction approaches\n    - Specific mention of pdfplumber for native table extraction (not pypdf or generic text extraction)\n    - Recognition that scanned PDFs require OCR (pytesseract, pdf2image) rather than direct text extraction\n    - A coherent workflow that handles mixed content (acknowledging the 5 PDFs with both types)\n    - Understanding of the practical challenges (OCR accuracy, consolidation logic, format consistency)\n    \n    Score 1.0 if: The response correctly identifies pdfplumber for structured tables AND pytesseract/OCR for scanned content, explains why these tools are necessary, and outlines a practical workflow that accounts for mixed PDF types. Shows understanding that different PDF types need different strategies.\n    \n    Score 0.5 if: The response suggests reasonable tools but misses key distinctions (e.g., treats all PDFs as needing the same extraction method, mentions pdfplumber but doesn't explain why it's better than pypdf, or overlooks the scanned PDF handling entirely). Or: correctly identifies the tools but provides only superficial reasoning about workflow integration.\n    \n    Score 0.0 if: The response suggests inappropriate tools (e.g., only pypdf without acknowledging its limitations for tables), shows no understanding of OCR for scanned PDFs, treats all PDFs identically, or provides a workflow that wouldn't actually work (e.g., trying to extract tables from scanned images using pdfplumber directly).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable domain knowledge about PDF processing workflows rather than trivia. It tests the critical understanding that different PDF types (native tables vs scanned images) require different tools (pdfplumber vs OCR), which is genuine practitioner knowledge. The grader is robust, accepting multiple valid approaches while checking for core understanding. The scenario is highly realistic - consolidating financial data from mixed PDF formats is a common real-world task. Minor deduction: the grader could be slightly more explicit about accepting alternative OCR tools or table extraction libraries, though the current rubric is reasonably flexible with phrases like 'reasonable tools.'"
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this PDF skill document, I'll analyze what it teaches and create a test case.\n\n## Analysis\n\n1. **What domain knowledge does this skill teach?** \n   - Core PDF manipulation operations (merge, split, rotate, watermark)\n   - Text extraction techniques including OCR for scanned documents\n   - Table extraction as structured data\n   - PDF creation and generation\n   - Form handling for both fillable and non-fillable PDFs\n   - Security features like password protection\n\n2. **What workflow decisions does it enable?**\n   - Choosing between pypdf for basic operations vs pdfplumber for table extraction\n   - When to use OCR (pytesseract) for scanned vs direct text extraction\n   - Deciding between programmatic Python solutions vs command-line tools\n   - How to approach fillable vs non-fillable form fields differently\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Extracting structured table data from PDFs and converting to usable formats\n   - Handling scanned PDFs that need OCR\n   - Knowing which library/tool is optimal for specific tasks\n   - Preserving layout and structure when extracting content\n\n## Test Case\n\n```yaml\nname: pdf-table-extraction-workflow\nskills:\n  - pdf\nprompt: |\n  You have a PDF report containing quarterly financial data with multiple tables spread across several pages. Each table has headers like \"Quarter\", \"Revenue\", \"Expenses\", and \"Net Income\". \n  \n  You need to:\n  1. Extract all the tables from the PDF\n  2. Combine them into a single dataset\n  3. Export the combined data to a spreadsheet format for analysis\n  \n  Explain your approach and provide Python code to accomplish this task. Consider that some tables might span multiple pages and you'll need to preserve the structure of the data.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates proper understanding of PDF table extraction and processing.\n    \n    The response should:\n    - Identify pdfplumber as the appropriate library for table extraction (not just basic text extraction)\n    - Show how to extract tables from multiple pages\n    - Demonstrate combining/concatenating tables from different pages\n    - Include conversion to a structured format (Excel, CSV, or DataFrame)\n    - Handle potential issues like empty tables or missing data\n    \n    Score 1.0 if: The response correctly uses pdfplumber for table extraction, shows iteration through pages to get all tables, combines the data appropriately (e.g., using pandas concat or similar), and exports to a spreadsheet format. Code should be functional and handle basic edge cases.\n    \n    Score 0.5 if: The response identifies the need for specialized table extraction but has minor issues like: using a less optimal library but still achieving the goal, missing some error handling, or having minor code issues that wouldn't prevent the main task from working.\n    \n    Score 0.0 if: The response only extracts text without preserving table structure, uses inappropriate tools that can't handle tables properly, or provides code that would fail to extract and combine the table data correctly.\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This is a strong test case that evaluates transferable PDF processing knowledge. It tests the genuine skill of choosing the right library (pdfplumber) for table extraction and combining structured data - a common real-world task. The grader is robust, accepting multiple valid approaches (pandas concat, CSV, Excel) and focusing on capability rather than exact implementation. Minor deduction: the test could be slightly more challenging by including edge cases like scanned PDFs or malformed tables to better differentiate skill levels, and the grader could be more explicit about accepting alternative libraries like tabula-py that also extract tables effectively."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain-skill analysis (what this skill teaches and what the test should target)\n\n1. **Core domain knowledge**\n   - PDFs vary: interactive (AcroForm) vs non-interactive “flat” forms; scanned/image-only vs text-based.\n   - Different operations need different tools: form field inspection/filling (pypdf), visual placement/overlays (reportlab + merge), rendering to images for validation (pypdfium2/pdf2image), OCR when necessary (pytesseract).\n   - PDF coordinate system pitfalls: origin at bottom-left in PDF space; converting to image pixel coordinates requires handling scale and y-axis inversion.\n\n2. **Workflow decisions enabled**\n   - First decision: determine whether a PDF has fillable fields before attempting to “type onto” it.\n   - If fillable: extract field IDs/types/options, then fill with correct values (esp. checkboxes/radios) and regenerate appearances.\n   - If non-fillable: render pages, locate where text should go, compute bounding boxes, then overlay text/marks and validate visually.\n\n3. **What practitioners struggle with without this skill**\n   - Wasting time trying to fill non-fillable PDFs with form APIs (or vice versa).\n   - Incorrect checkbox/radio values (not using the field’s export values).\n   - Misplaced text due to coordinate system misunderstandings and lack of visual validation.\n\n```yaml\nname: pdf-form-fillable-vs-nonfillable-workflow\nskills:\n- pdf\nprompt: |\n  You are automating completion of a government application form PDF at scale.\n\n  You receive an input PDF \"application.pdf\" and a JSON payload with applicant data like:\n  - full_name\n  - address\n  - date_of_birth\n  - checkbox \"I certify this is true\" (must be checked)\n  - radio group \"preferred_contact\" with options Email / Phone\n  You do NOT know whether application.pdf is an interactive (fillable) AcroForm PDF or a flattened/non-fillable PDF.\n\n  Task:\n  1) Describe a robust step-by-step workflow that first determines whether the PDF is fillable, then follows the correct branch:\n     - If fillable: extract the set of fields (including their IDs/names and types), determine the correct values for checkbox/radio selections, and write out a filled PDF.\n     - If non-fillable: explain how you would determine where to place text/marks on the page(s), how you would convert coordinates correctly, and how you would validate placement.\n  2) Provide concise Python-oriented pseudocode or code snippets showing the key operations and libraries you would use for each branch.\n\n  Constraints:\n  - Do not assume any bespoke internal scripts exist; rely on standard Python libraries and common CLI tools.\n  - Focus on correctness and debuggability: include at least one concrete validation step to catch misaligned text/fields before shipping.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates practical PDF form-processing competence and the key workflow decisions.\n\n    Score 1.0 if the response includes ALL of the following:\n    - A clear initial decision step to detect whether the PDF has interactive/fillable form fields (e.g., AcroForm inspection / listing fields) before attempting filling.\n    - A correct \"fillable\" branch that:\n      - Extracts field identifiers (names/IDs) and recognizes field types (text vs checkbox vs radio vs choice) OR at minimum explains that types/allowed values must be discovered from the PDF.\n      - Explicitly notes that checkboxes/radio buttons require setting the correct export/on value (not just True/False) and that radio selection must match one of the allowed option values.\n      - Describes writing a new output PDF and mentions an appearance/visibility concern (e.g., NeedAppearances / generating appearances / ensuring values render).\n      - Uses appropriate tooling such as pypdf (or a comparable PDF form library) in the snippets.\n    - A correct \"non-fillable\" branch that:\n      - Uses rendering to images (pdf2image or pypdfium2 or similar) for visual inspection/validation.\n      - Mentions PDF coordinate system considerations: origin at bottom-left in PDF space and the need to convert to image pixel coordinates (including y-axis inversion and scaling).\n      - Proposes a concrete overlay technique (e.g., reportlab to create an overlay PDF merged onto original pages, or adding annotations) and includes a validation step (e.g., draw bounding boxes, produce a preview image/PDF for human review).\n    - The overall workflow is realistic and debuggable (e.g., logging fields found, failing fast on unknown field names/options, generating preview artifacts).\n\n    Score 0.5 if the response:\n    - Correctly handles fillable vs non-fillable at a high level but misses one major practical detail (such as checkbox export values, coordinate conversion specifics, or a concrete validation step), OR\n    - Provides good detail for only one branch (fillable or non-fillable) while the other is vague but not incorrect.\n\n    Score 0.0 if the response:\n    - Does not distinguish fillable vs non-fillable forms, OR\n    - Gives only generic advice without actionable PDF-specific steps/tools, OR\n    - Contains fundamentally incorrect claims (e.g., assumes all PDFs have form fields; ignores coordinate system issues; suggests editing text “in place” without overlay/annotation).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of PDF form processing workflows and tests genuine domain knowledge (fillable vs non-fillable detection, coordinate systems, validation). The grader is comprehensive and accepts multiple valid approaches. However, the test has moderate realism concerns: while the scenario is plausible, asking for a 'step-by-step workflow description' with pseudocode is somewhat academic rather than a concrete practitioner task (e.g., 'fill this specific PDF' would be more realistic). The non-obviousness is excellent (8-9/10) as it tests transferable concepts about form types and coordinate systems rather than memorizing script names. Clarity is strong (8/10) with detailed rubric criteria, though the grader could be slightly more explicit about partial credit boundaries. Realism is good but not excellent (6-7/10) - practitioners do need this workflow knowledge, but they typically implement it for specific PDFs rather than describing it abstractly."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (per skill-doc reasoning)\n# 1) Domain knowledge taught:\n#    - PDFs vary: born-digital vs scanned/image-only; encrypted vs unencrypted; rotated pages; tables vs plain text.\n#    - Appropriate tools for each job: pypdf/qpdf for structural ops (decrypt/rotate/merge/split), pdfplumber/pdftotext for text+tables,\n#      OCR pipeline (pdf2image/pypdfium2 + pytesseract) for scanned pages, pandas for tabular export, reportlab for generating PDFs if needed.\n# 2) Workflow decisions enabled:\n#    - Decide when to use direct text extraction vs OCR (e.g., extract_text() empty/None -> OCR).\n#    - Decide whether to decrypt first (qpdf/pypdf) before any extraction.\n#    - Normalize rotations before extraction to improve OCR/table detection.\n#    - Choose table extraction tooling (pdfplumber extract_tables -> DataFrame -> Excel) vs plain text extraction (pdftotext -layout).\n# 3) What practitioners struggle with without this skill (test target):\n#    - Designing a robust end-to-end pipeline that handles mixed PDFs (encrypted + rotated + scanned) rather than failing silently or producing empty output.\n#    - Picking the right libraries and ordering operations correctly (decrypt -> rotate -> extract; OCR fallback).\nname: extract-text-and-tables-from-mixed-scanned-encrypted-pdf\nskills:\n- pdf\nprompt: |\n  You’re handed a single PDF file, `invoices.pdf`, that has these real-world issues:\n\n  - It is password-protected (you are given the password).\n  - Some pages are rotated 90 degrees.\n  - Some pages are “born-digital” PDFs with selectable text and tables.\n  - Some pages are scanned images (no embedded text), but still contain tables and key fields.\n  - You need to produce two outputs:\n    1) `invoices.xlsx` containing *all extracted tables* from the entire PDF (combined across pages).\n    2) `invoices.txt` containing all readable text, using OCR only when necessary.\n\n  Task:\n  Write a concise, practical step-by-step plan PLUS representative Python and/or CLI snippets to implement this pipeline.\n  Your answer must address:\n  - How you would remove/decrypt the password (or otherwise open the PDF for processing)\n  - How you would normalize/handle rotation\n  - How you would detect which pages need OCR vs direct extraction\n  - How you would extract tables into a single Excel file\n  - How you would extract text for all pages into a single text file\n\n  Constraints:\n  - Assume you cannot manually open the PDF to inspect pages.\n  - Prefer approaches that work at scale (hundreds of pages).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates a robust, realistic PDF-processing workflow.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Correct sequencing/architecture: decrypt/open -> (optionally) normalize rotation -> per-page extraction with OCR fallback -> write consolidated outputs.\n    - Decryption handling: mentions a valid approach such as qpdf --password ... --decrypt OR pypdf decryption/opening with password (any correct method).\n    - Rotation handling: mentions rotating pages using pypdf or qpdf (or an equivalent), or rendering with rotation accounted for before OCR/table extraction.\n    - OCR fallback logic: explicitly describes detecting scanned/image-only pages (e.g., page.extract_text() empty/None/very short) and then using an OCR pipeline\n      (e.g., pdf2image or pypdfium2 rendering + pytesseract).\n    - Table extraction: uses pdfplumber (or a comparably valid approach) to extract tables, converts to DataFrames, concatenates across pages, and writes to Excel.\n    - Text extraction: uses pdfplumber and/or pdftotext for born-digital text extraction and appends OCR text for scanned pages into a single .txt.\n    - Provides representative code/commands (not just high-level descriptions) that could realistically be implemented.\n\n    Score 0.5 if the answer is mostly correct but missing ONE major element above (e.g., no rotation strategy, or no clear OCR detection logic, or no table-to-Excel consolidation),\n    OR if it is correct conceptually but too vague to implement (minimal/no snippets).\n\n    Score 0.0 if the answer is generic/non-actionable, omits OCR entirely for scanned pages, omits decryption handling, or proposes incorrect tooling/ordering that would likely fail.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case demonstrates strong understanding of PDF processing workflows and tests genuine domain knowledge about handling real-world PDF challenges (encryption, rotation, mixed content types, OCR fallback). The grader is comprehensive and accepts multiple valid approaches while checking for key architectural decisions. However, it loses points on realism - while the individual challenges are realistic, combining ALL of them (password-protected + rotated + mixed scanned/digital + table extraction) in a single task is somewhat contrived. Most practitioners would encounter these issues separately. The test is also quite complex, which may make it harder to distinguish between partial understanding and complete mastery. The non-obviousness is excellent (8/10) as it tests transferable workflow design rather than trivia, clarity is strong (9/10) with a robust grader, but realism is moderate (7/10) due to the artificial combination of every possible PDF challenge."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis\n# 1) Domain knowledge: Practical PDF automation requires combining different techniques:\n#    - Structural PDF ops (merge/split/rotate/watermark/encrypt) vs. content extraction\n#    - Text/table extraction from digitally-generated PDFs vs. OCR for scanned/image-only pages\n#    - Generating new PDFs (cover sheets) and then composing them with existing PDFs\n# 2) Workflow decisions enabled:\n#    - When to use a table-aware extractor (vs. naive text parsing)\n#    - How to detect “no text” pages and trigger OCR (render-to-image -> OCR)\n#    - Choosing the right tool boundaries: read/extract vs. write/modify, and optional CLI helpers\n# 3) Practitioner struggle without this skill:\n#    - Getting clean tables (columns/rows) instead of mangled text\n#    - Handling scanned/rotated pages (extract_text returns empty) and integrating OCR output\n#    - Producing a finalized, secure deliverable PDF with rotation fixes, watermarking, and encryption\n\nname: pdf-invoice-packet-ocr-table-watermark-encrypt\nskills:\n  - pdf\nprompt: |\n  You’re building an unattended pipeline to process thousands of “vendor packet” PDFs.\n\n  Each input PDF (e.g., `vendor_packet.pdf`) has exactly 3 pages:\n  1) Page 1 is digitally-generated (selectable text) and contains the vendor name somewhere in the text.\n  2) Page 2 is digitally-generated and contains an invoice line-items TABLE with columns:\n     `Item`, `Qty`, `Unit Price`, `Total`.\n  3) Page 3 is a scanned signature page (image-only; `extract_text()` returns empty) and it’s rotated 90° clockwise.\n\n  Produce an end-to-end approach (brief explanation + Python code skeleton/pseudocode) that:\n  A) Extracts the page-2 table into a structured dataset (DataFrame) and saves it as CSV or Excel.\n  B) Extracts all text from pages 1–3 into a single UTF-8 `.txt`, using OCR for page 3.\n     - Show how you detect that OCR is required for a page.\n  C) Creates `final.pdf` that:\n     - Starts with a generated cover page summarizing:\n       - vendor name (from page 1 text),\n       - invoice total (sum of the `Total` column from the extracted table),\n       - page count of the original packet.\n     - Then appends the original pages 1–3,\n     - Rotates page 3 so it’s upright,\n     - Applies a “CONFIDENTIAL” watermark to every page,\n     - Encrypts the output with a user password.\n\n  Constraints:\n  - Prefer pure Python libraries, but you may optionally mention a CLI tool you’d use for robustness/performance (and why).\n  - Don’t assume manual inspection; this must run headless at scale.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates correct, practitioner-grade PDF automation decisions and a coherent pipeline.\n\n    Score 1.0 if the answer includes ALL of the following (tool names can vary; equivalents are fine):\n    - An end-to-end plan that explicitly covers A, B, and C (table extraction, full-text w/ OCR, and final PDF composition with cover+append+rotate+watermark+encrypt).\n    - Table extraction is done with a table-aware method/library (e.g., pdfplumber extract_tables/extract_table, Camelot, Tabula, etc.) producing structured rows/columns (e.g., DataFrame) rather than “extract all text then regex/parse columns”.\n    - OCR handling is correct: the answer explains that scanned/image-only PDFs require rendering pages to images (e.g., pdf2image or a renderer like PDFium) and then running OCR (e.g., Tesseract). It also includes a reasonable OCR-trigger heuristic (e.g., extracted text is empty/None/very short, or page has no text objects).\n    - PDF writing/modification is handled with an appropriate tool (e.g., pypdf/qpdf/pdftk/pdf-lib/reportlab):\n      - generates a new cover page (e.g., reportlab/pdf-lib),\n      - appends/merges with the original,\n      - rotates ONLY page 3 (or the correct target page),\n      - watermarks all pages (overlay/merge watermark page),\n      - encrypts the final output.\n    - Provides a plausible code skeleton/pseudocode showing key steps/APIs (not necessarily perfectly runnable, but concrete enough to implement).\n    - Mentions at least one scale/robustness consideration (batching, error handling, timeouts, OCR performance, dealing with malformed PDFs, optional CLI for decrypt/rotate/text extraction, etc.).\n\n    Score 0.5 if the answer addresses the overall task but misses ONE major required component or is notably hand-wavy in one critical area, for example:\n    - Mentions OCR but not render-to-image (or no OCR trigger logic),\n    - Extracts tables via naive text parsing (but still addresses OCR and final PDF steps),\n    - Builds a final PDF but omits either watermarking or encryption or rotation correction,\n    - Generates a cover page conceptually but provides no credible method/library.\n\n    Score 0.0 if the answer fails to demonstrate the key domain decisions, such as:\n    - No OCR path for scanned pages, OR\n    - No table-aware extraction (only plain text extraction), OR\n    - No coherent method to create/modify the final PDF (cover+append+rotate+watermark+encrypt).\n\n    Be flexible about specific library names/commands as long as the approach is correct and complete.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong domain knowledge by requiring practitioners to combine multiple PDF techniques (table extraction, OCR detection, PDF composition, watermarking, encryption) in a realistic workflow. The grader is robust and accepts multiple valid approaches while focusing on correct conceptual decisions rather than specific library names. However, the task is somewhat contrived (exactly 3 pages with specific characteristics) and could be more realistic by allowing variable page counts or mixed content types. The non-obviousness is high (8/10) as it tests transferable patterns like OCR triggers and table-aware extraction. Clarity is strong (9/10) with a well-structured rubric that accepts equivalent solutions. Realism is good but not perfect (7/10) - while invoice processing is real, the exact structure feels slightly artificial for testing purposes."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "The pdf skill covers general PDF manipulation: using Python libraries (pypdf for merging/splitting/metadata/rotation, pdfplumber for text & tables, reportlab for creation), command-line tools (pdftotext, qpdf, pdftk, pdfimages), OCR workflows (pdf2image + pytesseract for scanned docs), watermarking, password protection, image extraction, and form-handling procedures. Key workflow decisions include choosing pypdf for structural operations vs pdfplumber for content extraction, when to rely on command-line utilities for quick tasks, selecting OCR pipelines for scanned PDFs instead of attempting direct text extraction, and following strict multi-step processes for fillable vs non-fillable forms. Practitioners often struggle with identifying the right tool sequence—especially converting scanned PDFs to analyzable text/tables—and ensuring outputs (like Excel files) retain structure.\n\n```yaml\nname: ocr-pipeline-for-scanned-pdfs\nskills:\n  - pdf\nprompt: |\n  You receive a 12-page scanned PDF of warehouse inventory sheets. The text is not selectable, but each page has a consistent table with columns “Asset ID”, “Location”, and “Qty”. Describe a practical Python-based workflow to capture every row from the PDF into a single Excel file. Include:\n  - The tooling sequence you’d use (mention concrete libraries/utilities).\n  - How you’d convert the PDF pages for OCR, perform the OCR, and turn recognized content into structured tables.\n  - A representative code outline that shows the critical steps, including exporting to Excel.\n  - Any validation or cleanup steps you’d add to maintain accuracy across pages.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the answer:\n    - Explicitly treats the PDF as scanned and describes converting pages to images (e.g., with pdf2image) before OCR.\n    - Specifies OCR (pytesseract or equivalent) as a required step and integrates it into a pipeline.\n    - Provides a coherent code outline or pseudocode covering conversion, OCR, table parsing, and export to Excel (e.g., via pandas).\n    - Mentions validation or sanity checks (such as verifying column alignment or page order).\n\n    Score 0.5 if the answer hits some of the pipeline elements but is missing a critical step (e.g., mentions OCR but not image conversion, or exports text without structuring into tables/Excel), or the code outline is too vague to implement.\n\n    Score 0.0 if the answer ignores the scanned/OCR requirement, proposes irrelevant tooling, or fails to outline any workable workflow.\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong non-obviousness (9/10) by testing the transferable OCR pipeline pattern rather than trivia, excellent clarity (9/10) with a robust grader accepting multiple valid approaches, and good realism (8/10) as warehouse inventory digitization is a common real-world task. The grader appropriately focuses on conceptual understanding (image conversion → OCR → structured extraction → Excel export) rather than exact library names. Minor deduction: the prompt could be slightly more concise, and the grader could explicitly acknowledge alternative OCR tools beyond pytesseract."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\nThe pdf skill teaches practical knowledge about processing PDFs at scale, including how to choose tools for extracting textual content or tables (especially distinguishing between pypdf for structural operations and pdfplumber for content extraction), how to manipulate PDFs programmatically (merge, split, rotate, add watermarks, encrypt), and how to handle scanned documents using OCR before text extraction. The workflow decisions it enables involve selecting the right tool for the problem (e.g., pdfplumber for structured text/tables vs pypdf for page-level manipulations or watermarking, and invoking OCR when dealing with scanned content) and sequencing operations properly (e.g., rotate pages before watermarking, combine page extractions into Excel). Without this skill, a practitioner would struggle to design a coherent pipeline that simultaneously transforms the document, extracts both narrative and tabular data, and secures the result—especially when deciding when to OCR, how to aggregate tables, and how to reuse plane manipulations (rotate, watermark, encrypt) using the appropriate libraries.\n\n```yaml\nname: pdf-pipeline-text-table-rotation-watermark\nskills:\n  - pdf\nprompt: |\n  You are tasked with processing a 12-page client report that contains prose explanations on every page and two tables per page. The first page happens to be scanned sideways, and the client also wants the final PDF watermarked and password-protected. Your deliverables are:\n  1. A cleaned-up PDF where the first page is rotated upright, every page carries the same watermark, and the entire document is encrypted with a user password.\n  2. An Excel file that consolidates every table (two per page) from the report into a single sheet.\n  3. A short explanation of how you handled the fact that the first page is a scanned image and how that influenced your extraction pipeline.\n\n  Describe in detail which libraries or tools you would use for each step, why they are appropriate for that part of the workflow, and the sequence in which you would run them to produce all deliverables. Mention any OCR step, table extraction technique, and security/watermark approach necessary to succeed.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria explaining what a correct answer looks like.\n\n    Score 1.0 if: The response clearly describes a sequential workflow that (a) handles the scanned first page via OCR before extraction, (b) uses a text/table extraction tool suited for structured data (e.g., pdfplumber) to gather all tables into pandas/Excel, (c) applies pypdf (or equivalent) to rotate the first page, add a watermark to all pages, and encrypt the final PDF, and (d) reasons about choosing each library/tool versus alternatives.\n    Score 0.5 if: The response covers most steps but omits or glosses over one major requirement (e.g., forgets OCR for the scanned page, watermarks but does not encrypt, or fails to describe how tables are aggregated) or mixes up tool choices in a way that would break the pipeline.\n    Score 0.0 if: The response fails to outline a coherent workflow addressing the scanned page, table extraction, and PDF post-processing (rotation, watermark, encryption), or uses inappropriate tools that would not work for the described tasks.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong realism (a multi-step PDF processing pipeline is genuinely practical) and tests transferable knowledge about tool selection and workflow sequencing rather than trivia. However, clarity suffers significantly: the grader uses letta_judge without providing concrete evaluation criteria, making it unclear how partial credit would be assessed. The prompt also creates potential brittleness by specifying exact numbers (12 pages, 2 tables per page, first page scanned) that aren't essential to testing the underlying concepts, and the grader doesn't specify whether it would accept valid alternative approaches (e.g., using qpdf instead of pypdf for rotation, or different OCR libraries). The test would benefit from more robust grading criteria that focus on conceptual understanding rather than implementation details."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: pdf-tool-selection-and-extraction\nskills:\n  - pdf\nprompt: |\n  I am writing a Python script to process several PDF documents and need to choose the most appropriate libraries for specific tasks. \n\n  Please recommend the best library from your toolkit for each of the following scenarios and briefly explain why:\n  1. I need to extract a complex pricing table from a PDF invoice and convert it to a DataFrame. Preserving the row/column layout is critical.\n  2. I need to merge five different PDF reports into a single file.\n  3. I need to generate a brand new PDF certificate from scratch, drawing text and lines at specific coordinates.\n\n  Finally, write a short Python code snippet for task #1 (extracting the table) using the library you recommended.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the model's library recommendations and code based on the provided PDF skill documentation.\n\n    Score 1.0 if:\n    - Recommends `pdfplumber` for task #1 (table extraction).\n    - Recommends `pypdf` (or `pypdf2`) for task #2 (merging).\n    - Recommends `reportlab` for task #3 (generation/creation).\n    - The code snippet uses `pdfplumber`, opens the file, and uses the `.extract_tables()` method (or `.extract_table()`).\n\n    Score 0.5 if:\n    - Recommends the correct libraries but the code snippet uses the wrong method (e.g., trying to extract tables with pypdf).\n    - Recommends generic/incorrect libraries for one of the tasks (e.g., using reportlab to merge).\n\n    Score 0.0 if:\n    - Fails to recommend `pdfplumber` for table extraction (e.g., suggests using pypdf for complex tables).\n    - Code snippet is completely unrelated or hallucinated.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test measures transferable knowledge about choosing appropriate PDF libraries for different tasks (table extraction, merging, creation), which is practical domain knowledge rather than trivia. However, the grader has some brittleness: it only accepts pypdf/pypdf2 for merging when qpdf is also mentioned as a valid tool in the skill document, and it doesn't account for valid alternative approaches like using pandas.DataFrame() to convert extracted tables. The task is realistic as practitioners frequently need to select appropriate tools for different PDF operations."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis of the Skill\n\n1.  **What domain knowledge does this skill teach?**\n    The skill provides a specialized toolkit for PDF manipulation. Beyond just file formats, it teaches the distinction between **programmatic PDF generation** (using canvases vs. flowables in ReportLab), **structural extraction** (distinguishing between raw text extraction and coordinate-based table extraction via `pdfplumber`), and **graphical manipulation** (merging, rotating, and watermarking pages). It also covers the critical distinction between **digitally created PDFs** (searchable) and **scanned PDFs** (requiring OCR workflows).\n\n2.  **What workflow decisions does it enable?**\n    The skill helps an LLM decide which tool is right for a specific outcome:\n    *   Use `pypdf` for file-level operations (merge, split, rotate, metadata).\n    *   Use `pdfplumber` when the data is structured in tables or specific layouts.\n    *   Use `reportlab` for generating new documents from data.\n    *   Use `pytesseract` + `pdf2image` when `extract_text()` returns nothing (scanned documents).\n    *   Use `qpdf` or `pdftk` for high-performance command-line batch processing or decryption.\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner would likely struggle with **table extraction** (often resulting in \"word soup\" if using standard text extractors) and **complex page manipulation** like watermarking (which requires merging page objects rather than just appending them). They might also fail to recognize when a PDF is a scan, leading to failed extraction attempts.\n\n---\n\n### Test Case\n\n```yaml\nname: pdf-structured-table-extraction\nskills:\n- pdf\nprompt: |\n  I have a 10-page PDF document named 'monthly_inventory.pdf'. Each page contains a table with inventory data. \n  The tables have a specific structure: \n  - The first row (index 0) is a merge-cell title for the category (e.g., \"ELECTRONICS DEPARTMENT\").\n  - The second row (index 1) contains the actual column headers: \"SKU\", \"Item_Name\", \"Quantity\", and \"Unit_Price\".\n  \n  I need a Python script that uses the most appropriate library from your toolkit to:\n  1. Extract all tables from all pages.\n  2. Combine them into a single pandas DataFrame.\n  3. Ensure the DataFrame uses the headers from the second row of the tables and ignores the category title row.\n  4. Save the final combined data to 'inventory_summary.csv'.\n  \n  Please provide the complete script.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    Score 1.0 if:\n    - The model correctly identifies and uses `pdfplumber` as the library for table extraction (as it is the recommended tool for structured data in the skill).\n    - The code iterates through all pages of the PDF.\n    - The code correctly handles the row indexing: it uses the second row (`table[1]`) as columns and skips the first two rows for the data (`table[2:]`), or logically filters them.\n    - The code uses `pandas` to concatenate the results and save to 'inventory_summary.csv'.\n\n    Score 0.5 if:\n    - The model uses `pdfplumber` but fails the specific row-indexing logic (e.g., it uses the default `table[0]` for headers).\n    - OR the model provides a valid `pypdf` approach (though `pdfplumber` is significantly better for this specific task, `pypdf` is technically part of the toolkit but much harder to use for tables).\n\n    Score 0.0 if:\n    - The model suggests a library not mentioned in the skill or one that cannot perform table extraction.\n    - The code is logically broken or does not address the multi-page requirement.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of PDF table extraction workflows and tests genuine domain knowledge (choosing pdfplumber for structured data, handling multi-row headers). However, the grader has moderate brittleness: it heavily penalizes using pypdf (scoring 0.5) even though the skill document doesn't explicitly state pdfplumber is superior for tables, and the specific row-indexing requirement (skipping merge-cell titles) is somewhat contrived rather than a common real-world pattern. The test is realistic overall but the grading criteria could be more flexible about valid alternative approaches."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\nI'll analyze the PDF skill document and create a test case that measures whether an LLM has mastered the domain knowledge for PDF processing workflows.\n\n## Analysis\n\n**What domain knowledge does this skill teach?**\n- PDF manipulation operations (merge, split, extract, create)\n- Library selection decision-making (pypdf for basic operations, pdfplumber for text/tables, reportlab for creation)\n- Command-line vs programmatic approaches\n- Fillable vs non-fillable form handling workflows\n- OCR for scanned PDFs\n- Password protection and encryption\n\n**What workflow decisions does it enable?**\n- Choosing between libraries based on the task (pypdf vs pdfplumber vs reportlab)\n- Determining if command-line tools are better than Python libraries\n- Identifying when OCR is needed vs direct text extraction\n- Differentiating workflows for fillable vs non-fillable forms\n- Selecting appropriate techniques for different PDF types (native text vs scanned)\n\n**What would a practitioner actually struggle with without this skill?**\n- Choosing the right library for a specific PDF task\n- Knowing when to use OCR vs direct extraction\n- Understanding the workflow for form filling (the complex process in forms.md)\n- Efficiently handling batch PDF operations\n- Knowing which approach (programmatic vs CLI) is better for scale\n\n## Test Case\n\n```yaml\nname: pdf-workflow-decision-making\nskills:\n- pdf\nprompt: |\n  You are working on a document processing pipeline and need to handle various PDF processing tasks. For each of the following scenarios, recommend the most appropriate approach, library, and method to accomplish the task efficiently. your recommendations should be specific enough to implement.\n\n  SCENARIOS:\n\n  1. You have 500 invoices in PDF format that are mostly native text-based documents. You need to extract tabular data (invoice items, quantities, prices) from all of them and export to Excel for analysis.\n\n  2. You need to create a standardized report PDF from scratch that includes a header, multiple pages of content, and a footer on each page. This will be generated daily as part of an automated process.\n\n  3. You have a batch of 100 scanned PDF receipts from various sources that need to be processed through OCR to extract vendor names, dates, and amounts.\n\n  4. You need to merge every pair of PDFs in a directory (file1+file2, file3+file4, etc.) without loading all files into memory at once, as there are thousands of files.\n\n  5. You have a government form that needs to be filled programmatically with user data. Some fields are fillable form fields, but others are just blank areas where text needs to be placed.\n\n  For each scenario, provide:\n  - Primary library/tool recommendation\n  - Specific approach/method\n  - Key code pattern or command structure\n  - Any important considerations or alternatives\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    Score 1.0 if:\n    - Correctly identifies pdfplumber for tabular data extraction in scenario 1 (not just pypdf)\n    - Recommends reportlab for PDF creation in scenario 2 (not pypdf which is mainly for manipulation)\n    - Specifies OCR workflow with pytesseract/pdf2image for scenario 3\n    - Suggests streaming/memory-efficient approach for scenario 4 (could be qpdf CLI or pypdf with proper batching)\n    - Correctly identifies that scenario 5 requires the forms.md workflow approach (checking fillable vs non-fillable)\n    - Provides specific, implementable guidance for each scenario\n\n    Score 0.5 if:\n    - Gets 3-4 of the library/approach choices correct\n    - Provides generally correct approaches but lacks specific implementation details\n    - Shows understanding of the concepts but makes some suboptimal choices\n\n    Score 0.0 if:\n    - Suggests wrong libraries for most scenarios (e.g., using pypdf for tabular extraction or PDF creation)\n    - Shows no understanding of OCR requirements for scanned PDFs\n    - Recommends impractical approaches (like loading thousands of PDFs into memory simultaneously)\n    - Demonstrates lack of understanding of the fillable vs non-fillable form distinction\n\n    The response should demonstrate practical knowledge of PDF processing workflows, not just theoretical understanding of the libraries.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of PDF processing workflows and tests transferable domain knowledge (library selection, OCR vs direct extraction, memory efficiency). However, it has clarity issues: the grader is somewhat brittle by requiring specific library names (e.g., 'pdfplumber' vs other valid table extraction tools, 'reportlab' vs other PDF creation libraries like fpdf2), and scenario 5's grading conflates two distinct workflows (fillable fields vs non-fillable) that both require the forms.md approach. The test is realistic and measures genuine practitioner decision-making, but the grader could reject valid alternative solutions."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "```yaml\nname: extract-multiple-pdf-tables\nskills:\n  - pdf\nprompt: |\n  You need to extract sales data from quarterly reports. A company has provided you with a PDF containing their Q1 2024 sales report that has multiple tables spread across 3 pages:\n\n  Page 1: Regional sales summary table with headers [Region, Q1 Sales, Growth %]\n  Page 2: Product category breakdown table with headers [Category, Units Sold, Revenue]  \n  Page 3: Top performers table with headers [Sales Rep, Territory, Total Sales]\n\n  You need to:\n  1. Extract all three tables from the PDF\n  2. Structure them as clean tabular data\n  3. Combine them into a single output file for analysis\n  4. Show the code you would use to accomplish this\n\n  Write Python code that extracts these tables and saves them to an Excel file called \"q1_sales_analysis.xlsx\" with each table on a separate sheet.\n\n  Assume the PDF is located at \"q1_sales_report.pdf\".\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates proper knowledge of PDF table extraction using the pdfplumber library.\n\n    Score 1.0 if the response:\n    - Correctly uses pdfplumber for table extraction (not basic text extraction)\n    - Demonstrates understanding of the extract_tables() method\n    - Shows how to handle multiple tables across multiple pages\n    - Properly structures data using pandas DataFrames\n    - Creates output to Excel format with multiple sheets\n    - Includes proper file handling and error checking\n\n    Score 0.5 if the response:\n    - Uses pdfplumber but has minor errors in table extraction logic\n    - Correctly extracts tables but has issues with data structuring\n    - Missing some of the Excel output formatting\n\n    Score 0.0 if the response:\n    - Uses only basic text extraction without recognizing table structure\n    - Fails to use pdfplumber library entirely\n    - Shows fundamental misunderstanding of PDF table extraction workflow\n    - Does not attempt to structure the extracted data\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good domain knowledge of PDF table extraction using pdfplumber, which is a realistic and transferable skill. The grader is mostly robust and accepts the correct approach. However, the test loses points on clarity because it only explicitly accepts pdfplumber when other valid libraries (like tabula-py or camelot) could also extract tables from PDFs. The rubric specifically warns against 'only accepts one approach when alternatives exist.' The task is realistic (extracting sales data from quarterly reports is a common business need), scoring well on realism. Non-obviousness is strong as it tests understanding of table extraction patterns rather than memorizing specific script names."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: extract-tables-from-pdf\nskills:\n  - pdf\nprompt: |\n  You have been provided with a PDF document. Your task is to extract all tables from the document and save them into a single Excel file. The document may contain multiple pages with tables of varying structures. Ensure that the extracted tables are correctly formatted and each table is saved as a separate sheet in the Excel file.\n\n  Use the `pdfplumber` library to perform the extraction. If the tables are not clearly separated, you may need to adjust the extraction parameters to ensure accuracy.\n\n  Provide the Python code to accomplish this task.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the provided Python code based on the following criteria:\n\n    Score 1.0 if:\n    - The code successfully extracts all tables from the PDF document.\n    - Each table is saved as a separate sheet in an Excel file.\n    - The tables are correctly formatted and the data is accurate.\n    - The code uses the `pdfplumber` library to extract the tables.\n\n    Score 0.5 if:\n    - The code extracts some tables but not all.\n    - The tables are saved, but formatting or data accuracy issues exist.\n\n    Score 0.0 if:\n    - The code does not extract any tables.\n    - The code does not use the `pdfplumber` library.\n    - The code is incomplete or contains significant errors.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates realistic PDF table extraction (realism: 8/10) and tests transferable knowledge about table extraction patterns (non-obviousness: 7/10). However, the grader has significant clarity issues (4/10): it requires the specific `pdfplumber` library when alternatives like `tabula-py`, `camelot`, or even `pypdf` with custom parsing could validly solve the task. The grader also lacks specificity about what constitutes 'correctly formatted' tables and doesn't account for edge cases like empty tables or different valid approaches to organizing multi-table outputs. The requirement to provide 'Python code' rather than actually executing the task also reduces realism slightly."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: pdf-form-filling\nskills:\n- pdf\nprompt: |\n  Fill out the fields in the attached PDF form with the provided information.\n\n  Field 1: First name - John\n  Field 2: Last name - Doe\n  Field 3: Email - johndoe@example.com\n  Field 4: Checkbox - Check the box if you are 18 or older\n  Field 5: Radio button - Select the first option\n\n  Please return the completed PDF form.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the model correctly fills out all fields in the PDF form and returns the completed form.\n    Score 0.5 if the model fills out most fields correctly but makes minor mistakes or omits one field.\n    Score 0.0 if the model fails to fill out the form or returns an incomplete/incorrect form.\n  extractor: last_assistant\n```",
          "score": 0.15,
          "rationale": "This test case has critical flaws across all dimensions. **Non-obviousness (2/10)**: The test requires knowing the exact script names and workflow from forms.md (check_fillable_fields, extract_form_field_info.py, fill_fillable_fields.py), making it pure skill-specific trivia rather than transferable knowledge. **Clarity (1/10)**: The grader is fundamentally broken - it references 'the attached PDF form' but no PDF is provided in the test setup, making the test impossible to execute. The grader also cannot verify actual PDF field filling without programmatic validation. **Realism (3/10)**: While PDF form filling is a real task, the test is contrived because it doesn't provide the actual PDF file needed, and a real practitioner would have the actual form to work with. The test appears designed solely to check if the model memorized the specific scripts in forms.md rather than testing genuine PDF manipulation capability."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-HTTP Caching & Validation",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 10,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** HTTP Caching & Validation\n\n**SKILL.md Content:**\n```markdown\n---\nname: HTTP Caching & Validation\ndescription: Design and debug HTTP caching behavior using Cache-Control, ETag/If-None-Match, Last-Modified, CDN caches, and safe invalidation patterns.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# HTTP Caching & Validation\n\n## Overview\n\nCaching is a correctness feature, not just performance. The same endpoint can behave differently depending on:\n- Response headers (`Cache-Control`, `ETag`, `Vary`, `Set-Cookie`)\n- Client and CDN cache behavior\n- URL normalization and query params\n\n## Core Concepts\n\n- **Freshness**: `Cache-Control: max-age=...` controls how long a cached response is considered fresh.\n- **Validation**: `ETag` + `If-None-Match` enables revalidation (304 Not Modified).\n- **Shared vs private**: `public` vs `private` determines whether CDNs/browsers may share cached responses.\n- **Vary**: describes which request headers affect the response (e.g., `Vary: Accept-Encoding, Authorization`).\n\n## Workflow: Debug a “Stale Content” Bug\n\n1. Confirm the cache layer: browser cache, CDN, reverse proxy, application.\n2. Capture request/response headers (including `Age`, `Via`, `X-Cache` if present).\n3. Check cache-busting inputs:\n   - URL path and query params\n   - `Vary` headers\n   - Authorization/cookies (often imply `private` caching)\n4. Verify correctness headers:\n   - For user-specific content: `Cache-Control: private, no-store` (often safest)\n   - For static assets: long `max-age` + immutable filenames\n5. Add validation:\n   - Provide `ETag` or `Last-Modified` for resources that change.\n6. Invalidation strategy:\n   - Prefer **versioned URLs** for static assets.\n   - Avoid purges as the only mechanism (slow and error-prone).\n\n## Common Pitfalls\n\n- Caching responses that include `Set-Cookie` unintentionally.\n- Missing `Vary: Authorization` on endpoints that return user-specific content.\n- Using long `max-age` on mutable URLs (no versioning) → hard-to-debug staleness.\n- Assuming 304 means “no work happened” (it still requires revalidation request).\n\n## Checklist\n\n- [ ] Is content user-specific? If yes, ensure private/no-store.\n- [ ] Are assets versioned (hash in filename) if using long max-age?\n- [ ] Is `Vary` correct for headers that change response?\n- [ ] Do you provide `ETag`/`Last-Modified` for mutable resources?\n- [ ] Can you reproduce with CDN bypass and with empty browser cache?\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - HTTP Caching & Validation\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** HTTP Caching & Validation\n\n**SKILL.md Content:**\n```markdown\n---\nname: HTTP Caching & Validation\ndescription: Design and debug HTTP caching behavior using Cache-Control, ETag/If-None-Match, Last-Modified, CDN caches, and safe invalidation patterns.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# HTTP Caching & Validation\n\n## Overview\n\nCaching is a correctness feature, not just performance. The same endpoint can behave differently depending on:\n- Response headers (`Cache-Control`, `ETag`, `Vary`, `Set-Cookie`)\n- Client and CDN cache behavior\n- URL normalization and query params\n\n## Core Concepts\n\n- **Freshness**: `Cache-Control: max-age=...` controls how long a cached response is considered fresh.\n- **Validation**: `ETag` + `If-None-Match` enables revalidation (304 Not Modified).\n- **Shared vs private**: `public` vs `private` determines whether CDNs/browsers may share cached responses.\n- **Vary**: describes which request headers affect the response (e.g., `Vary: Accept-Encoding, Authorization`).\n\n## Workflow: Debug a “Stale Content” Bug\n\n1. Confirm the cache layer: browser cache, CDN, reverse proxy, application.\n2. Capture request/response headers (including `Age`, `Via`, `X-Cache` if present).\n3. Check cache-busting inputs:\n   - URL path and query params\n   - `Vary` headers\n   - Authorization/cookies (often imply `private` caching)\n4. Verify correctness headers:\n   - For user-specific content: `Cache-Control: private, no-store` (often safest)\n   - For static assets: long `max-age` + immutable filenames\n5. Add validation:\n   - Provide `ETag` or `Last-Modified` for resources that change.\n6. Invalidation strategy:\n   - Prefer **versioned URLs** for static assets.\n   - Avoid purges as the only mechanism (slow and error-prone).\n\n## Common Pitfalls\n\n- Caching responses that include `Set-Cookie` unintentionally.\n- Missing `Vary: Authorization` on endpoints that return user-specific content.\n- Using long `max-age` on mutable URLs (no versioning) → hard-to-debug staleness.\n- Assuming 304 means “no work happened” (it still requires revalidation request).\n\n## Checklist\n\n- [ ] Is content user-specific? If yes, ensure private/no-store.\n- [ ] Are assets versioned (hash in filename) if using long max-age?\n- [ ] Is `Vary` correct for headers that change response?\n- [ ] Do you provide `ETag`/`Last-Modified` for mutable resources?\n- [ ] Can you reproduce with CDN bypass and with empty browser cache?\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - HTTP Caching & Validation\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "HTTP Caching & Validation",
          "skill_dir": "http-caching",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain knowledge this skill teaches:\n- How PPTX files are structured (OOXML ZIP of XML + media) and what that implies for reading vs editing.\n- Practical constraints of HTML→PPTX conversion: which HTML elements actually become text, how shapes/backgrounds are represented, and what visual features do *not* translate (e.g., CSS gradients).\n- Presentation-design hygiene that affects successful renders: web-safe fonts, bullet/list handling, placeholders for later chart/table insertion, and validating output visually.\n\nWorkflow decisions it enables:\n- Choosing the right workflow for the job (text extraction vs raw XML editing vs HTML→PPTX generation).\n- When to rasterize visuals (gradients/icons) into images rather than relying on CSS features.\n- How to structure HTML so PptxGenJS/html2pptx can reliably capture text and layout.\n\nWhat practitioners struggle with without this skill (test target):\n- “My slide converted but text is missing / bullets look wrong / gradient didn’t show up.” Diagnosing that this is due to unsupported tags/CSS and fixing the HTML (not superficial CSS tweaks) is the key capability.\n\n```yaml\nname: html2pptx-missing-text-and-gradient-debug\nskills:\n- pptx\nprompt: |\n  You're helping a teammate who is generating PPTX slides from HTML using an HTML→PPTX workflow (PptxGenJS-based). They report:\n\n  - The PPTX renders, but the slide title and subtitle are missing.\n  - The bullet list appears as a single paragraph with literal \"•\" characters, and alignment looks off.\n  - A nice diagonal background gradient doesn't appear (it becomes a flat fill).\n  - They used the font \"Inter\" and the spacing changed a lot between machines.\n\n  Here is the HTML they used for slide 1:\n\n  ```html\n  <html>\n    <body style=\"width: 720pt; height: 405pt; font-family: Inter;\">\n      <div style=\"position:absolute; inset:0; background: linear-gradient(135deg,#111827,#2563EB);\"></div>\n\n      <div style=\"position:absolute; left:48pt; top:42pt; color:#FFFFFF;\">\n        <span style=\"font-size:44pt; font-weight:700;\">Q4 Pipeline Health</span><br/>\n        <span style=\"font-size:18pt; opacity:0.9;\">What changed since last month</span>\n      </div>\n\n      <div style=\"position:absolute; left:52pt; top:150pt; width:340pt; color:#E5E7EB; font-size:18pt;\">\n        • Coverage improved to 3.2×<br/>\n        • 6 deals slipped to Q1<br/>\n        • Top risk: procurement delays\n      </div>\n\n      <div style=\"position:absolute; right:52pt; top:150pt; width:240pt; height:160pt; background:#0B1220; border-radius:18pt; padding:18pt; color:white;\">\n        <div style=\"font-size:14pt; opacity:0.8;\">KPI</div>\n        <div style=\"font-size:40pt; font-weight:700;\">3.2×</div>\n        <div style=\"font-size:14pt; opacity:0.8;\">Coverage</div>\n      </div>\n    </body>\n  </html>\n  ```\n\n  Task:\n  1) Diagnose the root causes of each issue (missing title/subtitle, bullets, gradient, font differences) in the context of HTML→PPTX conversion.\n  2) Provide a corrected HTML version that is likely to render properly in PowerPoint, keeping the same visual intent (dark background, title/subtitle, 3 bullets, KPI card).\n     - Use only web-safe fonts.\n     - Do NOT use CSS gradients directly; keep a gradient look via a referenced image.\n     - Ensure all text appears reliably in PPTX.\n     - Use proper list tags for bullets (no manual bullet characters).\n  3) Briefly describe (no code required) how you would produce and include the gradient background asset.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates the practical HTML→PPTX knowledge needed to fix real conversion failures.\n\n    Score 1.0 if the answer:\n    - Correctly diagnoses that text is missing because it is not inside supported text tags (must place text inside <p> or <h1>-<h6> or <ul>/<ol>, not bare <div>/<span> text or <br>-driven layout), and proposes a fix accordingly.\n    - Correctly diagnoses bullet issues as caused by manual bullet characters and <br> instead of semantic lists, and rewrites using <ul>/<li> (or <ol>) with no \"•\" characters.\n    - Correctly diagnoses gradient loss as due to unsupported CSS gradients in the conversion, and proposes rasterizing the gradient to an image (PNG/JPG) and referencing it with <img> or background image on a <div>.\n    - Correctly diagnoses font inconsistencies as due to non-web-safe fonts not being universally available, and switches to a web-safe font (e.g., Arial/Helvetica/Verdana/etc.).\n    - Provides a corrected HTML snippet with explicit slide dimensions (720pt × 405pt), includes title/subtitle in <h1>/<p> (or similar supported tags), bullets in <ul>/<li>, and uses a <div> for shapes/backgrounds and an <img> (or equivalent) for the gradient asset.\n\n    Score 0.5 if the answer:\n    - Identifies at least two of the four root causes correctly (text-tag issue, bullets, gradient rasterization, web-safe fonts) and provides a partially corrected HTML snippet, but misses one key constraint (e.g., still uses manual bullets or still relies on CSS gradients), OR provides correct diagnoses but no concrete corrected HTML.\n\n    Score 0.0 if the answer:\n    - Fails to identify the supported-text-tag constraint (the main cause of missing text), or suggests irrelevant fixes (e.g., \"increase z-index\" only), or keeps gradients as CSS gradients, or does not provide any plausible corrected HTML.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of HTML→PPTX conversion gotchas (missing text due to unsupported tags, manual bullets vs semantic lists, CSS gradients not converting, non-web-safe fonts). The grader is robust and accepts multiple valid approaches while focusing on core concepts rather than trivia. However, the test is somewhat contrived—while debugging conversion issues is realistic, the specific combination of all four problems in one HTML snippet feels slightly artificial. The scenario could be more natural if framed as 'fix this broken slide' rather than 'diagnose these four specific issues.' Still, it effectively tests transferable domain knowledge about PPTX conversion constraints."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis:\n# 1) Domain knowledge: Constraints and best practices for creating PPTX via an HTML→PPTX conversion workflow:\n#    - Many converters only preserve text when it's inside semantic text tags (<p>, <h1>-<h6>, <ul>/<ol>/<li>); text in <div>/<span> may be dropped.\n#    - CSS gradients and inline SVG/icons often do not translate; they must be rasterized (e.g., PNG) and inserted as images.\n#    - Fonts must be web-safe to avoid rendering inconsistencies.\n#    - Charts/tables are typically inserted after conversion by reserving placeholder regions and using their x/y/w/h in a PPTX API.\n# 2) Workflow decisions: How to debug a broken conversion and restructure HTML/layout to match converter capabilities.\n# 3) Practitioner struggle: “My PPTX renders with missing text/incorrect bullets/flattened visuals” due to unsupported tags, gradients/SVG, manual bullets, and missing placeholder conventions.\n\nname: pptx-html2pptx-debug-missing-text-and-gradient\nskills:\n- pptx\nprompt: |\n  You're helping a teammate who generates PowerPoint slides using an HTML→PPTX conversion pipeline (HTML is rendered, then converted to PPTX; later a script inserts charts programmatically).\n\n  They report this symptom: when the slide is converted, **the gray chart box appears but almost all text is missing**, and the **gradient background becomes a flat color**. Also, the bullet list looks wrong.\n\n  Here is their current HTML:\n\n  ```html\n  <!doctype html>\n  <html>\n    <head>\n      <meta charset=\"utf-8\" />\n      <style>\n        body {\n          width: 720pt;\n          height: 405pt;\n          margin: 0;\n          font-family: \"Inter\", sans-serif;\n          color: #ffffff;\n          background: linear-gradient(135deg, #1E3C72, #2A5298);\n        }\n        .wrap { margin: 44pt; }\n        .title { font-size: 44pt; font-weight: 700; }\n        .bullets { margin-top: 18pt; font-size: 18pt; line-height: 1.4; }\n        .chartBox { margin-top: 18pt; width: 420pt; height: 220pt; background: #d9d9d9; border-radius: 14pt; }\n      </style>\n    </head>\n    <body>\n      <div class=\"wrap\">\n        <div class=\"title\">Q4 Results</div>\n\n        <div class=\"bullets\">\n          • Revenue up 18%<br/>\n          • CAC down 9%<br/>\n          • NRR at 122%\n        </div>\n\n        <span style=\"font-weight:700;color:#FFD166;\">Key takeaway:</span>\n        <span>Efficient growth is back.</span>\n\n        <div id=\"chartArea\" class=\"chartBox\"></div>\n      </div>\n    </body>\n  </html>\n  ```\n\n  Task:\n  1) Diagnose **at least four** concrete reasons this converts poorly (tie each reason to a typical HTML→PPTX converter limitation).\n  2) Rewrite the slide HTML so it will convert correctly to PPTX:\n     - Keep 16:9 size (720pt × 405pt)\n     - Use ONLY web-safe fonts\n     - Use a **two-column layout**: left = title + bullets + takeaway, right = a **chart placeholder region** that a JS step can find\n     - Do NOT use CSS gradients or inline SVG for visuals; show how you'd include the gradient and any icon/decoration in a compatible way\n     - Ensure the bullet list is created as an actual list (no manual bullet characters in the text)\n  3) Provide a minimal JS *pseudocode* snippet showing how you'd:\n     - Convert the HTML slide\n     - Locate the placeholder by id\n     - Insert a bar chart into that placeholder using PptxGenJS (or similar), using the placeholder x/y/w/h.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical, correct knowledge for fixing HTML→PPTX conversion issues and setting up a placeholder-based chart insertion workflow.\n\n    Score 1.0 if the answer includes ALL of the following:\n    - Diagnosis: At least 4 distinct, accurate causes, including:\n      (a) Text is missing because the converter only preserves text inside semantic text tags (e.g., p/h1-h6/ul/ol) and drops text in div/span.\n      (b) CSS gradients (linear-gradient/radial-gradient) do not convert; must rasterize to an image (PNG) and place via <img> or equivalent.\n      (c) Manual bullet characters (•, -, *) are not the right way; must use <ul>/<ol> with <li> items (no bullet glyphs in text).\n      (d) Non-web-safe font usage (e.g., Inter) is risky/unsupported; should switch to a web-safe font like Arial/Helvetica/etc.\n      (Other valid issues can count too, e.g., inline SVG/icons not supported and should be rasterized; need a real placeholder marker; layout problems.)\n    - Corrected HTML:\n      - Sets 720pt × 405pt and uses a web-safe font.\n      - All visible text is inside <p> and/or <h1>-<h6> and list tags (<ul>/<ol>/<li>), not bare div/span.\n      - Implements a two-column layout (e.g., flexbox) with left text and a right chart area.\n      - Marks the chart region as a placeholder the JS step can discover (e.g., class=\"placeholder\" plus an id).\n      - Avoids CSS gradients and shows a compatible approach (e.g., uses a pre-rendered PNG background image; mentions rasterizing gradient/icon with Sharp or similar).\n    - JS pseudocode:\n      - Shows the flow: convert HTML slide → obtain placeholder bounds → add a chart using those bounds → write the PPTX file.\n      - Uses placeholder id matching and uses x/y/w/h in the chart placement call (exact API names not required).\n\n    Score 0.5 if the answer is directionally correct but misses one critical element, such as:\n    - Only 2-3 correct diagnosis points, or\n    - Corrected HTML still uses unsupported constructs (e.g., keeps linear-gradient, keeps text in div/span), or\n    - No clear placeholder strategy / no chart insertion pseudocode using placeholder bounds.\n\n    Score 0.0 if the answer fails to identify the key conversion limitations (text-tag restriction and gradient rasterization), or provides a rewrite that would likely still drop text / not convert, and lacks a workable plan for placeholder-based chart insertion.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong domain knowledge of HTML→PPTX conversion pitfalls (text in non-semantic tags, CSS gradients, manual bullets, non-web-safe fonts) and requires practical debugging skills. The grader is robust and accepts multiple valid approaches while testing genuine transferable knowledge about conversion limitations. However, the test is somewhat lengthy and complex, potentially testing multiple concepts simultaneously rather than focusing on a single core skill pattern. The scenario is realistic but slightly contrived in combining all these issues in one example. Minor deduction for complexity that might make it harder to isolate what specific knowledge is being tested."
        },
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n**What domain knowledge does this skill teach?**\n1. PPTX files are ZIP archives containing XML - the internal structure with slides, notes, themes, media in specific paths\n2. Three distinct workflows: html2pptx for new presentations, OOXML editing for modifying existing files, and template-based creation using rearrange/inventory/replace scripts\n3. When to use text extraction (markitdown) vs raw XML access (unpack) based on what content you need\n4. Design principles for layouts - particularly avoiding vertical stacking of charts/tables, preferring two-column or full-slide layouts\n\n**What workflow decisions does it enable?**\n- Choosing between markitdown (quick text) vs unpacking (when you need speaker notes, comments, layouts, animations)\n- Selecting html2pptx (scratch) vs OOXML (editing) vs template workflow based on the task\n- Understanding that validation is critical after XML edits to avoid corrupting files\n\n**What would a practitioner struggle with without this skill?**\n- Not knowing where speaker notes, comments, or theme information are stored in the file structure\n- Using the wrong workflow (e.g., trying to edit XML when html2pptx would be simpler for new content)\n- Creating corrupt PPTX files by not validating after XML modifications\n\nI'll test the understanding of PPTX internal structure and when raw XML access is needed vs simple text extraction.\n\n```yaml\nname: pptx-content-access-methods\nskills:\n  - pptx\nprompt: |\n  I have a PowerPoint presentation file and I need to accomplish two different tasks:\n  \n  Task A: I want to extract all the visible text content from the slides to create a summary document.\n  \n  Task B: I need to read the speaker notes that the presenter wrote for each slide, plus check what comments reviewers left on specific slides.\n  \n  For each task, explain what approach I should use and why. If I need to access specific files within the PPTX, tell me what paths/locations I should look at.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of PPTX file access methods.\n    \n    Key concepts that should be present:\n    \n    1. For Task A (text extraction): Should recommend a simpler approach like using markitdown or a text extraction tool, since only visible slide text is needed. Should NOT require unpacking the archive for this simple use case.\n    \n    2. For Task B (speaker notes and comments): Should explain that raw XML access is needed because speaker notes and comments are stored in specific XML files that text extraction tools don't expose. Should mention:\n       - Speaker notes are in ppt/notesSlides/ directory (notesSlide1.xml, notesSlide2.xml, etc.)\n       - Comments are in ppt/comments/ directory\n       - Need to unpack/extract the PPTX (which is a ZIP archive) to access these files\n    \n    3. Should demonstrate understanding that PPTX files are ZIP archives containing XML files.\n    \n    Score 1.0 if: Response correctly distinguishes between the two tasks, recommends appropriate methods for each, and correctly identifies the file paths for notes (ppt/notesSlides/) and comments (ppt/comments/).\n    \n    Score 0.5 if: Response understands that different approaches are needed and correctly identifies that PPTX contains XML, but is missing or incorrect about specific file paths, OR recommends unpacking for both tasks without explaining why Task A doesn't require it.\n    \n    Score 0.0 if: Response doesn't distinguish between the tasks, doesn't mention that PPTX is a ZIP/archive containing XML, or provides fundamentally incorrect information about where notes/comments are stored.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test correctly identifies a transferable concept (when to use simple text extraction vs. raw XML access based on content needs) and has a robust grader that accepts multiple valid explanations. However, it has moderate realism issues: while practitioners do need to extract notes/comments, the specific task of 'explain what approach to use' is somewhat pedagogical rather than a direct work task. A more realistic framing would be 'extract the speaker notes from this presentation' rather than 'explain how you would extract them.' The test measures genuine understanding of PPTX structure and workflow selection, not trivia, earning strong marks for non-obviousness and clarity."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this PPTX skill document, let me reason through what domain knowledge and decisions it teaches:\n\n## Skill Analysis\n\n**Core Domain Knowledge:**\n- PPTX files are editable ZIP archives containing XML, not binary/opaque files\n- Multiple tools and workflows exist for different tasks (read-only vs edit vs create from scratch vs template-based)\n- Design decisions should be informed by content, not arbitrary choices\n\n**Key Workflow Decisions:**\n1. **When to read vs edit vs create**: Is the task just analyzing text (markitdown), extracting details (XML unpacking), creating new content (html2pptx), or modifying existing (OOXML editing)?\n2. **Design approach selection**: Before implementing, analyze content to choose colors, typography, layout patterns\n3. **Layout strategy**: Two-column vs full-slide for charts/tables (not vertical stacking)\n4. **Template vs from-scratch**: When should you use an existing template vs building from scratch?\n\n**Common Practitioner Struggles Without This Skill:**\n- Not knowing PPTX is editable/analyzable XML → thinking it's impossible to modify\n- Using wrong tools for the task (e.g., trying to unpack when markitdown would work)\n- Making design choices without content analysis\n- Creating layouts that don't match best practices (vertical stacking instead of two-column)\n- Not validating edits after modifying XML\n\nThe best test measures whether a model can **choose the right approach for a realistic presentation task**, considering both technical workflows and design principles.\n\n```yaml\nname: pptx-workflow-and-design-decisions\nskills:\n  - pptx\nprompt: |\n  You're asked to complete three separate presentation tasks:\n  \n  **Task 1**: You need to analyze the text content of an existing 50-slide corporate presentation to understand its structure and create a summary document. The presentation has speaker notes you need to access.\n  \n  **Task 2**: You need to create a brand-new presentation from scratch about \"The Evolution of AI in Healthcare\" (8-10 slides). Your audience is healthcare professionals who value clinical evidence and clear data visualization.\n  \n  **Task 3**: You have a template presentation with consistent branding that you need to customize with new content while preserving the design system.\n  \n  For each task, explain:\n  1. Which tools/approaches you would use and why\n  2. Any key decisions or principles you'd apply\n  3. How you'd handle any specific challenges\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of PPTX workflows and design principles.\n    \n    **Task 1 Assessment** (Text extraction with speaker notes):\n    - Should recognize that speaker notes are in raw XML (notesSlides), not accessible via simple text conversion\n    - Should mention unpacking the PPTX or using XML-based approaches\n    - Should understand that markitdown alone won't extract speaker notes\n    \n    **Task 2 Assessment** (New presentation from scratch):\n    - Should discuss content-informed design decisions BEFORE implementation\n    - Should mention analyzing the audience/topic to inform color and typography choices\n    - Should show awareness of web-safe fonts\n    - Should indicate understanding that healthcare topic might suggest specific design approaches\n    - Should mention layout strategy (two-column vs full-slide for data visualization)\n    - Should reference html2pptx or similar approach for creation\n    \n    **Task 3 Assessment** (Template customization):\n    - Should recognize template-based workflow (extract, analyze, rearrange, replace content)\n    - Should mention tools like rearrange.py, inventory.py, or replace.py\n    - Should understand preserving design system means editing text while keeping layout/styling\n    - Should show awareness of placeholder analysis and text replacement\n    \n    **Overall Assessment**:\n    - Does response show that PPTX files are editable/analyzable (not opaque)?\n    - Are workflow choices justified based on task requirements?\n    - Is design thinking present (not just technical mechanics)?\n    - Are decisions about when to use which tools sound reasonable?\n    \n    Score 1.0 if:\n    - All three tasks have appropriate workflows selected with clear justification\n    - Task 1 correctly identifies XML/unpacking as needed for speaker notes\n    - Task 2 shows content-informed design thinking and layout considerations\n    - Task 3 understands template preservation and content replacement strategy\n    - Overall demonstrates that PPTX is editable and analyzable\n    \n    Score 0.5 if:\n    - Two tasks have reasonable approaches but one is missing key considerations\n    - Shows understanding of multiple workflows but lacks design thinking\n    - Correctly identifies tools but doesn't fully justify choices\n    - Understands PPTX structure but gives generic advice without task-specific reasoning\n    \n    Score 0.0 if:\n    - Treats PPTX as non-editable or opaque format\n    - Suggests only one approach for all tasks regardless of differences\n    - Shows no awareness of design/content considerations\n    - Cannot differentiate between the three different workflows\n  \n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates good understanding of PPTX workflows and design principles, testing transferable knowledge about when to use different approaches (text extraction vs. creation vs. template editing). However, it has clarity issues: the grader is overly prescriptive about specific tool names (rearrange.py, inventory.py, replace.py) which are skill-specific implementation details rather than conceptual understanding. A model could demonstrate correct workflow understanding using alternative valid approaches (e.g., manual XML editing, different scripts) but might be penalized. The test would be stronger if it focused on conceptual decisions (unpacking for notes access, content-informed design, preserving template structure) rather than requiring specific script names. The realism is strong - these are genuine practitioner scenarios."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: ooxml-edit-slide-title-and-speaker-notes-schema-safe\nskills:\n- pptx\nprompt: |\n  You are editing an existing PowerPoint (.pptx) by unpacking it and modifying the raw Office Open XML (OOXML).\n\n  Goal:\n  1) On slide 3, change the title text to: \" Q1 2026 Results\" (IMPORTANT: there is a leading space before Q).\n  2) Format that title run as: Arial, 32pt, bold, white (#FFFFFF).\n  3) Add/update the speaker notes for slide 3 to contain the single paragraph:\n     \"Key message: margin expansion despite higher freight costs.\"\n\n  You opened `ppt/slides/slide3.xml` and found this (broken) fragment for the title shape’s text body:\n\n  ```xml\n  <p:txBody>\n    <a:p>\n      <a:r>\n        <a:rPr lang=\"en-US\" sz=\"2400\"/>\n        <a:t>Old Title</a:t>\n      </a:r>\n    </a:p>\n    <a:bodyPr/>\n    <a:lstStyle/>\n  </p:txBody>\n  ```\n\n  Task:\n  - Provide a corrected OOXML fragment for this `<p:txBody>` that (a) is schema-compliant and (b) applies the requested text and formatting, including correctly handling the leading space.\n  - Describe (at a practical workflow level) which other PPTX internal file(s) you would edit for the speaker notes, and what validation/repack steps you would perform to avoid creating a corrupt PPTX.\n\n  Constraints:\n  - Answer as if you cannot use PowerPoint UI; you must operate on unpacked OOXML.\n  - Don’t just say “edit the slide XML”; be specific about the internal paths and the schema/validation concerns you’re addressing.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical, schema-correct PPTX OOXML editing knowledge.\n\n    Score 1.0 if the answer includes ALL of:\n    - Identifies the correct internal file to edit for slide content (e.g., `ppt/slides/slide3.xml`) AND the correct internal file to edit for speaker notes (e.g., `ppt/notesSlides/notesSlide3.xml`).\n    - Provides a corrected `<p:txBody>` snippet where the element ordering is schema-compliant: `<a:bodyPr/>` then `<a:lstStyle/>` then `<a:p>...`.\n    - Handles the leading space in the new title correctly (explicitly mentions `xml:space=\"preserve\"` on `<a:t>` or an equivalent correct OOXML technique to preserve leading whitespace).\n    - Applies the requested formatting in a plausible OOXML way: 32pt expressed as `sz=\"3200\"` (or otherwise clearly correct units), bold enabled, font as Arial (e.g., via `typeface=\"Arial\"` or a valid OOXML font declaration), and white fill (`FFFFFF`).\n    - Mentions a realistic unpack → edit → validate → repack workflow, including immediate validation after edits to prevent corruption (tool names not required, but the concept of OOXML validation is required). Bonus but not required: mentions checking slide relationships if notes are newly added.\n\n    Score 0.5 if the answer is mostly correct but misses ONE key requirement above (e.g., fixes ordering but forgets whitespace preservation; or names slide file but is vague about notes file; or gives formatting without correct size units).\n\n    Score 0.0 if the answer is generic/non-OOXML (e.g., suggests using PowerPoint UI), fails to fix the ordering, or does not demonstrate knowledge of where notes live and how to avoid PPTX corruption (validation/repack).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates good understanding of OOXML editing workflows and schema compliance, testing genuine technical knowledge about element ordering, whitespace preservation, and validation workflows. However, it has clarity issues: the grader requires exact unit conversion (sz='3200' for 32pt) which is overly specific trivia, and the prompt's broken XML fragment is somewhat artificial. The test is realistic for practitioners who need to programmatically edit presentations, but the grading criteria around exact attribute values and unit conversions makes it somewhat brittle."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: pptx-html-structure-check\nskills:\n- pptx\nprompt: |\n  I am creating a new PowerPoint presentation from scratch using the `html2pptx` workflow. \n  Please write the full HTML code for a single 16:9 slide.\n  \n  The slide should have a dark grey background box (300x200pt) centered on the screen. \n  Inside this box, place the text \"Confidential\" in white.\n  \n  Ensure you follow the specific text rendering rules for this tool so the text is not ignored during conversion.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the HTML code provided by the assistant based on the `html2pptx` skill requirements.\n\n    The critical failure mode for this tool is that text placed directly in `<div>` or `<span>` tags is ignored/invisible. Text MUST be wrapped in `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags.\n\n    Score 1.0 if:\n    1. The `<body>` dimensions are set to width: 720pt and height: 405pt (standard 16:9).\n    2. The text \"Confidential\" is wrapped in a valid text tag (like `<p>`, `<h1>`, etc.) inside the container div. Example: `<div><p>Confidential</p></div>`.\n\n    Score 0.5 if:\n    - The text tag is correct, but dimensions are missing or incorrect.\n    - The dimensions are correct, but the HTML structure is slightly off (e.g., missing body tag), provided the text nesting is correct.\n\n    Score 0.0 if:\n    - The text \"Confidential\" is placed directly inside a `<div>` or `<span>` without an inner text tag (e.g., `<div>Confidential</div>`). This is a critical failure.\n    - The response creates a Python script using `python-pptx` instead of providing the HTML for the `html2pptx` workflow.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test correctly identifies a genuine gotcha (text must be in <p>/<h1>-<h6>/<ul>/<ol> tags, not bare in <div>/<span>) that transfers across HTML-to-document conversion contexts. However, it's somewhat narrow - testing only one specific rule rather than broader understanding of the html2pptx workflow. The grader is robust and clearly distinguishes between correct/incorrect text nesting. The task is realistic (creating a simple slide element), though slightly contrived as a standalone exercise. Scores: non_obviousness=7 (useful pattern but narrow), clarity=9 (clear grading criteria), realism=7 (plausible but simple)."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "### Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill provides a deep understanding of the Office Open XML (OOXML) structure of `.pptx` files, viewing them as ZIP archives of XML. It emphasizes that PowerPoint generation and editing are not just about content but about structural XML compliance (element ordering, namespaces) and specific rendering constraints (web-safe fonts, container-specific text rendering).\n\n2.  **What workflow decisions does it enable?**\n    It enables the practitioner to decide between:\n    *   **Text extraction** (`markitdown`) vs. **Deep analysis** (unpacking XML for notes/comments).\n    *   **Template-based creation** (rearranging and replacing) vs. **Scratch creation** (`html2pptx`).\n    *   **Layout choices**: Specifically the \"Two-column layout\" preference for slides containing charts or tables to ensure balance and readability.\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner would likely:\n    *   Attempt to use modern/system fonts (e.g., \"Inter\" or \"Roboto\") that fail to render correctly in `html2pptx` workflows.\n    *   Place text directly inside `<div>` or `<span>` tags, which the `html2pptx` engine ignores.\n    *   Fail to state a design approach before coding, leading to generic or mismatched visual styles.\n    *   Violate OOXML schema rules (like element ordering in `<p:txBody>`) when editing XML directly, resulting in corrupted files.\n    *   Use CSS gradients which are explicitly unsupported by the conversion engine.\n\n---\n\n```yaml\nname: ai-healthcare-slide-creation\nskills:\n- pptx\nprompt: |\n  I need you to create a single PowerPoint slide using the `html2pptx` workflow. \n  \n  Topic: \"The Role of AI in Diagnostic Imaging\"\n  Content: \n  - Bullet points: \n    - Automated detection of anomalies in X-rays and MRIs\n    - Reducing radiologist fatigue through prioritization\n    - Real-time image enhancement and noise reduction\n  - Add a placeholder for a \"Diagnostic Accuracy Comparison\" bar chart.\n\n  Your response must:\n  1. State your design approach (tone, palette, and font choice) before providing any code.\n  2. Provide the HTML structure for the slide (720pt x 405pt).\n  3. Ensure the layout is optimized for a slide containing both text and a chart.\n  4. Adhere strictly to the technical constraints of the `html2pptx` engine regarding text tags, fonts, and layout.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    1. Design Intent: Did the model explain its design choices (colors, tone, industry relevance) before providing code?\n    2. HTML Tagging: Is all text correctly wrapped in `<p>`, `<h1>-<h6>`, `<ul>`, or `<ol>` tags? (The skill states text in `<div>` or `<span>` alone will NOT appear).\n    3. Layout: Did the model use a two-column layout (as recommended for slides with charts) rather than stacking them vertically?\n    4. Fonts: Did the model use only web-safe fonts (Arial, Helvetica, etc.) as required?\n    5. Technical Accuracy: Did it avoid CSS gradients and use a proper placeholder class for the chart?\n\n    Score 1.0 if:\n    - Design approach is stated first.\n    - Uses a two-column flex layout.\n    - All text is inside appropriate text tags (`<p>`, `<li>`, etc.).\n    - Uses a web-safe font (e.g., Arial, Verdana).\n    - Includes a `<div class=\"placeholder\">` for the chart.\n\n    Score 0.5 if:\n    - Design approach is stated, but there is one technical error (e.g., used a non-web-safe font OR placed text directly in a `<div>` without a `<p>` tag).\n    - Or, technical implementation is perfect but the design approach was skipped.\n\n    Score 0.0 if:\n    - No design approach is stated AND the code has significant errors (like using CSS gradients, stacking chart below text, or putting text in tags that won't render).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's technical constraints (web-safe fonts, text tagging rules, two-column layout preference) and tests transferable knowledge about presentation design workflows. However, it has moderate clarity issues: the grader's 0.5 scoring criteria is ambiguous ('one technical error' could mean many things), and the design approach requirement, while realistic, may be too subjective for consistent evaluation. The realism is good—creating a healthcare slide with charts is a genuine task—but the explicit requirement to 'state design approach before code' feels somewhat artificial, as practitioners might iterate rather than pre-declare. The non-obviousness is solid (7-8 range) as it tests genuine gotchas like text-in-div failures and font constraints, not arbitrary trivia."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze what this skill actually teaches:\n\n### 1. What domain knowledge does this skill teach?\n\nThe core domain knowledge is:\n- **PowerPoint file architecture**: .pptx files are ZIP archives with XML structure\n- **Presentation design principles**: Color theory, typography, visual hierarchy, layout composition\n- **Content-to-layout mapping**: Understanding which layouts work for which content structures\n- **Three distinct workflows**: HTML-based creation, XML editing, and template-based workflows\n\n### 2. What workflow decisions does it enable?\n\nKey decisions practitioners must make:\n- **When to use which creation method**: From scratch (html2pptx) vs editing existing (OOXML) vs template-based\n- **Layout selection**: Matching slide layouts to actual content structure (not forcing 2 items into a 3-column layout)\n- **Design choices**: Selecting colors/fonts that match content context rather than using defaults\n- **Text structuring**: Knowing that text must be in proper semantic tags (p, h1, ul) not just divs\n\n### 3. What would a practitioner actually struggle with without this skill?\n\nReal-world struggles:\n- **Template mismatches**: Using layouts with more placeholders than content pieces, creating awkward slides\n- **Design defaults**: Using generic blue/gray schemes when content suggests different colors\n- **Content structure errors**: Text not appearing because it's not in proper tags\n- **Workflow inefficiency**: Using the wrong tool for the task (XML editing when html2pptx would be simpler)\n\n**The test target**: The most practical skill is understanding content-to-layout mapping - a judgment call that requires domain knowledge about presentation structure, not just following scripts.\n\n---\n\n```yaml\nname: presentation-layout-content-matching\nskills:\n  - pptx\nprompt: |\n  You need to create a 3-slide PowerPoint presentation about a local bakery's new product line. The content is:\n  \n  Slide 1: Title slide introducing \"Sweet & Savory: Our New Collection\"\n  \n  Slide 2: Describe their two new product categories:\n  - Sweet Treats: Artisan pastries, seasonal fruit tarts, and specialty cakes\n  - Savory Options: Fresh-baked breads, gourmet sandwiches, and quiches\n  \n  Slide 3: Highlight their signature item:\n  - The \"Golden Hour Croissant\" - a butter croissant with honey glaze, available only before 10am\n  \n  You have access to these template layouts:\n  - Layout A: Title slide (centered title and subtitle)\n  - Layout B: Single text block (title + one content area)\n  - Layout C: Two columns (title + two equal content areas)\n  - Layout D: Three columns (title + three equal content areas)\n  - Layout E: Quote layout (large centered text with attribution line)\n  \n  Which layouts would you choose for slides 2 and 3, and why? Explain your reasoning about matching content structure to layout structure.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The model should demonstrate understanding that layout selection must match actual content structure.\n    \n    For Slide 2: The correct choice is Layout C (two columns) because there are exactly 2 product categories.\n    For Slide 3: The correct choice is Layout B (single text block) because there is only 1 signature item.\n    \n    Score 1.0 if:\n    - Selects Layout C for Slide 2 (recognizes 2 categories = 2 columns)\n    - Selects Layout B for Slide 3 (recognizes 1 item = single content area)\n    - Explains the reasoning about matching content count to layout structure\n    - Shows understanding that using more/fewer columns than content pieces creates problems\n    \n    Score 0.5 if:\n    - Correctly selects layouts for slides 2 and 3\n    - But provides weak or incomplete reasoning about why layout structure should match content structure\n    OR\n    - Gets one slide correct with good reasoning, but makes an error on the other\n    \n    Score 0.0 if:\n    - Selects Layout D (three columns) for Slide 2 despite having only 2 categories\n    - Selects Layout C or D for Slide 3 despite having only 1 item\n    - Shows no understanding of matching content count to layout structure\n    - Suggests forcing content into mismatched layouts\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test demonstrates good understanding of content-to-layout mapping (non-obviousness: 7/10) and represents a realistic scenario (realism: 8/10), but has significant clarity issues (clarity: 4/10). The grader is overly rigid - it only accepts one specific layout combination despite the skill document showing multiple valid approaches exist (e.g., Layout B could work for Slide 2 with sequential presentation of categories). The test also doesn't actually require using the pptx skill tools - it's a multiple-choice reasoning question that could be answered from general presentation knowledge without accessing any scripts or creating actual slides. A better test would require the model to actually create the presentation and be graded on whether the final output demonstrates proper layout matching, accepting any valid implementation approach."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this PPTX skill document, I can see it teaches several key domain concepts:\n\n1. **PowerPoint file structure knowledge** - Understanding that PPTX files are ZIP archives containing XML files\n2. **Workflow decisions** - When to use different approaches (markitdown vs raw XML vs html2pptx vs templates)\n3. **Design principles** - Color selection, layout patterns, typography constraints\n4. **Technical constraints** - Web-safe fonts, CSS limitations, XML schema requirements\n\n## What practitioners would struggle with without this skill:\n\nWithout this knowledge, someone trying to programmatically work with PowerPoint files would not know:\n- That PPTX files can be unpacked and edited as XML\n- Where to find specific content (slides, notes, comments) in the file structure\n- How to handle layout decisions for different content types (charts, tables, text)\n- Technical limitations like CSS gradients not converting properly\n\n## Test Case:\n\n```yaml\nname: pptx-programmatic-editing-workflow\nskills:\n- pptx\nprompt: |\n  A colleague has asked you to help them programmatically modify a PowerPoint presentation. They need to:\n  \n  1. Extract all speaker notes from an existing presentation\n  2. Update the text on specific slides while preserving formatting\n  3. Add a comment to slide 3\n  \n  Explain the general approach and file structure they would need to work with to accomplish these tasks programmatically. Include:\n  - The fundamental nature of PPTX files and how to access their contents\n  - Where each type of content (slides, speaker notes, comments) is stored in the file structure\n  - The general workflow for making edits to an existing presentation\n  \n  You don't need to write actual code - just explain the approach and key concepts.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of PowerPoint file structure and programmatic editing workflows.\n\n    Score 1.0 if the response correctly explains:\n    - That PPTX files are ZIP archives containing XML files\n    - The location of speaker notes (ppt/notesSlides/), slides (ppt/slides/), and comments (ppt/comments/)\n    - The general workflow: unpack the PPTX, edit XML files, validate, and repack\n    - Shows understanding that content is stored as XML that can be programmatically edited\n\n    Score 0.5 if the response:\n    - Mentions that PPTX can be programmatically edited but lacks detail on file structure\n    - OR correctly identifies some but not all key locations/concepts\n    - OR suggests a valid alternative approach (like using a library) but shows some understanding of the underlying structure\n\n    Score 0.0 if the response:\n    - Does not demonstrate understanding that PPTX files are ZIP/XML based\n    - OR only suggests using PowerPoint GUI or manual editing\n    - OR provides incorrect information about the file structure\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of domain concepts (PPTX as ZIP/XML, file structure locations, workflow) and is realistic. However, it has significant clarity issues: it asks for an 'explanation' rather than testing actual capability, making it more of a knowledge quiz than a skill test. The grader accepts explanations without requiring the agent to demonstrate it can actually perform the task. A better test would ask the agent to actually extract notes or modify slides, then grade based on whether it successfully uses the correct tools and workflow. The non-obviousness is reasonable (7/10) as it tests transferable file structure knowledge, but clarity suffers (4/10) due to the explanation-based format, and realism is moderate (6/10) since practitioners would actually do the task, not just explain it."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\n1. **Domain knowledge**: The `pptx` skill centers on effectively creating, editing, and analyzing PowerPoint presentations by working directly with `.pptx` packages, their XML internals, and specialized workflows (`html2pptx`, `ooxml`, template handling). Core concepts include choosing design elements (colors, fonts, layout) based on content/audience, structuring HTML slides so that text lives in supported tags, handling placeholders for charts/tables, rasterizing gradients/icons before embedding, understanding layout conventions (two-column preferred for charts/tables, no vertical stacking), and the proper pipeline for conversion (preparing HTML, running `html2pptx`, validating thumbnails). For editing existing decks, it requires unpacking/repacking OOXML, reading XML structures for shapes/notes/comments, and carefully formatting replacement JSON.\n\n2. **Workflow decisions enabled**: The skill helps decide when to create slides from scratch vs. use templates, how to align visual design with the topic/brand, which layout type to choose for specific content (e.g., two-column for chart+text, full-slide for single focus), how to ensure text/bullets convert properly (using only `<p>`/`<h>`/list tags, no manual bullet characters), and when to rasterize visual effects in advance. It also teaches when raw XML inspection is necessary (comments/notes/layouts) versus when a higher-level conversion (html2pptx) suffices. \n\n3. **Practitioner struggle without skill**: Without this skill, an LLM might produce PPTX-worthy content but fail to obey critical formatting rules, leading to missing text (e.g., using `<div>` content), incompatible fonts, lack of design rationale, poor layout decisions (stacking charts and text vertically), or forgetting to rasterize gradients/icons before conversion. They might also skip the mandatory step of describing design choices or ignore the two-column preferred layout when presenting charts/tables. The test should verify the model can present an informed design approach, create compliant HTML slide markup with placeholders, select a thoughtful color palette linked to content, and explain the conversion workflow.\n\n**Test Case**\n\n```yaml\nname: thoughtful-html2pptx-design-to-deck\nskills:\n- pptx\nprompt: |\n  You need to produce a short three-slide PowerPoint deck (16:9) about “Sustainable Urban Mobility.” The deck should cover (1) the challenge, (2) a comparison of two solutions, and (3) an action plan with next steps. Before writing any code:\n  1. Describe your design approach: mention the content-informed tone, the role of branding or mood, and explain why you chose the colors, fonts, and layout. Name the 3–5 colors you’ll use, why they suit the topic, and how they will signal hierarchy/contrast.\n  2. Specify which layout you’ll use for each slide, justifying choices such as why a two-column layout suits any chart/table content and why you avoid vertical stacking.\n\n  Then provide:\n  - HTML for each slide (with correct dimensions, only supported text tags, sections using `class=\"placeholder\"` for data visuals, placeholder text for any images/charts, and web-safe font usage).\n  - A brief JavaScript snippet showing how you would call `html2pptx()` (naming the files/slides) and mentioning the need to generate thumbnails for validation before finalizing.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if: The response first states a clear, content-informed design approach (tone, branding/mood, color palette of 3-5 colors with rationale), mentions font choices and layout decisions, then supplies three HTML slide files matching 16:9 dimensions, using only allowed text tags (<p>, <h1>‑<h6>, <ul>, <ol>), placeholder elements for charts/tables, and another note about web-safe fonts. The response also names the planned layout per slide (e.g., two-column for chart/text) and avoids vertically stacking charts under text. It includes a snippet invoking html2pptx() for the HTML files and notes that thumbnails should be generated/inspected before delivery.\n\n    Score 0.5 if: The response partially follows instructions (e.g., provides a design rationale but misses layout justification or uses unsupported tags, or shows HTML but no html2pptx snippet or thumbnail note).\n\n    Score 0.0 if: The response omits the design approach, uses prohibited tags/bullets, skips the html2pptx workflow explanation, or otherwise ignores multiple critical requirements.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates understanding of core pptx skill concepts (design rationale, HTML structure, html2pptx workflow, layout decisions) and creates a realistic scenario. However, it has clarity issues: the grader uses brittle string-matching criteria (e.g., requiring specific mentions of 'two-column,' 'web-safe fonts,' 'thumbnail validation') that could reject valid solutions expressing the same concepts differently. A model could demonstrate proper design thinking and workflow knowledge without using exact phrases like 'content-informed design approach' or 'avoid vertically stacking.' The grader should focus on whether the submission shows understanding of design principles, proper HTML structure, and workflow steps, rather than requiring specific terminology. The 0.5 partial credit criteria is also vague about which combinations of omissions warrant that score."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "Here’s my reasoning based on the skill document:\n\n1. **Domain knowledge taught**: The skill covers how PowerPoint files are structured (XML inside a ZIP), methods for extracting or editing content (markdown conversion vs raw OOXML), and detailed workflows for creating slides via HTML→PPTX or editing existing decks. It also emphasizes design principles (color palettes, typography, layout choices) and tooling requirements (markitdown, html2pptx, ooxml scripts, validation/packing).\n\n2. **Workflow decisions enabled**: The skill tells practitioners when to read text via markitdown versus unpacking for OOXML edits, when to pick the html2pptx workflow (new deck without template) versus template rearrangement, and how to decide layouts, palettes, and typography before coding. It guides color/theme extraction, when to manipulate theme1.xml, and the mandated validation steps. It also stresses mandatory documentation reads (ooxml.md, html2pptx.md) and precise scripts for unpack/validate/repack.\n\n3. **Likely practitioner struggles without this skill**: Without it, they’d struggle to know that speaker notes/comments require raw XML access, how to change theme colors safely, when to run validation scripts, and how to repack. They might skip reading ooxml.md, forget to run validate.py, or edit the wrong files. They might not know to extract text first or how to manage design consistency.\n\n```yaml\nname: plan-existing-pptx-xml-editing\nskills:\n  - pptx\nprompt: |\n  You’re working in a headless environment with only the pptx skill toolchain available. A client sends you `Q2-Rollout.pptx` and asks for three updates:\n\n  1. Slides 2–4 need new speaker notes that summarize each slide’s key bullet points in two concise sentences.\n  2. The deck’s accent color must switch to a teal palette (#5EA8A7 primary, #277884 deep accent) while keeping typography and layout unchanged.\n  3. Slide 5 should gain a modern review comment reminding the presenter to verify metrics.\n\n  You cannot open PowerPoint; the only allowed actions are those described in the pptx skill docs. Describe a precise, end-to-end workflow that accomplishes all three requests. Your answer must cover:\n  - How you gather existing text context before editing.\n  - Exactly which files/folders you’ll inspect or modify for notes, colors, and comments.\n  - The sequence of commands/scripts you run (including required documentation reads and validations).\n  - How you ensure the final PPTX is valid.\n\n  Be specific about file paths, required scripts, and validation steps rather than giving generic advice.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates the pptx skill workflows.\n\n    Score 1.0 if:\n    - Mentions reading ooxml.md fully before editing.\n    - Uses markitdown or equivalent to extract slide text before XML edits.\n    - Describes running ooxml/scripts/unpack.py to access raw files and pinpoints ppt/notesSlides/notesSlideN.xml for speaker notes, ppt/theme/theme1.xml (or equivalent) for color changes, and ppt/comments/modernComment_*.xml (or slide rels) for comments.\n    - Specifies running ooxml/scripts/validate.py after edits and ooxml/scripts/pack.py (or equivalent) to rebuild the PPTX.\n    - Presents a logical, ordered plan covering all three tasks end-to-end.\n\n    Score 0.5 if the plan is partially correct (e.g., addresses some but not all tasks, omits a critical script, or is vague about file locations) yet still references OOXML editing concepts.\n\n    Score 0.0 if it ignores the required workflow (e.g., suggests manual editing in PowerPoint, fails to mention raw XML editing, or lacks the necessary steps/scripts).\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "This test case fails on multiple rubric criteria. **Non-obviousness (2/10)**: It tests pure skill-specific trivia - knowing exact script names (unpack.py, validate.py, pack.py), exact file paths (ppt/notesSlides/notesSlideN.xml, ppt/theme/theme1.xml), and specific workflow steps that are memorization rather than transferable domain knowledge. **Clarity (3/10)**: The grader is overly prescriptive, requiring specific script names and file paths rather than testing whether the candidate understands the underlying concepts (XML structure, validation importance, color theme modification). A valid solution using alternative tools or approaches would be rejected. **Realism (6/10)**: While editing presentations programmatically is realistic, the specific combination of tasks (speaker notes + theme colors + comments) feels contrived to test skill coverage rather than representing a genuine practitioner need. The test essentially asks candidates to recite the skill document rather than apply domain knowledge."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "I need to analyze this PPTX skill document to understand what domain knowledge it teaches and create a high-quality test case.\n\n## Analysis\n\n### 1. What domain knowledge does this skill teach?\nThis skill teaches comprehensive PowerPoint presentation management including:\n- **Reading and extracting content** from existing presentations (text extraction, raw XML access)\n- **Creating new presentations** from scratch using HTML-to-PowerPoint workflows\n- **Editing existing presentations** by manipulating raw OOXML format\n- **Design principles** for effective presentations (color theory, typography, layout)\n- **Technical implementation** details (XML structure, schema compliance, file formats)\n\n### 2. What workflow decisions does it enable?\nThe skill enables critical decision-making:\n- **When to use different approaches**: HTML2pptx for new creations vs OOXML editing for existing templates\n- **Design choice methodology**: How to select appropriate color palettes, typography, and layouts based on content\n- **Layout selection**: When to use one-column vs two-column vs three-column layouts based on content structure\n- **Technical approach selection**: When to use simple text extraction vs raw XML access\n- **Template vs from-scratch creation**: Deciding when to adapt existing templates vs create fresh designs\n\n### 3. What would a practitioner actually struggle with without this skill?\nWithout this skill, practitioners would struggle with:\n- **Design methodology**: How to systematically choose design elements that match content rather than using defaults\n- **Technical implementation**: Understanding OOXML structure, proper element ordering, and schema compliance\n- **Workflow optimization**: Choosing the right approach (HTML2pptx vs OOXML editing vs template adaptation) for different scenarios\n- **Content-to-layout mapping**: Properly matching existing content to appropriate slide layouts\n- **Quality validation**: Systematically checking for text overflow, positioning issues, and visual problems\n\nThe key insight is that without this skill, someone might create functionally correct presentations that are visually poor, technically brittle, or inappropriate for the content's context.\n\nNow I'll create a test case that focuses on the design methodology and workflow decision-making rather than technical trivia:\n\n```yaml\nname: pptx-design-methodology-workflow\nskills:\n- pptx\nprompt: |\n  You need to create a 5-slide PowerPoint presentation for a sustainable agriculture technology company called \"GreenTech Solutions\" that will be presented to potential investors. The presentation should cover:\n\n  1. Company overview and mission\n  2. Product showcase (3 key agricultural technology solutions)\n  3. Market opportunity and target customers\n  4. Business model and revenue streams\n  5. Investment ask and growth potential\n\n  Before you create any presentation content, you must first analyze the requirements and explain your design choices. Provide:\n\n  1. **Design approach**: What color palette selection and reasoning for a sustainable agriculture tech company targeting investors\n  2. **Layout strategy**: What layout patterns you'll use for each slide type and why\n  3. **Technical workflow**: Whether you'll create from scratch with HTML2pptx or use templates, and your reasoning\n  4. **Content mapping**: How you'll structure the content to match your chosen layouts\n\n  Do NOT generate any code or HTML - just provide your design and workflow methodology analysis. This tests your understanding of the design principles and workflow decision-making process before implementation.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the quality of design methodology and workflow decision-making:\n\n    Score 1.0 if:\n    - Provides thoughtful color palette selection with clear reasoning tied to sustainable agriculture and investor audience\n    - Explains layout strategy matching content types (e.g., product showcase layout, financial data layout)\n    - Justifies technical workflow choice (html2pptx vs template-based) with solid reasoning\n    - Demonstrates understanding of content-to-layout mapping principles\n    - Shows critical thinking beyond default choices\n\n    Score 0.5 if:\n    - Provides basic color palette but weak reasoning\n    - Mentions layouts but without clear content-type matching\n    - Chooses technical workflow but with superficial justification\n    - Addresses content mapping but in a generic way\n    - Some understanding shown but lacks depth\n\n    Score 0.0 if:\n    - Uses generic/default color palettes without reasoning\n    - Doesn't explain layout choices or uses inappropriate layouts\n    - No justification for technical workflow\n    - No content mapping consideration\n    - Shows no understanding of design methodology principles\n\n    Focus on the quality of reasoning and design thinking, not on color names or technical specifics being \"correct.\"\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. **Non-obviousness (3/10)**: While it attempts to test design methodology, it asks the model to explain its approach rather than actually demonstrate the skill through execution - this is meta-analysis, not skill application. A practitioner would create the presentation, not write an essay about how they'd create it. **Clarity (2/10)**: The grader is fundamentally broken - it explicitly asks the model NOT to generate code/HTML ('Do NOT generate any code or HTML'), making it impossible to verify if the model can actually use the skill. The grader evaluates reasoning quality subjectively without any concrete deliverable to assess. **Realism (3/10)**: No practitioner would be asked to write a design methodology document without creating the actual presentation - this is an artificial academic exercise that exists only to test knowledge, not a real-world task."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this skill document, I need to analyze what domain knowledge it teaches and what practitioners would struggle with without it.\n\n## Analysis\n\n**Domain Knowledge:**\n- PowerPoint file structure (.pptx as ZIP archive with XML)\n- OOXML format and editing workflows\n- Template analysis and inventory management\n- Design principles for presentations (color palettes, typography, layout)\n- Workflow decisions (html2pptx vs OOXML vs template-based approaches)\n\n**Key Struggles Without This Skill:**\n- Not knowing .pptx files contain editable XML structure\n- Choosing wrong workflow for different tasks\n- Poor template analysis and slide mapping\n- Design decisions based on content vs default choices\n- Understanding placeholder relationships in templates\n\n**Realistic Test Scenario:**\nA practitioner needs to create a presentation following an existing template's design. This tests template analysis, layout mapping decisions, content structuring, and maintaining design consistency.\n\n```yaml\nname: template-based-presentation-creation\nskills:\n  - pptx\nprompt: |\n  You need to create a 5-slide presentation about \"Remote Work Best Practices\" using an existing PowerPoint template.\n\n  **Task Requirements:**\n  1. Analyze the provided template structure and inventory all available slide layouts\n  2. Map your 5-slide content to appropriate template slides\n  3. Create the presentation maintaining the template's design consistency\n  4. Ensure each slide's content fits properly within the chosen template placeholders\n\n  **Content Structure:**\n  - Slide 1: Title slide with presentation title and subtitle\n  - Slide 2: Section divider \"Benefits of Remote Work\"\n  - Slide 3: Three main benefits with bullet points\n  - Slide 4: Section divider \"Challenges & Solutions\"  \n  - Slide 5: Closing slide with key takeaways\n\n  **Available Template:** The template contains 73 slides with various layouts including title slides, content layouts, section dividers, and closing slides.\n\n  **Critical Requirements:**\n  - Match layout structure to actual content (don't force content into inappropriate layouts)\n  - Choose layouts that actually fit your content pieces\n  - Maintain design consistency throughout\n  - Verify all placeholder relationships work correctly\n\n  You don't need to write the actual PowerPoint code, but explain your design approach and layout choices based on the template analysis.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates proper template-based presentation creation skills.\n\n    Score 1.0 if the response shows:\n    - Understanding that .pptx files are ZIP archives with XML structure\n    - Proper template analysis approach (inventory slides, understand layouts)\n    - Content-to-layout mapping decisions based on actual content needs\n    - Design consistency awareness\n    - Recognition of different workflow approaches (template-based vs other methods)\n    - Understanding of placeholder relationships in templates\n    - Practical design considerations (color schemes, typography choices)\n\n    Score 0.5 if the response shows:\n    - Basic understanding of template usage\n    - Some awareness of layout considerations\n    - Partial understanding of presentation structure\n    - Missing detailed analysis approach\n\n    Score 0.0 if the response:\n    - Treats PowerPoint as a black box without understanding structure\n    - Chooses layouts without considering content fit\n    - Ignores design consistency\n    - Shows no awareness of template analysis process\n    - Demonstrates no understanding of different presentation creation workflows\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test case has significant issues across all dimensions. For non-obviousness (low score): it asks the agent to 'explain your design approach' rather than actually perform the task, making it more of a knowledge recitation test than a capability test. The grader checks for 'understanding that .pptx files are ZIP archives' which is pure trivia from the skill document. For clarity (low score): the grader is extremely brittle, requiring the agent to mention specific implementation details like 'ZIP archives with XML structure' and 'placeholder relationships' rather than testing actual capability. For realism (moderate score): while creating presentations from templates is realistic, asking someone to explain their approach without actually doing it is artificial - practitioners create presentations, they don't write essays about how they would create them."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-presentation-with-template\nskills:\n  - pptx\nprompt: |\n  You have been asked to create a new PowerPoint presentation using an existing template. The template is located at `templates/sample-template.pptx`. The content for the new presentation is provided below:\n\n  ### Presentation Content\n  1. **Title Slide**\n     - Title: \"Project Overview\"\n     - Subtitle: \"A Detailed Look at Our New Initiative\"\n  2. **Introduction Slide**\n     - Title: \"Introduction\"\n     - Content: \"This presentation provides an overview of our new project, including its goals, scope, and timeline.\"\n  3. **Goals Slide**\n     - Title: \"Project Goals\"\n     - Content: \n       - \"Increase market share by 15%\"\n       - \"Launch a new product line\"\n       - \"Improve customer satisfaction scores by 10%\"\n  4. **Scope Slide**\n     - Title: \"Project Scope\"\n     - Content: \"The project will cover the following areas: marketing, product development, and customer support.\"\n  5. **Timeline Slide**\n     - Title: \"Project Timeline\"\n     - Content: \n       - \"Q1: Market research and analysis\"\n       - \"Q2: Product development\"\n       - \"Q3: Beta testing and customer feedback\"\n       - \"Q4: Launch and post-launch support\"\n  6. **Conclusion Slide**\n     - Title: \"Conclusion\"\n     - Content: \"Thank you for your attention. We are confident that this project will meet our goals and bring significant benefits.\"\n\n  **Requirements**:\n  - Use the template for consistent branding and design.\n  - Ensure the presentation is visually appealing and follows the design principles outlined in the skill document.\n  - Use a clear visual hierarchy and ensure readability.\n  - Include a visual thumbnail grid of the final presentation for review.\n\n  **Output**:\n  - Create a new PowerPoint presentation using the provided template.\n  - Save the final presentation as `output.pptx`.\n  - Generate a thumbnail grid of the final presentation and save it as `thumbnails.jpg`.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    **Score 1.0 if:**\n    - The presentation is created using the provided template.\n    - All content is accurately and clearly presented on the slides.\n    - The presentation follows the design principles outlined in the skill document, including consistent branding, clear visual hierarchy, and readability.\n    - A thumbnail grid of the final presentation is provided and all slides are visually correct with no text cutoff, overlap, or contrast issues.\n\n    **Score 0.5 if:**\n    - The presentation is mostly accurate, with minor content or design issues.\n    - Some design principles are followed, but not all.\n    - The thumbnail grid is provided, but there are minor visual issues on some slides.\n\n    **Score 0.0 if:**\n    - The presentation is not created using the provided template.\n    - Major content or design issues are present.\n    - The thumbnail grid is missing or the presentation has significant visual issues on multiple slides.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "This test requires a specific template file (`templates/sample-template.pptx`) that doesn't exist in the skill files, making it impossible to run. The grader is also brittle, requiring exact visual validation without clear criteria for what constitutes 'visually correct.' While the task itself (creating a presentation from a template) is realistic, the test fails on clarity due to missing dependencies and vague grading criteria. The non-obviousness is moderate as it tests the template workflow, but the broken setup significantly undermines the test's utility."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: creating-presentation-with-template\nskills:\n- pptx\nprompt: |\n  Create a new PowerPoint presentation using a provided template. The template has 10 slides with various layouts. Replace the placeholder text with new content on slides 1, 3, 5, and 7. Add a new chart to slide 5 and ensure all text is properly formatted and aligned. Use web-safe fonts and respect the template's design principles.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the presentation is correctly created with the template, all placeholder text is replaced with new content, a new chart is added to slide 5, and the formatting and alignment are proper.\n    Score 0.5 if the presentation is created but some elements are missing or improperly formatted.\n    Score 0.0 if the presentation is not created or does not follow the instructions.\n  extractor: last_assistant\n```",
          "score": 0.15,
          "rationale": "This test case has critical flaws across all dimensions. Non-obviousness (2/10): Requires knowing the exact workflow for template-based presentations including specific script names (rearrange.py, inventory.py, replace.py) rather than testing transferable knowledge about presentation design or structure. Clarity (1/10): Fundamentally broken - references 'a provided template' that doesn't exist, making the test impossible to execute. The grader cannot verify chart addition, text replacement, or formatting without access to the actual files. Realism (3/10): While using templates is realistic, the task is contrived around testing specific skill mechanics rather than solving an actual business need."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-hugging-face-datasets",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 11,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hugging-face-datasets\n\n**SKILL.md Content:**\n```markdown\n---\nname: hugging-face-datasets\ndescription: Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. Designed to work alongside HF MCP server for comprehensive dataset workflows.\n---\n\n# Overview\nThis skill provides tools to manage datasets on the Hugging Face Hub with a focus on creation, configuration, content management, and SQL-based data manipulation. It is designed to complement the existing Hugging Face MCP server by providing dataset editing and querying capabilities.\n\n## Integration with HF MCP Server\n- **Use HF MCP Server for**: Dataset discovery, search, and metadata retrieval\n- **Use This Skill for**: Dataset creation, content editing, SQL queries, data transformation, and structured data formatting\n\n# Version\n2.1.0\n\n# Dependencies\n# This skill uses PEP 723 scripts with inline dependency management\n# Scripts auto-install requirements when run with: uv run scripts/script_name.py\n\n- uv (Python package manager)\n- Getting Started: See \"Usage Instructions\" below for PEP 723 usage\n\n# Core Capabilities\n\n## 1. Dataset Lifecycle Management\n- **Initialize**: Create new dataset repositories with proper structure\n- **Configure**: Store detailed configuration including system prompts and metadata\n- **Stream Updates**: Add rows efficiently without downloading entire datasets\n\n## 2. SQL-Based Dataset Querying (NEW)\nQuery any Hugging Face dataset using DuckDB SQL via `scripts/sql_manager.py`:\n- **Direct Queries**: Run SQL on datasets using the `hf://` protocol\n- **Schema Discovery**: Describe dataset structure and column types\n- **Data Sampling**: Get random samples for exploration\n- **Aggregations**: Count, histogram, unique values analysis\n- **Transformations**: Filter, join, reshape data with SQL\n- **Export & Push**: Save results locally or push to new Hub repos\n\n## 3. Multi-Format Dataset Support\nSupports diverse dataset types through template system:\n- **Chat/Conversational**: Chat templating, multi-turn dialogues, tool usage examples\n- **Text Classification**: Sentiment analysis, intent detection, topic classification\n- **Question-Answering**: Reading comprehension, factual QA, knowledge bases\n- **Text Completion**: Language modeling, code completion, creative writing\n- **Tabular Data**: Structured data for regression/classification tasks\n- **Custom Formats**: Flexible schema definition for specialized needs\n\n## 4. Quality Assurance Features\n- **JSON Validation**: Ensures data integrity during uploads\n- **Batch Processing**: Efficient handling of large datasets\n- **Error Recovery**: Graceful handling of upload failures and conflicts\n\n# Usage Instructions\n\nThe skill includes two Python scripts that use PEP 723 inline dependency management:\n\n> **All paths are relative to the directory containing this SKILL.md\nfile.**\n> Scripts are run with: `uv run scripts/script_name.py [arguments]`\n\n- `scripts/dataset_manager.py` - Dataset creation and management\n- `scripts/sql_manager.py` - SQL-based dataset querying and transformation\n\n### Prerequisites\n- `uv` package manager installed\n- `HF_TOKEN` environment variable must be set with a Write-access token\n\n---\n\n# SQL Dataset Querying (sql_manager.py)\n\nQuery, transform, and push Hugging Face datasets using DuckDB SQL. The `hf://` protocol provides direct access to any public dataset (or private with token).\n\n## Quick Start\n\n```bash\n# Query a dataset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition' LIMIT 10\"\n\n# Get dataset schema\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\n\n# Sample random rows\nuv run scripts/sql_manager.py sample --dataset \"cais/mmlu\" --n 5\n\n# Count rows with filter\nuv run scripts/sql_manager.py count --dataset \"cais/mmlu\" --where \"subject='nutrition'\"\n```\n\n## SQL Query Syntax\n\nUse `data` as the table name in your SQL - it gets replaced with the actual `hf://` path:\n\n```sql\n-- Basic select\nSELECT * FROM data LIMIT 10\n\n-- Filtering\nSELECT * FROM data WHERE subject='nutrition'\n\n-- Aggregations\nSELECT subject, COUNT(*) as cnt FROM data GROUP BY subject ORDER BY cnt DESC\n\n-- Column selection and transformation\nSELECT question, choices[answer] AS correct_answer FROM data\n\n-- Regex matching\nSELECT * FROM data WHERE regexp_matches(question, 'nutrition|diet')\n\n-- String functions\nSELECT regexp_replace(question, '\\n', '') AS cleaned FROM data\n```\n\n## Common Operations\n\n### 1. Explore Dataset Structure\n```bash\n# Get schema\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\n\n# Get unique values in column\nuv run scripts/sql_manager.py unique --dataset \"cais/mmlu\" --column \"subject\"\n\n# Get value distribution\nuv run scripts/sql_manager.py histogram --dataset \"cais/mmlu\" --column \"subject\" --bins 20\n```\n\n### 2. Filter and Transform\n```bash\n# Complex filtering with SQL\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT subject, COUNT(*) as cnt FROM data GROUP BY subject HAVING cnt > 100\"\n\n# Using transform command\nuv run scripts/sql_manager.py transform \\\n  --dataset \"cais/mmlu\" \\\n  --select \"subject, COUNT(*) as cnt\" \\\n  --group-by \"subject\" \\\n  --order-by \"cnt DESC\" \\\n  --limit 10\n```\n\n### 3. Create Subsets and Push to Hub\n```bash\n# Query and push to new dataset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition'\" \\\n  --push-to \"username/mmlu-nutrition-subset\" \\\n  --private\n\n# Transform and push\nuv run scripts/sql_manager.py transform \\\n  --dataset \"ibm/duorc\" \\\n  --config \"ParaphraseRC\" \\\n  --select \"question, answers\" \\\n  --where \"LENGTH(question) > 50\" \\\n  --push-to \"username/duorc-long-questions\"\n```\n\n### 4. Export to Local Files\n```bash\n# Export to Parquet\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition'\" \\\n  --output \"nutrition.parquet\" \\\n  --format parquet\n\n# Export to JSONL\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data LIMIT 100\" \\\n  --output \"sample.jsonl\" \\\n  --format jsonl\n```\n\n### 5. Working with Dataset Configs/Splits\n```bash\n# Specify config (subset)\nuv run scripts/sql_manager.py query \\\n  --dataset \"ibm/duorc\" \\\n  --config \"ParaphraseRC\" \\\n  --sql \"SELECT * FROM data LIMIT 5\"\n\n# Specify split\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --split \"test\" \\\n  --sql \"SELECT COUNT(*) FROM data\"\n\n# Query all splits\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --split \"*\" \\\n  --sql \"SELECT * FROM data LIMIT 10\"\n```\n\n### 6. Raw SQL with Full Paths\nFor complex queries or joining datasets:\n```bash\nuv run scripts/sql_manager.py raw --sql \"\n  SELECT a.*, b.* \n  FROM 'hf://datasets/dataset1@~parquet/default/train/*.parquet' a\n  JOIN 'hf://datasets/dataset2@~parquet/default/train/*.parquet' b\n  ON a.id = b.id\n  LIMIT 100\n\"\n```\n\n## Python API Usage\n\n```python\nfrom sql_manager import HFDatasetSQL\n\nsql = HFDatasetSQL()\n\n# Query\nresults = sql.query(\"cais/mmlu\", \"SELECT * FROM data WHERE subject='nutrition' LIMIT 10\")\n\n# Get schema\nschema = sql.describe(\"cais/mmlu\")\n\n# Sample\nsamples = sql.sample(\"cais/mmlu\", n=5, seed=42)\n\n# Count\ncount = sql.count(\"cais/mmlu\", where=\"subject='nutrition'\")\n\n# Histogram\ndist = sql.histogram(\"cais/mmlu\", \"subject\")\n\n# Filter and transform\nresults = sql.filter_and_transform(\n    \"cais/mmlu\",\n    select=\"subject, COUNT(*) as cnt\",\n    group_by=\"subject\",\n    order_by=\"cnt DESC\",\n    limit=10\n)\n\n# Push to Hub\nurl = sql.push_to_hub(\n    \"cais/mmlu\",\n    \"username/nutrition-subset\",\n    sql=\"SELECT * FROM data WHERE subject='nutrition'\",\n    private=True\n)\n\n# Export locally\nsql.export_to_parquet(\"cais/mmlu\", \"output.parquet\", sql=\"SELECT * FROM data LIMIT 100\")\n\nsql.close()\n```\n\n## HF Path Format\n\nDuckDB uses the `hf://` protocol to access datasets:\n```\nhf://datasets/{dataset_id}@{revision}/{config}/{split}/*.parquet\n```\n\nExamples:\n- `hf://datasets/cais/mmlu@~parquet/default/train/*.parquet`\n- `hf://datasets/ibm/duorc@~parquet/ParaphraseRC/test/*.parquet`\n\nThe `@~parquet` revision provides auto-converted Parquet files for any dataset format.\n\n## Useful DuckDB SQL Functions\n\n```sql\n-- String functions\nLENGTH(column)                    -- String length\nregexp_replace(col, '\\n', '')     -- Regex replace\nregexp_matches(col, 'pattern')    -- Regex match\nLOWER(col), UPPER(col)           -- Case conversion\n\n-- Array functions  \nchoices[0]                        -- Array indexing (0-based)\narray_length(choices)             -- Array length\nunnest(choices)                   -- Expand array to rows\n\n-- Aggregations\nCOUNT(*), SUM(col), AVG(col)\nGROUP BY col HAVING condition\n\n-- Sampling\nUSING SAMPLE 10                   -- Random sample\nUSING SAMPLE 10 (RESERVOIR, 42)   -- Reproducible sample\n\n-- Window functions\nROW_NUMBER() OVER (PARTITION BY col ORDER BY col2)\n```\n\n---\n\n# Dataset Creation (dataset_manager.py)\n\n### Recommended Workflow\n\n**1. Discovery (Use HF MCP Server):**\n```python\n# Use HF MCP tools to find existing datasets\nsearch_datasets(\"conversational AI training\")\nget_dataset_details(\"username/dataset-name\")\n```\n\n**2. Creation (Use This Skill):**\n```bash\n# Initialize new dataset\nuv run scripts/dataset_manager.py init --repo_id \"your-username/dataset-name\" [--private]\n\n# Configure with detailed system prompt\nuv run scripts/dataset_manager.py config --repo_id \"your-username/dataset-name\" --system_prompt \"$(cat system_prompt.txt)\"\n```\n\n**3. Content Management (Use This Skill):**\n```bash\n# Quick setup with any template\nuv run scripts/dataset_manager.py quick_setup \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template classification\n\n# Add data with template validation\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template qa \\\n  --rows_json \"$(cat your_qa_data.json)\"\n```\n\n### Template-Based Data Structures\n\n**1. Chat Template (`--template chat`)**\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Natural user request\"},\n    {\"role\": \"assistant\", \"content\": \"Response with tool usage\"},\n    {\"role\": \"tool\", \"content\": \"Tool response\", \"tool_call_id\": \"call_123\"}\n  ],\n  \"scenario\": \"Description of use case\",\n  \"complexity\": \"simple|intermediate|advanced\"\n}\n```\n\n**2. Classification Template (`--template classification`)**\n```json\n{\n  \"text\": \"Input text to be classified\",\n  \"label\": \"classification_label\",\n  \"confidence\": 0.95,\n  \"metadata\": {\"domain\": \"technology\", \"language\": \"en\"}\n}\n```\n\n**3. QA Template (`--template qa`)**\n```json\n{\n  \"question\": \"What is the question being asked?\",\n  \"answer\": \"The complete answer\",\n  \"context\": \"Additional context if needed\",\n  \"answer_type\": \"factual|explanatory|opinion\",\n  \"difficulty\": \"easy|medium|hard\"\n}\n```\n\n**4. Completion Template (`--template completion`)**\n```json\n{\n  \"prompt\": \"The beginning text or context\",\n  \"completion\": \"The expected continuation\",\n  \"domain\": \"code|creative|technical|conversational\",\n  \"style\": \"description of writing style\"\n}\n```\n\n**5. Tabular Template (`--template tabular`)**\n```json\n{\n  \"columns\": [\n    {\"name\": \"feature1\", \"type\": \"numeric\", \"description\": \"First feature\"},\n    {\"name\": \"target\", \"type\": \"categorical\", \"description\": \"Target variable\"}\n  ],\n  \"data\": [\n    {\"feature1\": 123, \"target\": \"class_a\"},\n    {\"feature1\": 456, \"target\": \"class_b\"}\n  ]\n}\n```\n\n### Advanced System Prompt Template\n\nFor high-quality training data generation:\n```text\nYou are an AI assistant expert at using MCP tools effectively.\n\n## MCP SERVER DEFINITIONS\n[Define available servers and tools]\n\n## TRAINING EXAMPLE STRUCTURE\n[Specify exact JSON schema for chat templating]\n\n## QUALITY GUIDELINES\n[Detail requirements for realistic scenarios, progressive complexity, proper tool usage]\n\n## EXAMPLE CATEGORIES\n[List development workflows, debugging scenarios, data management tasks]\n```\n\n### Example Categories & Templates\n\nThe skill includes diverse training examples beyond just MCP usage:\n\n**Available Example Sets:**\n- `training_examples.json` - MCP tool usage examples (debugging, project setup, database analysis)\n- `diverse_training_examples.json` - Broader scenarios including:\n  - **Educational Chat** - Explaining programming concepts, tutorials\n  - **Git Workflows** - Feature branches, version control guidance\n  - **Code Analysis** - Performance optimization, architecture review\n  - **Content Generation** - Professional writing, creative brainstorming\n  - **Codebase Navigation** - Legacy code exploration, systematic analysis\n  - **Conversational Support** - Problem-solving, technical discussions\n\n**Using Different Example Sets:**\n```bash\n# Add MCP-focused examples\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(cat examples/training_examples.json)\"\n\n# Add diverse conversational examples\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(cat examples/diverse_training_examples.json)\"\n\n# Mix both for comprehensive training data\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(jq -s '.[0] + .[1]' examples/training_examples.json examples/diverse_training_examples.json)\"\n```\n\n### Commands Reference\n\n**List Available Templates:**\n```bash\nuv run scripts/dataset_manager.py list_templates\n```\n\n**Quick Setup (Recommended):**\n```bash\nuv run scripts/dataset_manager.py quick_setup --repo_id \"your-username/dataset-name\" --template classification\n```\n\n**Manual Setup:**\n```bash\n# Initialize repository\nuv run scripts/dataset_manager.py init --repo_id \"your-username/dataset-name\" [--private]\n\n# Configure with system prompt\nuv run scripts/dataset_manager.py config --repo_id \"your-username/dataset-name\" --system_prompt \"Your prompt here\"\n\n# Add data with validation\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template qa \\\n  --rows_json '[{\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence...\"}]'\n```\n\n**View Dataset Statistics:**\n```bash\nuv run scripts/dataset_manager.py stats --repo_id \"your-username/dataset-name\"\n```\n\n### Error Handling\n- **Repository exists**: Script will notify and continue with configuration\n- **Invalid JSON**: Clear error message with parsing details\n- **Network issues**: Automatic retry for transient failures\n- **Token permissions**: Validation before operations begin\n\n---\n\n# Combined Workflow Examples\n\n## Example 1: Create Training Subset from Existing Dataset\n```bash\n# 1. Explore the source dataset\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\nuv run scripts/sql_manager.py histogram --dataset \"cais/mmlu\" --column \"subject\"\n\n# 2. Query and create subset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject IN ('nutrition', 'anatomy', 'clinical_knowledge')\" \\\n  --push-to \"username/mmlu-medical-subset\" \\\n  --private\n```\n\n## Example 2: Transform and Reshape Data\n```bash\n# Transform MMLU to QA format with correct answers extracted\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT question, choices[answer] as correct_answer, subject FROM data\" \\\n  --push-to \"username/mmlu-qa-format\"\n```\n\n## Example 3: Merge Multiple Dataset Splits\n```bash\n# Export multiple splits and combine\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --split \"*\" \\\n  --output \"mmlu_all.parquet\"\n```\n\n## Example 4: Quality Filtering\n```bash\n# Filter for high-quality examples\nuv run scripts/sql_manager.py query \\\n  --dataset \"squad\" \\\n  --sql \"SELECT * FROM data WHERE LENGTH(context) > 500 AND LENGTH(question) > 20\" \\\n  --push-to \"username/squad-filtered\"\n```\n\n## Example 5: Create Custom Training Dataset\n```bash\n# 1. Query source data\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT question, subject FROM data WHERE subject='nutrition'\" \\\n  --output \"nutrition_source.jsonl\" \\\n  --format jsonl\n\n# 2. Process with your pipeline (add answers, format, etc.)\n\n# 3. Push processed data\nuv run scripts/dataset_manager.py init --repo_id \"username/nutrition-training\"\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"username/nutrition-training\" \\\n  --template qa \\\n  --rows_json \"$(cat processed_data.json)\"\n```\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hugging-face-datasets\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hugging-face-datasets\n\n**SKILL.md Content:**\n```markdown\n---\nname: hugging-face-datasets\ndescription: Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. Designed to work alongside HF MCP server for comprehensive dataset workflows.\n---\n\n# Overview\nThis skill provides tools to manage datasets on the Hugging Face Hub with a focus on creation, configuration, content management, and SQL-based data manipulation. It is designed to complement the existing Hugging Face MCP server by providing dataset editing and querying capabilities.\n\n## Integration with HF MCP Server\n- **Use HF MCP Server for**: Dataset discovery, search, and metadata retrieval\n- **Use This Skill for**: Dataset creation, content editing, SQL queries, data transformation, and structured data formatting\n\n# Version\n2.1.0\n\n# Dependencies\n# This skill uses PEP 723 scripts with inline dependency management\n# Scripts auto-install requirements when run with: uv run scripts/script_name.py\n\n- uv (Python package manager)\n- Getting Started: See \"Usage Instructions\" below for PEP 723 usage\n\n# Core Capabilities\n\n## 1. Dataset Lifecycle Management\n- **Initialize**: Create new dataset repositories with proper structure\n- **Configure**: Store detailed configuration including system prompts and metadata\n- **Stream Updates**: Add rows efficiently without downloading entire datasets\n\n## 2. SQL-Based Dataset Querying (NEW)\nQuery any Hugging Face dataset using DuckDB SQL via `scripts/sql_manager.py`:\n- **Direct Queries**: Run SQL on datasets using the `hf://` protocol\n- **Schema Discovery**: Describe dataset structure and column types\n- **Data Sampling**: Get random samples for exploration\n- **Aggregations**: Count, histogram, unique values analysis\n- **Transformations**: Filter, join, reshape data with SQL\n- **Export & Push**: Save results locally or push to new Hub repos\n\n## 3. Multi-Format Dataset Support\nSupports diverse dataset types through template system:\n- **Chat/Conversational**: Chat templating, multi-turn dialogues, tool usage examples\n- **Text Classification**: Sentiment analysis, intent detection, topic classification\n- **Question-Answering**: Reading comprehension, factual QA, knowledge bases\n- **Text Completion**: Language modeling, code completion, creative writing\n- **Tabular Data**: Structured data for regression/classification tasks\n- **Custom Formats**: Flexible schema definition for specialized needs\n\n## 4. Quality Assurance Features\n- **JSON Validation**: Ensures data integrity during uploads\n- **Batch Processing**: Efficient handling of large datasets\n- **Error Recovery**: Graceful handling of upload failures and conflicts\n\n# Usage Instructions\n\nThe skill includes two Python scripts that use PEP 723 inline dependency management:\n\n> **All paths are relative to the directory containing this SKILL.md\nfile.**\n> Scripts are run with: `uv run scripts/script_name.py [arguments]`\n\n- `scripts/dataset_manager.py` - Dataset creation and management\n- `scripts/sql_manager.py` - SQL-based dataset querying and transformation\n\n### Prerequisites\n- `uv` package manager installed\n- `HF_TOKEN` environment variable must be set with a Write-access token\n\n---\n\n# SQL Dataset Querying (sql_manager.py)\n\nQuery, transform, and push Hugging Face datasets using DuckDB SQL. The `hf://` protocol provides direct access to any public dataset (or private with token).\n\n## Quick Start\n\n```bash\n# Query a dataset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition' LIMIT 10\"\n\n# Get dataset schema\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\n\n# Sample random rows\nuv run scripts/sql_manager.py sample --dataset \"cais/mmlu\" --n 5\n\n# Count rows with filter\nuv run scripts/sql_manager.py count --dataset \"cais/mmlu\" --where \"subject='nutrition'\"\n```\n\n## SQL Query Syntax\n\nUse `data` as the table name in your SQL - it gets replaced with the actual `hf://` path:\n\n```sql\n-- Basic select\nSELECT * FROM data LIMIT 10\n\n-- Filtering\nSELECT * FROM data WHERE subject='nutrition'\n\n-- Aggregations\nSELECT subject, COUNT(*) as cnt FROM data GROUP BY subject ORDER BY cnt DESC\n\n-- Column selection and transformation\nSELECT question, choices[answer] AS correct_answer FROM data\n\n-- Regex matching\nSELECT * FROM data WHERE regexp_matches(question, 'nutrition|diet')\n\n-- String functions\nSELECT regexp_replace(question, '\\n', '') AS cleaned FROM data\n```\n\n## Common Operations\n\n### 1. Explore Dataset Structure\n```bash\n# Get schema\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\n\n# Get unique values in column\nuv run scripts/sql_manager.py unique --dataset \"cais/mmlu\" --column \"subject\"\n\n# Get value distribution\nuv run scripts/sql_manager.py histogram --dataset \"cais/mmlu\" --column \"subject\" --bins 20\n```\n\n### 2. Filter and Transform\n```bash\n# Complex filtering with SQL\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT subject, COUNT(*) as cnt FROM data GROUP BY subject HAVING cnt > 100\"\n\n# Using transform command\nuv run scripts/sql_manager.py transform \\\n  --dataset \"cais/mmlu\" \\\n  --select \"subject, COUNT(*) as cnt\" \\\n  --group-by \"subject\" \\\n  --order-by \"cnt DESC\" \\\n  --limit 10\n```\n\n### 3. Create Subsets and Push to Hub\n```bash\n# Query and push to new dataset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition'\" \\\n  --push-to \"username/mmlu-nutrition-subset\" \\\n  --private\n\n# Transform and push\nuv run scripts/sql_manager.py transform \\\n  --dataset \"ibm/duorc\" \\\n  --config \"ParaphraseRC\" \\\n  --select \"question, answers\" \\\n  --where \"LENGTH(question) > 50\" \\\n  --push-to \"username/duorc-long-questions\"\n```\n\n### 4. Export to Local Files\n```bash\n# Export to Parquet\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject='nutrition'\" \\\n  --output \"nutrition.parquet\" \\\n  --format parquet\n\n# Export to JSONL\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data LIMIT 100\" \\\n  --output \"sample.jsonl\" \\\n  --format jsonl\n```\n\n### 5. Working with Dataset Configs/Splits\n```bash\n# Specify config (subset)\nuv run scripts/sql_manager.py query \\\n  --dataset \"ibm/duorc\" \\\n  --config \"ParaphraseRC\" \\\n  --sql \"SELECT * FROM data LIMIT 5\"\n\n# Specify split\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --split \"test\" \\\n  --sql \"SELECT COUNT(*) FROM data\"\n\n# Query all splits\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --split \"*\" \\\n  --sql \"SELECT * FROM data LIMIT 10\"\n```\n\n### 6. Raw SQL with Full Paths\nFor complex queries or joining datasets:\n```bash\nuv run scripts/sql_manager.py raw --sql \"\n  SELECT a.*, b.* \n  FROM 'hf://datasets/dataset1@~parquet/default/train/*.parquet' a\n  JOIN 'hf://datasets/dataset2@~parquet/default/train/*.parquet' b\n  ON a.id = b.id\n  LIMIT 100\n\"\n```\n\n## Python API Usage\n\n```python\nfrom sql_manager import HFDatasetSQL\n\nsql = HFDatasetSQL()\n\n# Query\nresults = sql.query(\"cais/mmlu\", \"SELECT * FROM data WHERE subject='nutrition' LIMIT 10\")\n\n# Get schema\nschema = sql.describe(\"cais/mmlu\")\n\n# Sample\nsamples = sql.sample(\"cais/mmlu\", n=5, seed=42)\n\n# Count\ncount = sql.count(\"cais/mmlu\", where=\"subject='nutrition'\")\n\n# Histogram\ndist = sql.histogram(\"cais/mmlu\", \"subject\")\n\n# Filter and transform\nresults = sql.filter_and_transform(\n    \"cais/mmlu\",\n    select=\"subject, COUNT(*) as cnt\",\n    group_by=\"subject\",\n    order_by=\"cnt DESC\",\n    limit=10\n)\n\n# Push to Hub\nurl = sql.push_to_hub(\n    \"cais/mmlu\",\n    \"username/nutrition-subset\",\n    sql=\"SELECT * FROM data WHERE subject='nutrition'\",\n    private=True\n)\n\n# Export locally\nsql.export_to_parquet(\"cais/mmlu\", \"output.parquet\", sql=\"SELECT * FROM data LIMIT 100\")\n\nsql.close()\n```\n\n## HF Path Format\n\nDuckDB uses the `hf://` protocol to access datasets:\n```\nhf://datasets/{dataset_id}@{revision}/{config}/{split}/*.parquet\n```\n\nExamples:\n- `hf://datasets/cais/mmlu@~parquet/default/train/*.parquet`\n- `hf://datasets/ibm/duorc@~parquet/ParaphraseRC/test/*.parquet`\n\nThe `@~parquet` revision provides auto-converted Parquet files for any dataset format.\n\n## Useful DuckDB SQL Functions\n\n```sql\n-- String functions\nLENGTH(column)                    -- String length\nregexp_replace(col, '\\n', '')     -- Regex replace\nregexp_matches(col, 'pattern')    -- Regex match\nLOWER(col), UPPER(col)           -- Case conversion\n\n-- Array functions  \nchoices[0]                        -- Array indexing (0-based)\narray_length(choices)             -- Array length\nunnest(choices)                   -- Expand array to rows\n\n-- Aggregations\nCOUNT(*), SUM(col), AVG(col)\nGROUP BY col HAVING condition\n\n-- Sampling\nUSING SAMPLE 10                   -- Random sample\nUSING SAMPLE 10 (RESERVOIR, 42)   -- Reproducible sample\n\n-- Window functions\nROW_NUMBER() OVER (PARTITION BY col ORDER BY col2)\n```\n\n---\n\n# Dataset Creation (dataset_manager.py)\n\n### Recommended Workflow\n\n**1. Discovery (Use HF MCP Server):**\n```python\n# Use HF MCP tools to find existing datasets\nsearch_datasets(\"conversational AI training\")\nget_dataset_details(\"username/dataset-name\")\n```\n\n**2. Creation (Use This Skill):**\n```bash\n# Initialize new dataset\nuv run scripts/dataset_manager.py init --repo_id \"your-username/dataset-name\" [--private]\n\n# Configure with detailed system prompt\nuv run scripts/dataset_manager.py config --repo_id \"your-username/dataset-name\" --system_prompt \"$(cat system_prompt.txt)\"\n```\n\n**3. Content Management (Use This Skill):**\n```bash\n# Quick setup with any template\nuv run scripts/dataset_manager.py quick_setup \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template classification\n\n# Add data with template validation\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template qa \\\n  --rows_json \"$(cat your_qa_data.json)\"\n```\n\n### Template-Based Data Structures\n\n**1. Chat Template (`--template chat`)**\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Natural user request\"},\n    {\"role\": \"assistant\", \"content\": \"Response with tool usage\"},\n    {\"role\": \"tool\", \"content\": \"Tool response\", \"tool_call_id\": \"call_123\"}\n  ],\n  \"scenario\": \"Description of use case\",\n  \"complexity\": \"simple|intermediate|advanced\"\n}\n```\n\n**2. Classification Template (`--template classification`)**\n```json\n{\n  \"text\": \"Input text to be classified\",\n  \"label\": \"classification_label\",\n  \"confidence\": 0.95,\n  \"metadata\": {\"domain\": \"technology\", \"language\": \"en\"}\n}\n```\n\n**3. QA Template (`--template qa`)**\n```json\n{\n  \"question\": \"What is the question being asked?\",\n  \"answer\": \"The complete answer\",\n  \"context\": \"Additional context if needed\",\n  \"answer_type\": \"factual|explanatory|opinion\",\n  \"difficulty\": \"easy|medium|hard\"\n}\n```\n\n**4. Completion Template (`--template completion`)**\n```json\n{\n  \"prompt\": \"The beginning text or context\",\n  \"completion\": \"The expected continuation\",\n  \"domain\": \"code|creative|technical|conversational\",\n  \"style\": \"description of writing style\"\n}\n```\n\n**5. Tabular Template (`--template tabular`)**\n```json\n{\n  \"columns\": [\n    {\"name\": \"feature1\", \"type\": \"numeric\", \"description\": \"First feature\"},\n    {\"name\": \"target\", \"type\": \"categorical\", \"description\": \"Target variable\"}\n  ],\n  \"data\": [\n    {\"feature1\": 123, \"target\": \"class_a\"},\n    {\"feature1\": 456, \"target\": \"class_b\"}\n  ]\n}\n```\n\n### Advanced System Prompt Template\n\nFor high-quality training data generation:\n```text\nYou are an AI assistant expert at using MCP tools effectively.\n\n## MCP SERVER DEFINITIONS\n[Define available servers and tools]\n\n## TRAINING EXAMPLE STRUCTURE\n[Specify exact JSON schema for chat templating]\n\n## QUALITY GUIDELINES\n[Detail requirements for realistic scenarios, progressive complexity, proper tool usage]\n\n## EXAMPLE CATEGORIES\n[List development workflows, debugging scenarios, data management tasks]\n```\n\n### Example Categories & Templates\n\nThe skill includes diverse training examples beyond just MCP usage:\n\n**Available Example Sets:**\n- `training_examples.json` - MCP tool usage examples (debugging, project setup, database analysis)\n- `diverse_training_examples.json` - Broader scenarios including:\n  - **Educational Chat** - Explaining programming concepts, tutorials\n  - **Git Workflows** - Feature branches, version control guidance\n  - **Code Analysis** - Performance optimization, architecture review\n  - **Content Generation** - Professional writing, creative brainstorming\n  - **Codebase Navigation** - Legacy code exploration, systematic analysis\n  - **Conversational Support** - Problem-solving, technical discussions\n\n**Using Different Example Sets:**\n```bash\n# Add MCP-focused examples\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(cat examples/training_examples.json)\"\n\n# Add diverse conversational examples\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(cat examples/diverse_training_examples.json)\"\n\n# Mix both for comprehensive training data\nuv run scripts/dataset_manager.py add_rows --repo_id \"your-username/dataset-name\" \\\n  --rows_json \"$(jq -s '.[0] + .[1]' examples/training_examples.json examples/diverse_training_examples.json)\"\n```\n\n### Commands Reference\n\n**List Available Templates:**\n```bash\nuv run scripts/dataset_manager.py list_templates\n```\n\n**Quick Setup (Recommended):**\n```bash\nuv run scripts/dataset_manager.py quick_setup --repo_id \"your-username/dataset-name\" --template classification\n```\n\n**Manual Setup:**\n```bash\n# Initialize repository\nuv run scripts/dataset_manager.py init --repo_id \"your-username/dataset-name\" [--private]\n\n# Configure with system prompt\nuv run scripts/dataset_manager.py config --repo_id \"your-username/dataset-name\" --system_prompt \"Your prompt here\"\n\n# Add data with validation\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"your-username/dataset-name\" \\\n  --template qa \\\n  --rows_json '[{\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence...\"}]'\n```\n\n**View Dataset Statistics:**\n```bash\nuv run scripts/dataset_manager.py stats --repo_id \"your-username/dataset-name\"\n```\n\n### Error Handling\n- **Repository exists**: Script will notify and continue with configuration\n- **Invalid JSON**: Clear error message with parsing details\n- **Network issues**: Automatic retry for transient failures\n- **Token permissions**: Validation before operations begin\n\n---\n\n# Combined Workflow Examples\n\n## Example 1: Create Training Subset from Existing Dataset\n```bash\n# 1. Explore the source dataset\nuv run scripts/sql_manager.py describe --dataset \"cais/mmlu\"\nuv run scripts/sql_manager.py histogram --dataset \"cais/mmlu\" --column \"subject\"\n\n# 2. Query and create subset\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT * FROM data WHERE subject IN ('nutrition', 'anatomy', 'clinical_knowledge')\" \\\n  --push-to \"username/mmlu-medical-subset\" \\\n  --private\n```\n\n## Example 2: Transform and Reshape Data\n```bash\n# Transform MMLU to QA format with correct answers extracted\nuv run scripts/sql_manager.py query \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT question, choices[answer] as correct_answer, subject FROM data\" \\\n  --push-to \"username/mmlu-qa-format\"\n```\n\n## Example 3: Merge Multiple Dataset Splits\n```bash\n# Export multiple splits and combine\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --split \"*\" \\\n  --output \"mmlu_all.parquet\"\n```\n\n## Example 4: Quality Filtering\n```bash\n# Filter for high-quality examples\nuv run scripts/sql_manager.py query \\\n  --dataset \"squad\" \\\n  --sql \"SELECT * FROM data WHERE LENGTH(context) > 500 AND LENGTH(question) > 20\" \\\n  --push-to \"username/squad-filtered\"\n```\n\n## Example 5: Create Custom Training Dataset\n```bash\n# 1. Query source data\nuv run scripts/sql_manager.py export \\\n  --dataset \"cais/mmlu\" \\\n  --sql \"SELECT question, subject FROM data WHERE subject='nutrition'\" \\\n  --output \"nutrition_source.jsonl\" \\\n  --format jsonl\n\n# 2. Process with your pipeline (add answers, format, etc.)\n\n# 3. Push processed data\nuv run scripts/dataset_manager.py init --repo_id \"username/nutrition-training\"\nuv run scripts/dataset_manager.py add_rows \\\n  --repo_id \"username/nutrition-training\" \\\n  --template qa \\\n  --rows_json \"$(cat processed_data.json)\"\n```\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hugging-face-datasets\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "hugging-face-datasets",
          "skill_dir": "hugging-face-datasets",
          "skill_file_tree": "SKILL.md\nexamples/diverse_training_examples.json\nexamples/system_prompt_template.txt\nexamples/training_examples.json\nscripts/dataset_manager.py\nscripts/sql_manager.py\ntemplates/chat.json\ntemplates/classification.json\ntemplates/completion.json\ntemplates/custom.json\ntemplates/qa.json\ntemplates/tabular.json",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "Let me analyze this skill document:\n\n1. **What domain knowledge does this skill teach?**\n   - Structure of skills for Claude Code plugins (SKILL.md + bundled resources)\n   - Progressive disclosure design principle (three-level loading: metadata → SKILL.md → bundled resources)\n   - Content organization decisions (what goes in SKILL.md vs references/ vs scripts/ vs assets/)\n   - Writing style requirements (imperative form, third-person descriptions with trigger phrases)\n   - Effective trigger description design\n\n2. **What workflow decisions does it enable?**\n   - When to put content in SKILL.md (~1,500-2,000 words) vs move to references/ (detailed content loaded as needed)\n   - How to differentiate scripts/ (executable code) vs references/ (documentation for context) vs assets/ (output files)\n   - How to write descriptions that trigger appropriately (third-person, specific user phrases)\n\n3. **What would a practitioner struggle with without this skill?**\n   - Organizing content for context efficiency (bloated SKILL.md vs proper progressive disclosure)\n   - Writing trigger descriptions that actually work (vague vs specific, wrong person vs third-person)\n   - Deciding which resource type to use for different content\n\nLet me create a test that focuses on the core design decision of progressive disclosure and content organization:\n\n```yaml\nname: skill-content-organization-and-trigger-design\nskills:\n  - Skill Development\nprompt: |\n  I'm creating a skill for a Claude Code plugin that helps users work with database migrations. The skill needs to cover:\n  \n  1. Core workflow for creating migrations (about 500 words of essential steps)\n  2. Detailed documentation of all migration CLI commands and options (about 3,000 words)\n  3. A Python script that validates migration file syntax\n  4. Common migration patterns and anti-patterns (about 2,500 words)\n  5. A template SQL file users can copy for new migrations\n  \n  I'm planning to put everything in a single SKILL.md file to keep it simple.\n  \n  Two questions:\n  1. What's wrong with my plan, and how should I organize this content across the skill's directory structure?\n  2. Write an improved frontmatter description for this skill. My current description is: \"Use this skill when you need database migration help.\"\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of skill development best practices for Claude Code plugins.\n    \n    **Question 1 - Content Organization (Progressive Disclosure):**\n    A correct answer should identify that:\n    - Putting everything in SKILL.md would bloat it (~6,000+ words when target is 1,500-2,000)\n    - SKILL.md should contain only the core workflow (~500 words of essential steps)\n    - The detailed CLI documentation (~3,000 words) should go in references/ (e.g., references/cli-reference.md)\n    - The migration patterns (~2,500 words) should go in references/ (e.g., references/patterns.md)\n    - The validation script should go in scripts/ (e.g., scripts/validate_migration.py)\n    - The template SQL file should go in assets/ (since it's used in output, not loaded into context)\n    \n    **Question 2 - Trigger Description:**\n    A correct answer should:\n    - Use third-person format (\"This skill should be used when...\")\n    - Include specific trigger phrases users would say (e.g., \"create a migration\", \"run migrations\", \"rollback migration\", \"validate migration\")\n    - Be concrete rather than vague\n    \n    Score 1.0 if: Both questions are answered correctly - content is properly distributed across SKILL.md/references/scripts/assets with appropriate reasoning about context efficiency, AND the description uses third-person with specific trigger phrases.\n    \n    Score 0.5 if: One question is answered well but the other is weak OR partially correct on both (e.g., mentions moving content to references but doesn't distinguish scripts/assets, OR improves description but doesn't use proper third-person format).\n    \n    Score 0.0 if: Fails to address the core issues - doesn't recommend splitting content out of SKILL.md, or provides a description that's still vague/not third-person.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test effectively measures understanding of progressive disclosure (core skill concept) and trigger description design, both transferable knowledge. The grader is robust, accepting various valid organizational approaches and description formats. However, the test is somewhat contrived - asking someone to critique their own flawed plan is less realistic than asking them to organize content from scratch. The test would be stronger if it presented raw content and asked for organization without the leading 'what's wrong' framing."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nBefore writing the test case, let me analyze what this skill teaches:\n\n1. **Core domain knowledge:**\n   - Progressive disclosure architecture (metadata → SKILL.md body → bundled resources)\n   - Resource type taxonomy: scripts (executable), references (documentation), assets (output files)\n   - Writing conventions: imperative form for instructions, third-person for triggers\n   - Size management: keeping SKILL.md lean (1,500-2,000 words) by moving details to references\n\n2. **Workflow decisions it enables:**\n   - When to use references/ vs putting content in SKILL.md (progressive disclosure)\n   - How to write triggering descriptions (specific user phrases vs vague descriptions)\n   - When to create scripts vs references vs assets (execution vs documentation vs output)\n   - How to organize information across files for efficient context loading\n\n3. **What practitioners struggle with:**\n   - Writing vague descriptions that don't trigger at the right time\n   - Bloating SKILL.md with all content instead of using references\n   - Using second person (\"you should\") instead of imperative form\n   - Not understanding when to create scripts vs references vs assets\n   - Failing to reference supporting files so Claude doesn't know they exist\n\n**Test target:** Give a flawed skill structure and ask for improvements. The model should identify violations of progressive disclosure, trigger design, writing style, and resource organization.\n\n```yaml\nname: skill-architecture-critique\nskills:\n  - Skill Development\nprompt: |\n  You're reviewing a skill that another developer created for a Claude Code plugin. Here's what they have:\n  \n  **Skill structure:**\n  ```\n  database-helper/\n  └── SKILL.md  (6,500 words)\n  ```\n  \n  **SKILL.md frontmatter:**\n  ```yaml\n  ---\n  name: Database Helper\n  description: Use this skill when you need help with databases.\n  version: 0.1.0\n  ---\n  ```\n  \n  **SKILL.md body excerpt:**\n  ```markdown\n  # Database Helper\n  \n  You should start by understanding your database schema. When you're working with databases,\n  you'll need to know the table structures. You can find this information by querying the \n  information schema.\n  \n  Here are all 50+ database schemas with full field definitions (4,000 words of schema details \n  embedded directly in SKILL.md...)\n  \n  You should also validate your queries before running them. Here's validation code you should \n  use (500 lines of Python code inline...)\n  \n  When you need to generate reports, you should follow these steps...\n  ```\n  \n  Identify at least 4 significant problems with this skill's design and suggest specific improvements.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should identify problems with the skill design based on skill development best practices.\n    \n    Key issues to identify:\n    1. **Vague trigger description** - \"when you need help with databases\" is not specific, should include concrete user phrases like \"query the database\", \"check schema\"\n    2. **Wrong person in description** - Uses \"Use this skill when you\" instead of third-person \"This skill should be used when the user asks to...\"\n    3. **Bloated SKILL.md** - 6,500 words with schemas and code inline violates progressive disclosure; should be ~1,500-2,000 words\n    4. **Missing progressive disclosure** - Schema details should be in references/, validation code should be in scripts/\n    5. **Second person writing** - Body uses \"You should\" instead of imperative form (\"Start by understanding...\")\n    6. **No resource organization** - Everything in one file, should have references/ and scripts/ directories\n    \n    Score 1.0 if:\n    - Identifies at least 4 distinct, significant problems\n    - Shows understanding of progressive disclosure (content too large/detailed for SKILL.md)\n    - Mentions trigger description being vague or needing specific phrases\n    - Identifies writing style issues (second person usage)\n    - Provides concrete, actionable improvements (move schemas to references/, create scripts/)\n    \n    Score 0.5 if:\n    - Identifies 2-3 problems correctly\n    - Shows partial understanding of progressive disclosure or trigger design\n    - Suggests some improvements but lacks specificity\n    - Misses some major issues like writing style or resource organization\n    \n    Score 0.0 if:\n    - Identifies fewer than 2 significant problems\n    - Focuses only on superficial issues (typos, formatting)\n    - Shows no understanding of progressive disclosure, trigger design, or writing conventions\n    - Suggests incorrect changes that would make the skill worse\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively evaluates transferable skill development knowledge (progressive disclosure, trigger design, writing conventions) rather than trivia. The grader is robust and accepts multiple valid problem identifications with clear scoring criteria. However, there's minor brittleness in requiring exactly 4+ problems when the rubric lists 6 distinct issues - a response identifying 3 critical problems deeply might be penalized despite demonstrating strong understanding. The task is highly realistic as reviewing and improving poorly-structured skills is a common practitioner activity."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what practitioners actually need to know:\n\n## Analysis\n\n**Domain Knowledge:** The skill teaches how to create effective skills for Claude Code plugins, focusing on:\n- Progressive disclosure design (metadata → SKILL.md → bundled resources)\n- Organizational structure (what goes in scripts/ vs references/ vs assets/ vs examples/)\n- Writing style requirements (imperative/infinitive form, third-person descriptions)\n- Validation and common mistakes\n- The 6-step skill creation process\n\n**Workflow Decisions It Enables:**\n- When to extract content from SKILL.md into references/\n- Whether to use scripts, references, or assets based on use case\n- How to write effective trigger phrases\n- Understanding why progressive disclosure matters for token efficiency\n- Recognizing when content should be inline vs bundled\n\n**Practitioner Struggles Without This Skill:**\n- Creating skills with bloated SKILL.md files (>5k words)\n- Writing vague descriptions that don't trigger correctly\n- Using second-person voice instead of imperative form\n- Not understanding when to use each bundled resource type\n- Missing the validation checklist and common mistakes section\n\nThe best test would evaluate whether an LLM can make practical structural and organizational decisions about a realistic skill development scenario.\n\n```yaml\nname: skill-organization-and-structure-decisions\nskills:\n  - Skill Development\nprompt: |\n  You're building a new skill for a Claude Code plugin. Users will ask questions like:\n  - \"How do I validate my database schema?\"\n  - \"Show me an example of a valid schema\"\n  - \"What's the full reference for all schema fields?\"\n  - \"Validate this schema file for me\"\n  \n  You've identified that:\n  1. A comprehensive schema reference document exists (8,000+ words of detailed field definitions)\n  2. A validation script is repeatedly rewritten by users\n  3. A few complete working examples exist\n  4. The core workflow (understanding what validation means in your context) takes ~1,500 words\n  \n  Provide:\n  1. The directory structure you would create for this skill (show the layout)\n  2. For each file/directory, briefly explain what would go there and why\n  3. Write the description field for SKILL.md frontmatter (the 2-3 sentence description that tells Claude when to load this skill)\n  4. Explain one organizational decision you made using the progressive disclosure principle\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of skill organization and progressive disclosure. Look for:\n\n    **Structure Quality (Is the directory layout sound?):**\n    - Skill includes SKILL.md as required\n    - Large reference content (8,000+ words) placed in references/ directory (NOT in SKILL.md)\n    - Reusable validation script placed in scripts/ directory\n    - Working examples placed in examples/ directory\n    - Shows understanding that SKILL.md should be lean (<2,000 words), not comprehensive\n\n    **Description Quality (Are trigger phrases effective?):**\n    - Uses third-person format (\"This skill should be used when...\")\n    - Includes specific, concrete trigger phrases users would actually say (\"validate schema\", \"schema reference\", etc.)\n    - Not vague or generic\n    - Appropriate length (2-3 sentences)\n\n    **Progressive Disclosure Understanding:**\n    - Explains why large reference content goes in references/ vs SKILL.md\n    - Discusses token efficiency or loading only what's needed\n    - Shows understanding that scripts can be executed without loading into context\n    - Demonstrates concept that SKILL.md triggers the skill, then references load as needed\n\n    **Writing Style:**\n    - If SKILL.md content is shown, uses imperative/infinitive form (not second person)\n    - Shows awareness that instructions should be verb-first\n\n    Score 1.0 if: Response demonstrates all four areas above. Directory structure is appropriate, description uses third-person with specific triggers, progressive disclosure principle is correctly applied, and reasoning shows understanding of why decisions were made.\n\n    Score 0.5 if: Response gets most of the structure right but misses one element (e.g., good directory structure but weak description, or good description but misses bundling large references), or shows partial understanding of progressive disclosure without explaining it clearly.\n\n    Score 0.0 if: Response puts most/all content in SKILL.md (missing progressive disclosure), uses second-person voice, lacks specific trigger phrases, or shows fundamental misunderstanding of skill organization.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable domain knowledge about progressive disclosure, organizational structure, and trigger phrase design—concepts that apply broadly to modular documentation systems. The grader is robust, accepting multiple valid approaches while checking for understanding of core principles (lean SKILL.md, references/ for detailed content, third-person triggers). The scenario is realistic: practitioners genuinely need to decide how to organize skill content. Minor deduction: the test could be slightly more open-ended to avoid guiding toward specific answers (e.g., explicitly mentioning '8,000+ words' might cue the answer), and the grader is somewhat verbose but still functional."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "## Analysis\n\nLooking at this skill document, I need to identify what domain knowledge it teaches and what practitioners would struggle with without it:\n\n1. **What domain knowledge does this skill teach?** \n   - The progressive disclosure design principle for managing context efficiently (metadata → SKILL.md → bundled resources)\n   - Proper skill structure with SKILL.md and optional resource directories\n   - Writing style requirements (imperative form for body, third-person for descriptions)\n   - When to use different resource types (scripts for repeated code, references for documentation, assets for output files)\n   - Keeping SKILL.md lean (1,500-2,000 words) while organizing detailed content\n\n2. **What workflow decisions does it enable?**\n   - When to move content from SKILL.md to references/ (detailed docs vs core concepts)\n   - How to write trigger descriptions that ensure skills load when needed\n   - Whether to bundle scripts vs write inline code\n   - How to organize resources for progressive disclosure\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - Creating bloated SKILL.md files that waste context\n   - Writing ineffective trigger descriptions that don't activate the skill\n   - Using wrong writing style (second person instead of imperative)\n   - Not understanding how to organize resources effectively\n   - Missing the progressive disclosure pattern entirely\n\n## Test Case\n\n```yaml\nname: skill-progressive-disclosure\nskills:\n  - Skill Development\nprompt: |\n  I'm creating a skill called \"database-migration\" for a Claude Code plugin that helps developers migrate between different database systems (PostgreSQL, MySQL, MongoDB). \n\n  The skill needs to include:\n  - Core migration concepts and workflow\n  - Detailed migration patterns for each database type (about 3,000 words each)\n  - Schema conversion utilities (Python scripts)\n  - Example migration configurations\n  - Template SQL files for common migrations\n\n  Create the skill structure and write the SKILL.md file. Show me:\n  1. The directory structure you would create\n  2. The complete SKILL.md file with proper frontmatter\n  3. An explanation of what content goes in each resource file\n\n  Focus on demonstrating proper progressive disclosure and organization.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the response demonstrates proper skill development practices according to the progressive disclosure principle.\n\n    Score 1.0 if ALL of the following are present:\n    - Directory structure includes SKILL.md and appropriate subdirectories (references/, scripts/, examples/ or assets/ as needed)\n    - SKILL.md frontmatter has third-person description with specific trigger phrases like \"migrate database\", \"convert schema\", etc.\n    - SKILL.md body uses imperative/infinitive form (not \"you should\" but \"To migrate...\" or \"Configure...\")\n    - SKILL.md is kept lean (indicates ~1,500-2,000 words or explicitly states keeping it concise)\n    - Detailed migration patterns are moved to references/ files (not kept in SKILL.md)\n    - Python utilities are placed in scripts/ directory\n    - Example configurations go in examples/\n    - Template SQL files go in assets/ or examples/\n    - Clear explanation of progressive disclosure (metadata always loaded, SKILL.md when triggered, resources as needed)\n\n    Score 0.5 if MOST (6-8) of the above criteria are met but some key aspects are missing, such as:\n    - Uses correct structure but SKILL.md is too verbose\n    - Good organization but uses wrong writing style\n    - Understands progressive disclosure but implements it imperfectly\n\n    Score 0.0 if:\n    - Puts all content in SKILL.md without using references/\n    - Uses second person writing style throughout\n    - Missing proper trigger descriptions\n    - Shows no understanding of progressive disclosure\n    - Creates inappropriate structure for the resources\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively measures understanding of progressive disclosure and skill organization principles rather than trivia. The grader is robust, accepting multiple valid implementations while checking for core concepts (lean SKILL.md, proper resource organization, imperative writing style). The task is realistic—organizing complex documentation is a genuine challenge practitioners face. Minor deduction: the grader could be slightly more flexible about exact word counts and might reject some valid organizational choices, but overall it tests transferable knowledge about information architecture and context management rather than memorizing specific details from the skill document."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain-skill analysis (what this skill teaches and what to test)\n\n1) Domain knowledge: How to design “skills” for Claude Code plugins as modular onboarding guides, including the anatomy (SKILL.md + optional scripts/references/assets), progressive disclosure (metadata vs SKILL.md vs resources), and writing conventions (third-person trigger description; imperative/infinitive body).\n\n2) Workflow decisions enabled: Choosing what belongs in SKILL.md vs references/ vs scripts/ vs assets/; crafting strong trigger phrases so the skill auto-loads at the right time; keeping SKILL.md lean while making deeper material discoverable; validating/testing a plugin skill in-place under `skills/`.\n\n3) What practitioners struggle with without it: They write vague descriptions that don’t trigger; bloat SKILL.md with everything; use inconsistent/second-person prose that’s harder for an LLM to follow; forget to reference/organize resources; omit validation/testing steps and plugin-specific placement.\n\n```yaml\nname: design-plugin-skill-with-progressive-disclosure-and-triggers\nskills:\n- Skill Development\nprompt: |\n  You are maintaining a Claude Code plugin repo and want to add a new skill to help with a recurring, real workflow:\n\n  **Skill goal:** “PostgreSQL performance triage” for on-call engineers. Typical user requests include:\n  - Investigating sudden latency spikes\n  - Finding slow queries and missing indexes\n  - Checking locks, deadlocks, and long-running transactions\n  - Capturing diagnostics safely (read-only, minimal impact)\n  - Producing a concise incident summary with next steps\n\n  Task:\n  1) Propose the **plugin skill directory structure** under `my-plugin/skills/pg-perf-triage/` (only create directories you will actually use).\n  2) Draft a **SKILL.md YAML frontmatter** (name, description, version) where the description is third-person and contains **specific trigger phrases users would say** (quote them).\n  3) Draft the **SKILL.md body** as a *lean* set of instructions (do not write a giant document). Requirements:\n     - Use **imperative/infinitive form** (verb-first instructions) and avoid second person.\n     - Include a short “Purpose” and a “Workflow” section with a realistic multi-step triage flow.\n     - Include an “Additional Resources” section that explicitly references any `references/`, `examples/`, and/or `scripts/` files you propose.\n  4) List what content should go into **references vs scripts vs examples** for this skill, and briefly justify each choice (e.g., deterministic utilities vs large documentation vs runnable examples).\n  5) Provide a short **validation/testing checklist** specific to plugin skills (triggering, structure, style, references existence).\n\n  Keep the response practical and implementation-ready.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates strong plugin-skill design per best practices (triggering, progressive disclosure, organization, and writing style), not mere generic PostgreSQL advice.\n\n    Score 1.0 if the response includes ALL of the following:\n    - Proposes a correct plugin skill location/structure under `my-plugin/skills/pg-perf-triage/` and only includes directories it intends to use (e.g., references/, scripts/, examples/ as needed).\n    - Provides SKILL.md YAML frontmatter with at least name + description + version.\n    - Description is explicitly third-person and uses wording equivalent to “This skill should be used when…” AND includes 3+ concrete quoted trigger phrases (e.g., “find slow queries”, “check locks”, “latency spike”) that would plausibly cause the skill to load.\n    - SKILL.md body is lean and written predominantly in imperative/infinitive form (verb-first). It should NOT be primarily second-person (“you should/you can”); occasional unavoidable instances are acceptable but must not dominate.\n    - Includes a realistic multi-step triage workflow (e.g., safety checks, gather symptoms, inspect slow queries/locks, capture diagnostics, summarize findings) and emphasizes safe/read-only/minimal impact practices.\n    - Demonstrates progressive disclosure by explicitly allocating detailed material to references/ and pointing to it from SKILL.md (“Additional Resources” section), rather than embedding everything in the main body.\n    - Clearly differentiates what belongs in scripts vs references vs examples, with sensible justification (scripts for deterministic recurring commands/collection, references for large docs/query catalog/patterns, examples for runnable sample commands/reports).\n    - Includes a plugin-skill validation/testing checklist that covers: frontmatter presence, trigger phrase quality, imperative style, referenced files exist, and testing that the skill triggers/loads when expected.\n\n    Score 0.5 if the response is directionally correct but missing 2-3 of the full-credit elements (e.g., weak/vague triggers, no explicit Additional Resources pointers, mixed writing style, unclear separation of references/scripts/examples, or incomplete validation checklist).\n\n    Score 0.0 if the response fails to present a plugin-skill design (e.g., no frontmatter, no trigger phrases, no directory/resource plan) or is mostly generic PostgreSQL performance tips without skill-structure/progressive-disclosure considerations.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of skill development principles by testing progressive disclosure, trigger phrase design, and resource organization—all transferable domain knowledge. The grader is comprehensive and accepts multiple valid approaches while checking for genuine skill-design competence rather than trivia. The task is highly realistic (creating a triage skill for on-call engineers). Minor deductions: the test is somewhat complex with 8 full-credit requirements that could make grading subjective, and the 0.5 partial credit band ('missing 2-3 elements') is broad. However, the test effectively measures whether the model understands the core skill-development methodology rather than memorizing arbitrary details."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain analysis (what this skill teaches and what to test)\n1. Core domain knowledge: How to design “skills” for Claude Code plugins as modular onboarding guides, with strong trigger metadata, imperative writing style, and progressive disclosure across SKILL.md vs references/ vs scripts/ vs assets/.\n2. Workflow decisions enabled: Choosing what belongs in SKILL.md (core procedure) vs references/ (detailed docs) vs scripts/ (deterministic/reusable automation) vs examples/ (copyable working configs). Also deciding how to write metadata so the skill reliably triggers (third-person + concrete quoted phrases).\n3. Practitioner struggle without the skill: People commonly write vague descriptions, dump everything into SKILL.md, use inconsistent second-person prose, forget to reference bundled resources, and omit a validation/testing plan—leading to skills that don’t trigger, bloat context, or aren’t reusable.\n\n```yaml\nname: refactor-plugin-skill-for-progressive-disclosure\nskills:\n  - Skill Development\nprompt: |\n  You are helping maintain a Claude Code plugin named `oncall-helper`. A teammate drafted a new plugin skill, but it is not triggering reliably and it bloats context.\n\n  Below is the current draft `SKILL.md` (abridged). It is too vague, uses second person, and mixes long reference material directly into the body.\n\n  --- BEGIN DRAFT ---\n  ---\n  name: Incident Response\n  description: Help with incidents and on-call.\n  version: 0.1.0\n  ---\n\n  # Incident Response Skill\n\n  You should use this skill when you need help dealing with incidents. You can start by checking dashboards and then you should look at logs. If you need to notify people you can message the on-call channel.\n\n  ## Runbook (very long)\n  - Our severity levels:\n    - SEV0: complete outage affecting all customers\n    - SEV1: major feature down\n    - SEV2: partial degradation\n    - SEV3: minor issue\n  - Required incident document sections:\n    - Summary, Impact, Timeline, Root Cause, Corrective Actions\n  - Communication templates:\n    - \"We are investigating...\"\n    - \"We have identified the cause...\"\n  - Escalation policy:\n    - Page SRE then Backend then Product...\n  - Postmortem checklist:\n    - Gather graphs, annotate, create timeline, identify action items...\n  --- END DRAFT ---\n\n  The team has shared 4 concrete example user requests that this skill should cover:\n  1) \"Triage this alert: elevated 500s in api-gateway. What should I do first?\"\n  2) \"Create an incident timeline from these log lines and deploy events.\"\n  3) \"Draft a customer-facing status update for a SEV1 payments degradation.\"\n  4) \"Validate this incident JSON against our required fields and point out what's missing.\"\n\n  Also: (a) engineers repeatedly rewrite small scripts to validate incident JSON and (b) repeatedly format timelines into Markdown.\n\n  Task:\n  Propose a refactor that follows Claude Code plugin skill best practices. Provide:\n\n  1) A recommended plugin skill directory structure under `oncall-helper/skills/incident-response/` (show a tree) including which of `references/`, `examples/`, `scripts/`, `assets/` you would create (only include what you would actually use).\n  2) A rewritten YAML frontmatter where the description is third-person and includes several specific quoted trigger phrases drawn from the example requests above (or close paraphrases).\n  3) A rewritten opening section of the SKILL.md body (about 10-20 lines) written in imperative/infinitive instructional style (no second person), that stays lean and points to any reference/script/example files you propose.\n  4) A short validation/test checklist (5-8 bullets) specific to plugin skills (triggering, style, references exist, scripts executable, etc.).\n\n  Do NOT write the full runbook. Instead, demonstrate progressive disclosure by moving detailed policies/templates/checklists into references and pointing to them from SKILL.md.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical “Skill Development” competence for Claude Code plugin skills (strong triggers + progressive disclosure + reusable resources).\n\n    Score 1.0 if the answer includes all of the following:\n    - A plausible directory tree rooted at `oncall-helper/skills/incident-response/` that uses progressive disclosure (SKILL.md plus at least one of references/scripts/examples as justified), and does not propose unnecessary folders without rationale.\n    - Rewritten YAML frontmatter with:\n      * third-person phrasing (\"This skill should be used when...\")\n      * multiple concrete quoted trigger phrases that clearly align with the 4 example requests (triage alert, create timeline, draft status update, validate incident JSON).\n    - A lean SKILL.md opening section written in imperative/infinitive style (verb-first instructions), avoiding second-person wording (\"you/your\") for the instructions, and explicitly referencing the proposed bundled resources (e.g., references files and scripts) so another model knows they exist and when to load/use them.\n    - A validation/test checklist that mentions plugin-skill concerns such as: frontmatter presence, trigger phrase quality, writing style check, SKILL.md kept lean with details in references, referenced files exist, scripts are executable/working, and testing that the skill triggers via example user queries (e.g., in a local plugin install).\n\n    Score 0.5 if the answer is directionally correct but misses one major element (e.g., weak/vague trigger phrases, no scripts despite the repeated deterministic tasks, no explicit resource referencing, or body still largely second person), while still showing some understanding of progressive disclosure and plugin skill structure.\n\n    Score 0.0 if the answer largely ignores the requested refactor structure, keeps everything in SKILL.md, provides a vague non-triggering description, or fails to distinguish references vs scripts vs examples and does not address imperative writing style.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of skill development principles by testing progressive disclosure, trigger phrase design, and imperative writing style through a realistic refactoring scenario. The grader is robust and accepts multiple valid approaches while checking for core competencies. However, the test is somewhat lengthy and complex, which may make it harder to isolate specific skill knowledge failures, and the grading criteria could be slightly more precise about what constitutes 'plausible' directory structures or 'adequate' trigger phrases."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis\n# 1) Domain knowledge taught:\n#    - How to design high-quality, modular “skills” for Claude Code plugins: metadata-triggering,\n#      lean procedural guidance, and packaging reusable resources (references/examples/scripts/assets)\n#      using progressive disclosure.\n# 2) Workflow decisions enabled:\n#    - Decide what belongs in SKILL.md vs references/ vs scripts/ vs assets/ (and why).\n#    - Write metadata that reliably triggers skill loading (third-person + concrete user phrases).\n#    - Author instructions in imperative/objective style for AI consumption.\n#    - Validate/test a plugin skill so it is discoverable, triggers correctly, and doesn’t bloat context.\n# 3) Practitioner struggle without this skill:\n#    - Skills that don’t trigger (vague descriptions), load too much context (schema/policy dumps),\n#      have unclear/“chatty” second-person prose, and omit or misplace reusable resources—leading to\n#      inconsistent outcomes and poor performance in real plugin usage.\n\nname: incident-status-updates-skill-design\nskills:\n  - Skill Development\nprompt: |\n  You maintain a Claude Code plugin repo named `acme-ops`. Users keep asking Claude Code for help\n  writing incident communications, but responses are inconsistent and sometimes miss required fields.\n\n  Create a new plugin skill named `incident-status-updates` to standardize incident comms.\n\n  Users commonly ask things like:\n  - \"Draft a Slack update for incident INC-123 (API 500s, mitigation in progress)\"\n  - \"Write a customer-facing status page update for database latency\"\n  - \"Create a postmortem outline for last night's outage\"\n  - \"What should be included in an incident update?\"\n  - \"Turn these notes into an executive incident summary\"\n\n  Your company has long internal docs that should NOT be pasted into SKILL.md:\n  - Incident Communications Policy (~12k words)\n  - Approved wording/tone guide (~8k words)\n  - A postmortem document template\n\n  Deliverables (in a single response):\n  A) A recommended directory tree under `acme-ops/skills/incident-status-updates/` (include only the\n     folders/files you would actually create).\n  B) The complete contents of `SKILL.md` (frontmatter + body). Keep it workflow-focused and point to\n     additional resources rather than embedding long policies.\n  C) A short list of additional bundled resource files you would add (references/examples/scripts/assets),\n     with a one-line purpose for each (do NOT write their full contents).\n  D) A quick validation/testing checklist to ensure the skill is discoverable, triggers when expected,\n     and stays efficient to load in a plugin context.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates best-practice Claude Code plugin skill development.\n\n    Score 1.0 if the response includes ALL of the following:\n    - (A) A plausible plugin skill directory tree rooted at `acme-ops/skills/incident-status-updates/`\n      that includes `SKILL.md` and at least one appropriate bundled-resource directory (e.g., references/,\n      examples/, scripts/, assets/) consistent with the prompt’s “long internal docs/template” constraint.\n    - (B) A complete SKILL.md with valid YAML frontmatter containing `name:` and `description:`.\n      The description is written in third person (e.g., \"This skill should be used when...\") and contains\n      multiple concrete trigger phrases that resemble real user queries (at least ~4; quoted or clearly\n      delineated). It is not vague.\n    - (B) SKILL.md body is predominantly imperative/objective procedural guidance (verb-first style),\n      NOT written primarily in second person (\"you/your\"). Minor incidental second-person occurrences are\n      acceptable only in quoted example user prompts.\n    - (B+C) Demonstrates progressive disclosure: does NOT paste long policy text; instead, it points to\n      separate reference/template files. SKILL.md includes an “Additional Resources” (or similar) section\n      that references specific file paths that match the proposed tree/resource list.\n    - (C) Provides a categorized list of additional resources (references/examples/scripts/assets) with\n      clear purposes, showing correct “what goes where” reasoning (policies/tone in references; templates\n      as assets or examples; scripts only if deterministic/repeated automation is warranted).\n    - (D) Validation/testing checklist includes plugin-skill-relevant checks: frontmatter present, trigger\n      phrases quality, referenced files exist, writing style consistency, progressive disclosure/context\n      efficiency, and a realistic way to test triggering in a plugin run (local install/run and asking\n      trigger-like queries).\n\n    Score 0.5 if the response is directionally correct but misses one or two key best-practice elements,\n    such as: vague/non-third-person description, too few/no concrete trigger phrases, insufficient resource\n    separation (still dumps policies), weak/absent resource cross-references, or a generic (non-plugin-specific)\n    validation checklist.\n\n    Score 0.0 if it omits major deliverables (no tree or no SKILL.md), lacks YAML frontmatter, provides a\n    vague description with no realistic triggers, primarily uses second-person prose, or ignores progressive\n    disclosure by embedding large policy content directly in SKILL.md.\n\n    Be flexible about exact filenames and folder choices as long as the structure and reasoning align with\n    plugin skill best practices and the response is internally consistent.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of skill development principles by testing progressive disclosure, trigger phrase design, and resource organization—all transferable domain knowledge. The grader is comprehensive and accepts multiple valid approaches (flexible about filenames/structure). However, the task is somewhat contrived (creating a skill from scratch with specific constraints) rather than a common real-world scenario practitioners would encounter organically, and the grader's complexity with multiple sub-criteria could potentially miss some valid solutions that take different but equally valid approaches to skill organization."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "## Analysis\n\n**Domain Knowledge:** This skill teaches how to create effective skills for Claude Code plugins, focusing on progressive disclosure design, skill anatomy, and the skill creation process.\n\n**Workflow Decisions:** When to use scripts vs references vs assets, what content belongs in SKILL.md vs bundled resources, how to write trigger descriptions, and when to use imperative vs second-person writing.\n\n**Test Target:** Whether the model can apply progressive disclosure principles and skill development best practices to structure a skill properly, including writing appropriate trigger descriptions and organizing content across SKILL.md and bundled resources.\n\n```yaml\nname: skill-development-progressive-disclosure\nskills:\n  - Skill Development\nprompt: |\n  You are helping a developer create a new skill for their Claude Code plugin called \"database-optimizer\". The skill will help users optimize database queries, create indexes, and analyze query performance.\n\n  Based on best practices for skill development, help them:\n  1. Write a proper SKILL.md frontmatter description with specific trigger phrases\n  2. Plan what content should go in SKILL.md vs bundled resources\n  3. Suggest what scripts, references, and assets would be helpful\n  4. Explain the rationale for your organizational decisions\n\n  The developer wants to create a comprehensive skill that includes:\n  - Query optimization techniques\n  - Index creation scripts\n  - Performance analysis tools\n  - Company-specific database schemas\n  - Common optimization patterns\n  - API reference documentation\n  \n  Focus on applying progressive disclosure principles and skill structure best practices.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of skill development best practices, specifically:\n\n    Score 1.0 if the response:\n    - Writes frontmatter description using third-person format (\"This skill should be used when...\")\n    - Includes specific trigger phrases that users would actually say\n    - Demonstrates understanding of progressive disclosure (what goes in SKILL.md vs bundled resources)\n    - Correctly identifies when to use scripts, references, and assets\n    - Uses imperative/infinitive form (verb-first instructions), not second person\n    - Provides logical rationale for content organization\n    - Shows understanding that detailed content should be moved to references/\n\n    Score 0.5 if the response:\n    - Shows partial understanding of progressive disclosure\n    - Has some correct elements but makes significant organizational errors\n    - Uses some second-person language instead of imperative form\n    - Includes trigger phrases but they are vague or not specific enough\n    - Makes reasonable organizational suggestions but lacks clear rationale\n\n    Score 0.0 if the response:\n    - Uses second-person writing throughout (\"You should...\", \"You need to...\")\n    - Treats all content as equally important for SKILL.md\n    - Provides vague or generic trigger descriptions\n    - Shows no understanding of when to use scripts, references, vs assets\n    - Lacks progressive disclosure principles\n    - Doesn't demonstrate skill development knowledge\n\n    Focus on evaluating the APPLICATION of skill development principles, not specific technical details about databases.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "This test case effectively evaluates transferable knowledge about progressive disclosure, content organization, and skill structure principles that apply broadly to documentation and modular system design. The grader is robust, accepting multiple valid approaches while focusing on core principles (third-person descriptions, imperative form, progressive disclosure) rather than exact strings. The task is realistic—developers genuinely need to organize documentation and decide what content belongs where. Minor deduction: the test could be slightly more challenging by requiring the model to identify a subtle mistake or edge case, and the grader could be more explicit about partial credit scenarios."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "**Analysis**\n\nThe Skill Development doc centers on designing effective Claude Code plugin skills. Core knowledge includes: defining precise third-person trigger metadata, enforcing imperative writing style, and organizing content via progressive disclosure (SKILL.md vs references/examples/scripts/assets). Workflow decisions revolve around when to gather concrete use cases, how to plan reusable resources (scripts for repeated code, references for detailed docs, assets for output templates), and how to validate skill structure and triggers. Practitioners without this skill would struggle to scope skills properly, misplace information (bloated SKILL.md or missing references), forget trigger phrasing, or skip validation steps—leading to poorly discoverable or unusable skills. A good test should therefore require planning a new skill end-to-end, emphasizing trigger descriptions, resource allocation, and validation workflow rather than trivia.\n\n```yaml\nname: plan-data-quality-skill\nskills:\n  - Skill Development\nprompt: |\n  You have been asked to create a new Claude Code plugin skill called \"data-quality-audit\" that helps analysts run repeatable data quality checks on warehouse tables.\n\n  Produce a concise deliverable covering ALL of the following:\n  1. **Frontmatter description draft**: Provide the `description` line written in third person with at least three concrete trigger phrases analysts might say.\n  2. **Resource plan**: Identify which reusable assets belong in `scripts/`, `references/`, and `assets/`, explaining why each belongs there (use progressive disclosure reasoning).\n  3. **SKILL.md outline**: Give a bullet outline of the major sections you would include in the body, written in imperative/infinitive style, and highlight how it stays within the 1,500–2,000 word target.\n  4. **Validation checklist**: List the specific checks you will perform before shipping this skill, referencing the plugin-specific validation best practices from the skill doc.\n\n  Make sure your response reflects the methodology described in the Skill Development guide; answers that ignore trigger phrasing, resource structuring, or validation guidance will not receive full credit.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    The response should demonstrate mastery of the Skill Development guidance.\n\n    Score 1.0 if:\n    - Frontmatter description is in third person, includes at least three realistic trigger phrases.\n    - Resource plan clearly assigns items to scripts/references/assets with justification tied to progressive disclosure.\n    - SKILL.md outline lists imperative-style sections and mentions keeping the body lean (~1.5–2k words).\n    - Validation checklist references multiple plugin-specific checks (structure, triggers, style, resources, testing).\n    \n    Score 0.5 if:\n    - Only two of the four required components are fully addressed, or all four appear but with noticeable errors (e.g., vague triggers, missing justification, no mention of word target, or incomplete validation steps).\n    \n    Score 0.0 if:\n    - Fails to provide the required components or shows no understanding of the Skill Development practices.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong understanding of Skill Development principles by requiring candidates to apply trigger phrasing, progressive disclosure, imperative style, and validation workflows. However, it has moderate clarity issues: the multi-part deliverable format (four distinct components) creates brittleness where partial responses could be valid but scored harshly, and the grader's 0.5 threshold ('only two of four') is ambiguous about which combinations qualify. The test is realistic for plugin developers but could be more robust by accepting alternative valid approaches to demonstrating the same knowledge."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: design-plugin-skill-structure\nskills:\n  - Skill Development\nprompt: |\n  I'm building a Claude Code plugin and need to add a 'Legacy Code Refactor' skill. \n  \n  I currently have these resources:\n  1. A 15,000-word document detailing specific COBOL-to-Python migration patterns.\n  2. A Python script called `verify_syntax.py` that validates the converted code.\n  3. A standard operating procedure (SOP) checklist for the refactoring workflow.\n\n  I was planning to paste all of this into a single `SKILL.md` file so the AI has all the context.\n  \n  Please correct my approach by proposing the optimal file directory structure for this skill and writing the `SKILL.md` file content (including the YAML frontmatter) following the \"Progressive Disclosure\" design principle and best practices for writing style.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the \"Skill Development\" best practices.\n\n    Score 1.0 if the response meets all criteria:\n    1. **Progressive Disclosure (Structure)**: Explicitly advises AGAINST putting the 15,000-word document in SKILL.md. It MUST place this document in a `references/` subdirectory (e.g., `references/patterns.md`).\n    2. **Scripts**: Places `verify_syntax.py` in a `scripts/` subdirectory.\n    3. **Frontmatter Description**: The `SKILL.md` YAML frontmatter `description` field uses the **third-person** format (e.g., \"This skill should be used when...\") AND includes specific **trigger phrases** (e.g., \"refactor legacy code\", \"convert COBOL\").\n    4. **Writing Style**: The body of the `SKILL.md` uses **imperative/infinitive** form (e.g., \"Validate the syntax,\" not \"You should validate...\").\n\n    Score 0.5 if:\n    - The directory structure is correct (uses `references/` and `scripts/`), BUT the frontmatter description is in the second person (e.g., \"Use this skill to...\") or vague.\n    - OR the structure is correct, but the writing style is chatty/second-person (\"You should...\").\n\n    Score 0.0 if:\n    - It encourages putting the 15k word document directly into SKILL.md.\n    - It fails to use the plugin skill directory structure (`skills/skill-name/...`).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test effectively measures understanding of progressive disclosure (moving 15k words to references/), script organization, and writing style conventions—all transferable skill development principles. However, the grader has moderate brittleness: it requires specific directory names ('references/', 'scripts/') and exact phrasing patterns, which could reject valid alternatives like 'docs/' or slightly different frontmatter wording that still follows third-person convention. The task is realistic (organizing skill resources) but somewhat contrived as a setup specifically designed to test the skill rather than a natural practitioner need."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "## Analysis: Skill Development\n\n1.  **What domain knowledge does this skill teach?**\n    The skill focuses on the architecture and optimization of \"Skills\" for Claude Code plugins. The core concept is **Progressive Disclosure**: managing the AI's limited context window by stratifying information into metadata (triggers), core procedural instructions (`SKILL.md`), and detailed supplementary resources (references, scripts, assets). It also emphasizes a specific linguistic style (imperative/verb-first) optimized for AI-to-AI instruction.\n\n2.  **What workflow decisions does it enable?**\n    It enables the developer to decide *where* information belongs. Instead of dumping all documentation into a single file, the developer must decide:\n    *   Does this need to be always available (Metadata/Triggers)?\n    *   Is this a core workflow (SKILL.md)?\n    *   Is this a detailed reference or an edge case (References)?\n    *   Is this a repetitive code task (Scripts)?\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner without this skill would likely create \"context-bloated\" skills that consume too many tokens, leading to performance degradation. They would likely use \"Second Person\" writing (e.g., \"You should do X\"), which is discouraged in favor of more direct, imperative commands. They would also likely write vague skill descriptions that fail to trigger the skill when the user actually needs it.\n\n---\n\n```yaml\nname: skill-development-refactor\nskills:\n  - Skill Development\nprompt: |\n  I am developing a Claude Code plugin called \"Kube-Master\". I have a draft for a skill called \"cluster-troubleshooting\", but it's poorly organized. \n  \n  Please refactor this into a high-quality skill following \"Skill Development\" best practices for Claude Code plugins. \n  \n  **Draft Content:**\n  Description: Use this skill when you want help with Kubernetes clusters. \n  \n  Body:\n  If you see a Pod in ImagePullBackOff, you should first check the events. You can do this by running `kubectl describe pod`. You also need to check if the secret exists for private registries.\n  \n  I also have 50 pages of specific error codes and 10 python scripts for automated log analysis that I want to include in this skill.\n  \n  **Your Task:**\n  1. Provide the content for the refactored `SKILL.md` (including YAML frontmatter).\n  2. Explain the directory structure you would create for this skill.\n  3. Describe how you would apply \"Progressive Disclosure\" to handle the 50 pages of error codes and 10 scripts.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the refactored skill based on these criteria:\n    \n    1. **Metadata (Frontmatter):**\n       - Score 1.0 if the description uses third-person (\"This skill should be used when...\") and includes specific, concrete trigger phrases (e.g., \"debug ImagePullBackOff\", \"troubleshoot cluster\"). \n       - Score 0.0 if it uses second person or is vague.\n    \n    2. **Writing Style:**\n       - Score 1.0 if the body uses imperative/infinitive form (e.g., \"Check events for pods,\" \"Verify registry secrets\") instead of \"You should\" or \"You can\".\n    \n    3. **Progressive Disclosure & Structure:**\n       - Score 1.0 if the model correctly moves the 50 pages of error codes to `references/` and the 10 scripts to `scripts/`, and references them within `SKILL.md`.\n       - Score 1.0 if the directory structure follows the plugin pattern: `skills/cluster-troubleshooting/{SKILL.md, references/, scripts/}`.\n\n    Final Score:\n    - 1.0: Follows all best practices (Third-person triggers, Imperative style, correct directory structure, and proper use of references/scripts for progressive disclosure).\n    - 0.5: Follows some practices (e.g., gets the structure right) but fails on writing style (uses \"You should\") or uses a weak description.\n    - 0.0: Keeps all content in SKILL.md or fails to provide a structured response.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of progressive disclosure and skill architecture principles, testing genuine domain knowledge rather than trivia. However, the grader has significant clarity issues: it uses multiple 1.0 scores for different criteria without clear aggregation logic, making it unclear how to compute the final score. The grader also doesn't specify how to weight the four criteria (metadata, writing style, progressive disclosure, structure) when combining them. The test is realistic and measures transferable knowledge about context management and AI-optimized documentation, but the broken scoring rubric undermines its utility."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\n```yaml\nname: plugin-skill-creation-methodology\nskills:\n- Skill Development\nprompt: |\n  You are helping a developer create a new Claude Code plugin skill for \"database schema management\". The developer tells you the skill should help users who frequently ask questions like:\n  \n  - \"Show me the user table relationships\"\n  - \"What fields are in the orders table?\"\n  - \"Generate sample queries for the analytics schema\"\n  - \"Validate this SQL against our database schema\"\n  \n  The developer wants to include database schema documentation and SQL validation utilities in the skill.\n\n  Explain how you would structure this skill, including:\n  1. What directory structure to create\n  2. What content should go in SKILL.md vs references/ vs scripts/\n  3. How to write the frontmatter description with proper trigger phrases\n  4. Key writing style requirements to follow\n\n  Focus on applying the skill development methodology rather than just listing components.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of skill development methodology:\n\n    Score 1.0 if:\n    - Correctly identifies directory structure (skill-name/SKILL.md with optional references/, scripts/, examples/)\n    - Properly applies progressive disclosure by assigning appropriate content to each location (core workflow in SKILL.md, detailed schemas in references/, validation utilities in scripts/)\n    - Writes frontmatter description in third person with specific trigger phrases from the provided user queries\n    - Uses imperative/infinitive form for instructions (not second person)\n    - References progressive disclosure methodology and explains the reasoning behind content organization choices\n    - Includes validation considerations and iterative improvement process\n\n    Score 0.5 if:\n    - Basic directory structure is correct but content organization is suboptimal\n    - Frontmatter description is written but may use incorrect person or lack specific triggers\n    - Mentions some best practices but misses key methodology elements\n    - Writing style may mix imperative with second person\n\n    Score 0.0 if:\n    - Incorrect directory structure or fails to follow skill development methodology\n    - Puts everything in SKILL.md without progressive disclosure consideration\n    - Uses second person throughout or writes poor trigger descriptions\n    - Does not demonstrate understanding of the skill creation workflow\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test demonstrates strong understanding of skill development methodology by requiring application of progressive disclosure, proper frontmatter structure, and writing style conventions. However, it has moderate clarity issues: the grader accepts 'basic directory structure' for 0.5 points even when 'content organization is suboptimal,' which could allow responses that fundamentally misunderstand progressive disclosure to receive partial credit. The test is realistic and tests transferable knowledge about organizing documentation and applying design principles, not skill-specific trivia. The grading criteria could be more precise about what constitutes 'suboptimal' organization versus fundamentally incorrect methodology."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Skill Analysis**\n\n1. **Domain knowledge taught:** The skill teaches how to construct and organize Claude Code plugin skills, emphasizing metadata requirements, progressive disclosure, structured directories (SKILL.md, references, examples, scripts), writing style (imperative body, third-person trigger descriptions), and reusable resources. It covers best practices for describing purpose, identifying trigger phrases, referencing auxiliary files, and maintaining lean documentation while offloading details to references.\n\n2. **Workflow decisions enabled:** It enables deciding when to add scripts, references, or assets based on repeated tasks, deterministic code needs, or large reference material. It guides when to keep content in SKILL.md versus references, when to include references/examples/scripts, and how to organize validation/testing workflows (skill reviewer, structure checks). It also helps determine trigger descriptions and iteration steps for improving skills.\n\n3. **Practitioner struggles without this skill:** Without the skill, a practitioner might misidentify trigger phrases, bloat SKILL.md with unreferenced content, use second-person writing, forget metadata/structure, skip progressive disclosure, or fail to validate/testing the skill. They would struggle to know what resources to include, how to describe when the skill should fire, and how to reference supporting files.\n\n**Test Case**\n\n```yaml\nname: skill-development-creation-review\nskills:\n  - Skill Development\nprompt: |\n  A plugin developer requests: \"I want to add a new skill that lets Claude help users set up webhooks. Outline the structure and describe how the SKILL.md should differ from the references files. Specify trigger phrases for the description, what to include in SKILL.md vs references/examples/scripts, and describe how to validate the new skill before release. Provide an example of how to refer to bundled resources.\"\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response shows understanding of skill development best practices.\n\n    Score 1.0 if: The response outlines the directory structure, differentiates SKILL.md content from references/examples/scripts, provides third-person trigger phrases for description, mentions progressive disclosure, describes validation steps (e.g., metadata checks, writing style, references), and includes an example of referencing bundled resources.\n\n    Score 0.5 if: The response covers some but not all of the above (e.g., structure and resource types but missing validation or trigger phrase guidance).\n\n    Score 0.0 if: The response lacks a coherent skill-development workflow, omits trigger phrase guidance, or fails to explain how to organize SKILL.md vs other resources.\n  extractor: last_assistant\n ```",
          "score": 0.65,
          "rationale": "The test case demonstrates moderate quality across dimensions. **Non-obviousness (6/10):** Tests genuine skill development concepts (progressive disclosure, trigger phrases, validation) rather than pure trivia, though the comprehensive checklist format makes it somewhat mechanical. **Clarity (7/10):** The grader is reasonably robust with clear scoring tiers, though 'outlines the directory structure' and 'mentions progressive disclosure' are somewhat vague criteria that could accept varied responses. **Realism (6/10):** The scenario is plausible but slightly contrived—a real practitioner would more likely ask 'help me create a webhook skill' rather than requesting an outline of structure/validation/references upfront. The meta-level framing (asking about how to organize rather than doing the task) reduces authenticity."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: create-specialized-skill-for-claude-code-plugin\nskills:\n- Skill Development\nprompt: |\n  Create a new skill named \".latitude-longitude-converter\" for a Claude Code plugin. \n  Include a script in the `scripts/` directory that takes a latitude and longitude as input and outputs whether the location is in the Northern or Southern hemisphere and the Eastern or Western hemisphere.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes a complete and correct skill directory structure for \"latitude-longitude-converter\" with a script in `scripts/` that correctly determines the hemispheres for given latitude and longitude values.\n    Score 0.5 if the response includes a mostly complete but incorrect skill directory structure or a script that determines the hemispheres but with errors.\n    Score 0.0 if the response does not include a recognizable skill directory structure or a functioning script.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test measures genuine skill knowledge (creating a skill with proper structure, scripts, and frontmatter) rather than trivia, earning moderate non-obviousness points. However, the grader is somewhat brittle—it only checks for 'correct' hemisphere determination without specifying what 'correct' means, and doesn't account for valid alternative implementations (e.g., different script languages, inline vs. file-based scripts). The task is realistic but slightly contrived—while creating skills is real work, the specific latitude-longitude converter feels artificial rather than addressing a genuine practitioner need. The grader also doesn't verify critical skill components like proper YAML frontmatter, trigger phrases in description, or imperative writing style that the skill document emphasizes."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-plugin-skill\nskills:\n  - Skill Development\nprompt: |\n  You have been tasked with creating a new skill for a Claude Code plugin. The skill should help users who need to manipulate and analyze PDF documents. This includes tasks like rotating pages, extracting text, and adding watermarks.\n\n  1. Identify the concrete examples of how this skill will be used.\n  2. Plan the reusable resources (scripts, references, assets) that will be included in the skill.\n  3. Create the skill directory structure.\n  4. Write the SKILL.md file with a third-person description and specific trigger phrases.\n  5. Reference the supporting files in the SKILL.md.\n  6. Validate the skill by ensuring it follows best practices.\n\n  Provide a detailed plan and the content for the SKILL.md file.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    Score 1.0 if:\n    - The response clearly identifies concrete examples of how the skill will be used.\n    - The plan for reusable resources (scripts, references, assets) is well thought out and relevant.\n    - The skill directory structure is correctly created.\n    - The SKILL.md file includes a third-person description with specific trigger phrases.\n    - The SKILL.md references the supporting files correctly.\n    - The skill follows best practices for progressive disclosure and writing style.\n\n    Score 0.5 if:\n    - The response partially meets the criteria, but is missing one or two key elements (e.g., missing specific trigger phrases or incomplete resource planning).\n\n    Score 0.0 if:\n    - The response does not meet the criteria or is significantly incomplete.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test has significant issues across all dimensions. **Non-obviousness (3/10)**: The task is essentially 'follow the skill creation process from the skill document,' which tests memorization of the specific workflow rather than transferable domain knowledge about skill design principles. **Clarity (4/10)**: The grader is vague ('well thought out,' 'correctly created') without specifying what makes resources relevant or structure correct, and doesn't account for valid alternative approaches. The task asks to 'create the skill directory structure' but provides no filesystem to work with, making it unclear if the model should output commands or describe the structure. **Realism (5/10)**: While creating a skill is realistic, the prompt artificially walks through all 6 steps of the documented process rather than presenting a natural scenario where someone needs to create a PDF manipulation skill. A practitioner would say 'help me create a skill for PDF manipulation' not 'follow steps 1-6 of the skill creation process.'"
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-hugging-face-model-trainer",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 12,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hugging-face-model-trainer\n\n**SKILL.md Content:**\n```markdown\n---\nname: hugging-face-model-trainer\ndescription: This skill should be used when users want to train or fine-tune language models using TRL (Transformer Reinforcement Learning) on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes guidance on the TRL Jobs package, UV scripts with PEP 723 format, dataset preparation and validation, hardware selection, cost estimation, Trackio monitoring, Hub authentication, and model persistence. Should be invoked for tasks involving cloud GPU training, GGUF conversion, or when users mention training on Hugging Face Jobs without local GPU setup.\nlicense: Complete terms in LICENSE.txt\n---\n\n# TRL Training on Hugging Face Jobs\n\n## Overview\n\nTrain language models using TRL (Transformer Reinforcement Learning) on fully managed Hugging Face infrastructure. No local GPU setup required—models train on cloud GPUs and results are automatically saved to the Hugging Face Hub.\n\n**TRL provides multiple training methods:**\n- **SFT** (Supervised Fine-Tuning) - Standard instruction tuning\n- **DPO** (Direct Preference Optimization) - Alignment from preference data\n- **GRPO** (Group Relative Policy Optimization) - Online RL training\n- **Reward Modeling** - Train reward models for RLHF\n\n**For detailed TRL method documentation:**\n```python\nhf_doc_search(\"your query\", product=\"trl\")\nhf_doc_fetch(\"https://huggingface.co/docs/trl/sft_trainer\")  # SFT\nhf_doc_fetch(\"https://huggingface.co/docs/trl/dpo_trainer\")  # DPO\n# etc.\n```\n\n**See also:** `references/training_methods.md` for method overviews and selection guidance\n\n## When to Use This Skill\n\nUse this skill when users want to:\n- Fine-tune language models on cloud GPUs without local infrastructure\n- Train with TRL methods (SFT, DPO, GRPO, etc.)\n- Run training jobs on Hugging Face Jobs infrastructure\n- Convert trained models to GGUF for local deployment (Ollama, LM Studio, llama.cpp)\n- Ensure trained models are permanently saved to the Hub\n- Use modern workflows with optimized defaults\n\n## Key Directives\n\nWhen assisting with training jobs:\n\n1. **ALWAYS use `hf_jobs()` MCP tool** - Submit jobs using `hf_jobs(\"uv\", {...})`, NOT bash `trl-jobs` commands. The `script` parameter accepts Python code directly. Do NOT save to local files unless the user explicitly requests it. Pass the script content as a string to `hf_jobs()`. If user asks to \"train a model\", \"fine-tune\", or similar requests, you MUST create the training script AND submit the job immediately using `hf_jobs()`.\n\n2. **Always include Trackio** - Every training script should include Trackio for real-time monitoring. Use example scripts in `scripts/` as templates.\n\n3. **Provide job details after submission** - After submitting, provide job ID, monitoring URL, estimated time, and note that the user can request status checks later.\n\n4. **Use example scripts as templates** - Reference `scripts/train_sft_example.py`, `scripts/train_dpo_example.py`, etc. as starting points.\n\n## Local Script Dependencies\n\nTo run scripts locally (like `estimate_cost.py`), install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Prerequisites Checklist\n\nBefore starting any training job, verify:\n\n### ✅ **Account & Authentication**\n- Hugging Face Account with [Pro](https://hf.co/pro), [Team](https://hf.co/enterprise), or [Enterprise](https://hf.co/enterprise) plan (Jobs require paid plan)\n- Authenticated login: Check with `hf_whoami()`\n- **HF_TOKEN for Hub Push** ⚠️ CRITICAL - Training environment is ephemeral, must push to Hub or ALL training results are lost\n- Token must have write permissions  \n- **MUST pass `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}` in job config** to make token available (the `$HF_TOKEN` syntax\n  references your actual token value)\n\n### ✅ **Dataset Requirements**\n- Dataset must exist on Hub or be loadable via `datasets.load_dataset()`\n- Format must match training method (SFT: \"messages\"/text/prompt-completion; DPO: chosen/rejected; GRPO: prompt-only)\n- **ALWAYS validate unknown datasets** before GPU training to prevent format failures (see Dataset Validation section below)\n- Size appropriate for hardware (Demo: 50-100 examples on t4-small; Production: 1K-10K+ on a10g-large/a100-large)\n\n### ⚠️ **Critical Settings**\n- **Timeout must exceed expected training time** - Default 30min is TOO SHORT for most training. Minimum recommended: 1-2 hours. Job fails and loses all progress if timeout is exceeded.\n- **Hub push must be enabled** - Config: `push_to_hub=True`, `hub_model_id=\"username/model-name\"`; Job: `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}`\n\n## Asynchronous Job Guidelines\n\n**⚠️ IMPORTANT: Training jobs run asynchronously and can take hours**\n\n### Action Required\n\n**When user requests training:**\n1. **Create the training script** with Trackio included (use `scripts/train_sft_example.py` as template)\n2. **Submit immediately** using `hf_jobs()` MCP tool with script content inline - don't save to file unless user requests\n3. **Report submission** with job ID, monitoring URL, and estimated time\n4. **Wait for user** to request status checks - don't poll automatically\n\n### Ground Rules\n- **Jobs run in background** - Submission returns immediately; training continues independently\n- **Initial logs delayed** - Can take 30-60 seconds for logs to appear\n- **User checks status** - Wait for user to request status updates\n- **Avoid polling** - Check logs only on user request; provide monitoring links instead\n\n### After Submission\n\n**Provide to user:**\n- ✅ Job ID and monitoring URL\n- ✅ Expected completion time\n- ✅ Trackio dashboard URL\n- ✅ Note that user can request status checks later\n\n**Example Response:**\n```\n✅ Job submitted successfully!\n\nJob ID: abc123xyz\nMonitor: https://huggingface.co/jobs/username/abc123xyz\n\nExpected time: ~2 hours\nEstimated cost: ~$10\n\nThe job is running in the background. Ask me to check status/logs when ready!\n```\n\n## Quick Start: Three Approaches\n\n**💡 Tip for Demos:** For quick demos on smaller GPUs (t4-small), omit `eval_dataset` and `eval_strategy` to save ~40% memory. You'll still see training loss and learning progress.\n\n### Sequence Length Configuration\n\n**TRL config classes use `max_length` (not `max_seq_length`)** to control tokenized sequence length:\n\n```python\n# ✅ CORRECT - If you need to set sequence length\nSFTConfig(max_length=512)   # Truncate sequences to 512 tokens\nDPOConfig(max_length=2048)  # Longer context (2048 tokens)\n\n# ❌ WRONG - This parameter doesn't exist\nSFTConfig(max_seq_length=512)  # TypeError!\n```\n\n**Default behavior:** `max_length=1024` (truncates from right). This works well for most training.\n\n**When to override:**\n- **Longer context**: Set higher (e.g., `max_length=2048`)\n- **Memory constraints**: Set lower (e.g., `max_length=512`)\n- **Vision models**: Set `max_length=None` (prevents cutting image tokens)\n\n**Usually you don't need to set this parameter at all** - the examples below use the sensible default.\n\n### Approach 1: UV Scripts (Recommended—Default Choice)\n\nUV scripts use PEP 723 inline dependencies for clean, self-contained training. **This is the primary approach for Claude Code.**\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"\"\"\n# /// script\n# dependencies = [\"trl>=0.12.0\", \"peft>=0.7.0\", \"trackio\"]\n# ///\n\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom trl import SFTTrainer, SFTConfig\nimport trackio\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\n# Create train/eval split for monitoring\ndataset_split = dataset.train_test_split(test_size=0.1, seed=42)\n\ntrainer = SFTTrainer(\n    model=\"Qwen/Qwen2.5-0.5B\",\n    train_dataset=dataset_split[\"train\"],\n    eval_dataset=dataset_split[\"test\"],\n    peft_config=LoraConfig(r=16, lora_alpha=32),\n    args=SFTConfig(\n        output_dir=\"my-model\",\n        push_to_hub=True,\n        hub_model_id=\"username/my-model\",\n        num_train_epochs=3,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        report_to=\"trackio\",\n        project=\"meaningful_prject_name\", # project name for the training name (trackio)\n        run_name=\"meaningful_run_name\",   # descriptive name for the specific training run (trackio)\n    )\n)\n\ntrainer.train()\ntrainer.push_to_hub()\n\"\"\",\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"2h\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}\n})\n```\n\n**Benefits:** Direct MCP tool usage, clean code, dependencies declared inline (PEP 723), no file saving required, full control\n**When to use:** Default choice for all training tasks in Claude Code, custom training logic, any scenario requiring `hf_jobs()`\n\n#### Working with Scripts\n\n⚠️ **Important:** The `script` parameter accepts either inline code (as shown above) OR a URL. **Local file paths do NOT work.**\n\n**Why local paths don't work:**\nJobs run in isolated Docker containers without access to your local filesystem. Scripts must be:\n- Inline code (recommended for custom training)\n- Publicly accessible URLs\n- Private repo URLs (with HF_TOKEN)\n\n**Common mistakes:**\n```python\n# ❌ These will all fail\nhf_jobs(\"uv\", {\"script\": \"train.py\"})\nhf_jobs(\"uv\", {\"script\": \"./scripts/train.py\"})\nhf_jobs(\"uv\", {\"script\": \"/path/to/train.py\"})\n```\n\n**Correct approaches:**\n```python\n# ✅ Inline code (recommended)\nhf_jobs(\"uv\", {\"script\": \"# /// script\\n# dependencies = [...]\\n# ///\\n\\n<your code>\"})\n\n# ✅ From Hugging Face Hub\nhf_jobs(\"uv\", {\"script\": \"https://huggingface.co/user/repo/resolve/main/train.py\"})\n\n# ✅ From GitHub\nhf_jobs(\"uv\", {\"script\": \"https://raw.githubusercontent.com/user/repo/main/train.py\"})\n\n# ✅ From Gist\nhf_jobs(\"uv\", {\"script\": \"https://gist.githubusercontent.com/user/id/raw/train.py\"})\n```\n\n**To use local scripts:** Upload to HF Hub first:\n```bash\nhuggingface-cli repo create my-training-scripts --type model\nhuggingface-cli upload my-training-scripts ./train.py train.py\n# Use: https://huggingface.co/USERNAME/my-training-scripts/resolve/main/train.py\n```\n\n### Approach 2: TRL Maintained Scripts (Official Examples)\n\nTRL provides battle-tested scripts for all methods. Can be run from URLs:\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py\",\n    \"script_args\": [\n        \"--model_name_or_path\", \"Qwen/Qwen2.5-0.5B\",\n        \"--dataset_name\", \"trl-lib/Capybara\",\n        \"--output_dir\", \"my-model\",\n        \"--push_to_hub\",\n        \"--hub_model_id\", \"username/my-model\"\n    ],\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"2h\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}\n})\n```\n\n**Benefits:** No code to write, maintained by TRL team, production-tested\n**When to use:** Standard TRL training, quick experiments, don't need custom code\n**Available:** Scripts are available from https://github.com/huggingface/trl/tree/main/examples/scripts\n\n### Finding More UV Scripts on Hub\n\nThe `uv-scripts` organization provides ready-to-use UV scripts stored as datasets on Hugging Face Hub:\n\n```python\n# Discover available UV script collections\ndataset_search({\"author\": \"uv-scripts\", \"sort\": \"downloads\", \"limit\": 20})\n\n# Explore a specific collection\nhub_repo_details([\"uv-scripts/classification\"], repo_type=\"dataset\", include_readme=True)\n```\n\n**Popular collections:** ocr, classification, synthetic-data, vllm, dataset-creation\n\n### Approach 3: HF Jobs CLI (Direct Terminal Commands)\n\nWhen the `hf_jobs()` MCP tool is unavailable, use the `hf jobs` CLI directly.\n\n**⚠️ CRITICAL: CLI Syntax Rules**\n\n```bash\n# ✅ CORRECT syntax - flags BEFORE script URL\nhf jobs uv run --flavor a10g-large --timeout 2h --secrets HF_TOKEN \"https://example.com/train.py\"\n\n# ❌ WRONG - \"run uv\" instead of \"uv run\"\nhf jobs run uv \"https://example.com/train.py\" --flavor a10g-large\n\n# ❌ WRONG - flags AFTER script URL (will be ignored!)\nhf jobs uv run \"https://example.com/train.py\" --flavor a10g-large\n\n# ❌ WRONG - \"--secret\" instead of \"--secrets\" (plural)\nhf jobs uv run --secret HF_TOKEN \"https://example.com/train.py\"\n```\n\n**Key syntax rules:**\n1. Command order is `hf jobs uv run` (NOT `hf jobs run uv`)\n2. All flags (`--flavor`, `--timeout`, `--secrets`) must come BEFORE the script URL\n3. Use `--secrets` (plural), not `--secret`\n4. Script URL must be the last positional argument\n\n**Complete CLI example:**\n```bash\nhf jobs uv run \\\n  --flavor a10g-large \\\n  --timeout 2h \\\n  --secrets HF_TOKEN \\\n  \"https://huggingface.co/user/repo/resolve/main/train.py\"\n```\n\n**Check job status via CLI:**\n```bash\nhf jobs ps                        # List all jobs\nhf jobs logs <job-id>             # View logs\nhf jobs inspect <job-id>          # Job details\nhf jobs cancel <job-id>           # Cancel a job\n```\n\n### Approach 4: TRL Jobs Package (Simplified Training)\n\nThe `trl-jobs` package provides optimized defaults and one-liner training.\n\n```bash\n# Install\npip install trl-jobs\n\n# Train with SFT (simplest possible)\ntrl-jobs sft \\\n  --model_name Qwen/Qwen2.5-0.5B \\\n  --dataset_name trl-lib/Capybara\n```\n\n**Benefits:** Pre-configured settings, automatic Trackio integration, automatic Hub push, one-line commands\n**When to use:** User working in terminal directly (not Claude Code context), quick local experimentation\n**Repository:** https://github.com/huggingface/trl-jobs\n\n⚠️ **In Claude Code context, prefer using `hf_jobs()` MCP tool (Approach 1) when available.**\n\n## Hardware Selection\n\n| Model Size | Recommended Hardware | Cost (approx/hr) | Use Case |\n|------------|---------------------|------------------|----------|\n| <1B params | `t4-small` | ~$0.75 | Demos, quick tests only without eval steps |\n| 1-3B params | `t4-medium`, `l4x1` | ~$1.50-2.50 | Development |\n| 3-7B params | `a10g-small`, `a10g-large` | ~$3.50-5.00 | Production training |\n| 7-13B params | `a10g-large`, `a100-large` | ~$5-10 | Large models (use LoRA) |\n| 13B+ params | `a100-large`, `a10g-largex2` | ~$10-20 | Very large (use LoRA) |\n\n**GPU Flavors:** cpu-basic/upgrade/performance/xl, t4-small/medium, l4x1/x4, a10g-small/large/largex2/largex4, a100-large, h100/h100x8\n\n**Guidelines:**\n- Use **LoRA/PEFT** for models >7B to reduce memory\n- Multi-GPU automatically handled by TRL/Accelerate\n- Start with smaller hardware for testing\n\n**See:** `references/hardware_guide.md` for detailed specifications\n\n## Critical: Saving Results to Hub\n\n**⚠️ EPHEMERAL ENVIRONMENT—MUST PUSH TO HUB**\n\nThe Jobs environment is temporary. All files are deleted when the job ends. If the model isn't pushed to Hub, **ALL TRAINING IS LOST**.\n\n### Required Configuration\n\n**In training script/config:**\n```python\nSFTConfig(\n    push_to_hub=True,\n    hub_model_id=\"username/model-name\",  # MUST specify\n    hub_strategy=\"every_save\",  # Optional: push checkpoints\n)\n```\n\n**In job submission:**\n```python\n{\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}  # Enables authentication\n}\n```\n\n### Verification Checklist\n\nBefore submitting:\n- [ ] `push_to_hub=True` set in config\n- [ ] `hub_model_id` includes username/repo-name\n- [ ] `secrets` parameter includes HF_TOKEN\n- [ ] User has write access to target repo\n\n**See:** `references/hub_saving.md` for detailed troubleshooting\n\n## Timeout Management\n\n**⚠️ DEFAULT: 30 MINUTES—TOO SHORT FOR TRAINING**\n\n### Setting Timeouts\n\n```python\n{\n    \"timeout\": \"2h\"   # 2 hours (formats: \"90m\", \"2h\", \"1.5h\", or seconds as integer)\n}\n```\n\n### Timeout Guidelines\n\n| Scenario | Recommended | Notes |\n|----------|-------------|-------|\n| Quick demo (50-100 examples) | 10-30 min | Verify setup |\n| Development training | 1-2 hours | Small datasets |\n| Production (3-7B model) | 4-6 hours | Full datasets |\n| Large model with LoRA | 3-6 hours | Depends on dataset |\n\n**Always add 20-30% buffer** for model/dataset loading, checkpoint saving, Hub push operations, and network delays.\n\n**On timeout:** Job killed immediately, all unsaved progress lost, must restart from beginning\n\n## Cost Estimation\n\n**Offer to estimate cost when planning jobs with known parameters.** Use `scripts/estimate_cost.py`:\n\n```bash\nuv run scripts/estimate_cost.py \\\n  --model meta-llama/Llama-2-7b-hf \\\n  --dataset trl-lib/Capybara \\\n  --hardware a10g-large \\\n  --dataset-size 16000 \\\n  --epochs 3\n```\n\nOutput includes estimated time, cost, recommended timeout (with buffer), and optimization suggestions.\n\n**When to offer:** User planning a job, asks about cost/time, choosing hardware, job will run >1 hour or cost >$5\n\n## Example Training Scripts\n\n**Production-ready templates with all best practices:**\n\nLoad these scripts for correctly:\n\n- **`scripts/train_sft_example.py`** - Complete SFT training with Trackio, LoRA, checkpoints\n- **`scripts/train_dpo_example.py`** - DPO training for preference learning\n- **`scripts/train_grpo_example.py`** - GRPO training for online RL\n\nThese scripts demonstrate proper Hub saving, Trackio integration, checkpoint management, and optimized parameters. Pass their content inline to `hf_jobs()` or use as templates for custom scripts.\n\n## Monitoring and Tracking\n\n**Trackio** provides real-time metrics visualization. See `references/trackio_guide.md` for complete setup guide.\n\n**Key points:**\n- Add `trackio` to dependencies\n- Configure trainer with `report_to=\"trackio\" and run_name=\"meaningful_name\"`\n\n### Trackio Configuration Defaults\n\n**Use sensible defaults unless user specifies otherwise.** When generating training scripts with Trackio:\n\n**Default Configuration:**\n- **Space ID**: `{username}/trackio` (use \"trackio\" as default space name)\n- **Run naming**: Unless otherwise specified, name the run in a way the user will recognize (e.g., descriptive of the task, model, or purpose)\n- **Config**: Keep minimal - only include hyperparameters and model/dataset info\n- **Project Name**: Use a Project Name to associate runs with a particular Project \n\n**User overrides:** If user requests specific trackio configuration (custom space, run naming, grouping, or additional config), apply their preferences instead of defaults.\n\n\nThis is useful for managing multiple jobs with the same configuration or keeping training scripts portable.\n\nSee `references/trackio_guide.md` for complete documentation including grouping runs for experiments.\n\n### Check Job Status\n\n```python\n# List all jobs\nhf_jobs(\"ps\")\n\n# Inspect specific job\nhf_jobs(\"inspect\", {\"job_id\": \"your-job-id\"})\n\n# View logs\nhf_jobs(\"logs\", {\"job_id\": \"your-job-id\"})\n```\n\n**Remember:** Wait for user to request status checks. Avoid polling repeatedly.\n\n## Dataset Validation\n\n**Validate dataset format BEFORE launching GPU training to prevent the #1 cause of training failures: format mismatches.**\n\n### Why Validate\n\n- 50%+ of training failures are due to dataset format issues\n- DPO especially strict: requires exact column names (`prompt`, `chosen`, `rejected`)\n- Failed GPU jobs waste $1-10 and 30-60 minutes\n- Validation on CPU costs ~$0.01 and takes <1 minute\n\n### When to Validate\n\n**ALWAYS validate for:**\n- Unknown or custom datasets\n- DPO training (CRITICAL - 90% of datasets need mapping)\n- Any dataset not explicitly TRL-compatible\n\n**Skip validation for known TRL datasets:**\n- `trl-lib/ultrachat_200k`, `trl-lib/Capybara`, `HuggingFaceH4/ultrachat_200k`, etc.\n\n### Usage\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py\",\n    \"script_args\": [\"--dataset\", \"username/dataset-name\", \"--split\", \"train\"]\n})\n```\n\nThe script is fast, and will usually complete synchronously.\n\n### Reading Results\n\nThe output shows compatibility for each training method:\n\n- **`✓ READY`** - Dataset is compatible, use directly\n- **`✗ NEEDS MAPPING`** - Compatible but needs preprocessing (mapping code provided)\n- **`✗ INCOMPATIBLE`** - Cannot be used for this method\n\nWhen mapping is needed, the output includes a **\"MAPPING CODE\"** section with copy-paste ready Python code.\n\n### Example Workflow\n\n```python\n# 1. Inspect dataset (costs ~$0.01, <1 min on CPU)\nhf_jobs(\"uv\", {\n    \"script\": \"https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py\",\n    \"script_args\": [\"--dataset\", \"argilla/distilabel-math-preference-dpo\", \"--split\", \"train\"]\n})\n\n# 2. Check output markers:\n#    ✓ READY → proceed with training\n#    ✗ NEEDS MAPPING → apply mapping code below\n#    ✗ INCOMPATIBLE → choose different method/dataset\n\n# 3. If mapping needed, apply before training:\ndef format_for_dpo(example):\n    return {\n        'prompt': example['instruction'],\n        'chosen': example['chosen_response'],\n        'rejected': example['rejected_response'],\n    }\ndataset = dataset.map(format_for_dpo, remove_columns=dataset.column_names)\n\n# 4. Launch training job with confidence\n```\n\n### Common Scenario: DPO Format Mismatch\n\nMost DPO datasets use non-standard column names. Example:\n\n```\nDataset has: instruction, chosen_response, rejected_response\nDPO expects: prompt, chosen, rejected\n```\n\nThe validator detects this and provides exact mapping code to fix it.\n\n## Converting Models to GGUF\n\nAfter training, convert models to **GGUF format** for use with llama.cpp, Ollama, LM Studio, and other local inference tools.\n\n**What is GGUF:**\n- Optimized for CPU/GPU inference with llama.cpp\n- Supports quantization (4-bit, 5-bit, 8-bit) to reduce model size\n- Compatible with Ollama, LM Studio, Jan, GPT4All, llama.cpp\n- Typically 2-8GB for 7B models (vs 14GB unquantized)\n\n**When to convert:**\n- Running models locally with Ollama or LM Studio\n- Reducing model size with quantization\n- Deploying to edge devices\n- Sharing models for local-first use\n\n**See:** `references/gguf_conversion.md` for complete conversion guide, including production-ready conversion script, quantization options, hardware requirements, usage examples, and troubleshooting.\n\n**Quick conversion:**\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"<see references/gguf_conversion.md for complete script>\",\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"45m\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"},\n    \"env\": {\n        \"ADAPTER_MODEL\": \"username/my-finetuned-model\",\n        \"BASE_MODEL\": \"Qwen/Qwen2.5-0.5B\",\n        \"OUTPUT_REPO\": \"username/my-model-gguf\"\n    }\n})\n```\n\n## Common Training Patterns\n\nSee `references/training_patterns.md` for detailed examples including:\n- Quick demo (5-10 minutes)\n- Production with checkpoints\n- Multi-GPU training\n- DPO training (preference learning)\n- GRPO training (online RL)\n\n## Common Failure Modes\n\n### Out of Memory (OOM)\n\n**Fix (try in order):**\n1. Reduce batch size: `per_device_train_batch_size=1`, increase `gradient_accumulation_steps=8`. Effective batch size is `per_device_train_batch_size` x `gradient_accumulation_steps`. For best performance keep effective batch size close to 128. \n2. Enable: `gradient_checkpointing=True`\n3. Upgrade hardware: t4-small → l4x1, a10g-small → a10g-large etc. \n\n### Dataset Misformatted\n\n**Fix:**\n1. Validate first with dataset inspector:\n   ```bash\n   uv run https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py \\\n     --dataset name --split train\n   ```\n2. Check output for compatibility markers (✓ READY, ✗ NEEDS MAPPING, ✗ INCOMPATIBLE)\n3. Apply mapping code from inspector output if needed\n\n### Job Timeout\n\n**Fix:**\n1. Check logs for actual runtime: `hf_jobs(\"logs\", {\"job_id\": \"...\"})`\n2. Increase timeout with buffer: `\"timeout\": \"3h\"` (add 30% to estimated time)\n3. Or reduce training: lower `num_train_epochs`, use smaller dataset, enable `max_steps`\n4. Save checkpoints: `save_strategy=\"steps\"`, `save_steps=500`, `hub_strategy=\"every_save\"`\n\n**Note:** Default 30min is insufficient for real training. Minimum 1-2 hours.\n\n### Hub Push Failures\n\n**Fix:**\n1. Add to job: `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}`\n2. Add to config: `push_to_hub=True`, `hub_model_id=\"username/model-name\"`\n3. Verify auth: `mcp__huggingface__hf_whoami()`\n4. Check token has write permissions and repo exists (or set `hub_private_repo=True`)\n\n### Missing Dependencies\n\n**Fix:**\nAdd to PEP 723 header:\n```python\n# /// script\n# dependencies = [\"trl>=0.12.0\", \"peft>=0.7.0\", \"trackio\", \"missing-package\"]\n# ///\n```\n\n## Troubleshooting\n\n**Common issues:**\n- Job times out → Increase timeout, reduce epochs/dataset, use smaller model/LoRA\n- Model not saved to Hub → Check push_to_hub=True, hub_model_id, secrets=HF_TOKEN\n- Out of Memory (OOM) → Reduce batch size, increase gradient accumulation, enable LoRA, use larger GPU\n- Dataset format error → Validate with dataset inspector (see Dataset Validation section)\n- Import/module errors → Add PEP 723 header with dependencies, verify format\n- Authentication errors → Check `mcp__huggingface__hf_whoami()`, token permissions, secrets parameter\n\n**See:** `references/troubleshooting.md` for complete troubleshooting guide\n\n## Resources\n\n### References (In This Skill)\n- `references/training_methods.md` - Overview of SFT, DPO, GRPO, KTO, PPO, Reward Modeling\n- `references/training_patterns.md` - Common training patterns and examples\n- `references/gguf_conversion.md` - Complete GGUF conversion guide\n- `references/trackio_guide.md` - Trackio monitoring setup\n- `references/hardware_guide.md` - Hardware specs and selection\n- `references/hub_saving.md` - Hub authentication troubleshooting\n- `references/troubleshooting.md` - Common issues and solutions\n\n### Scripts (In This Skill)\n- `scripts/train_sft_example.py` - Production SFT template\n- `scripts/train_dpo_example.py` - Production DPO template\n- `scripts/train_grpo_example.py` - Production GRPO template\n- `scripts/estimate_cost.py` - Estimate time and cost (offer when appropriate)\n- `scripts/convert_to_gguf.py` - Complete GGUF conversion script\n\n### External Scripts\n- [Dataset Inspector](https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py) - Validate dataset format before training (use via `uv run` or `hf_jobs`)\n\n### External Links\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [TRL Jobs Training Guide](https://huggingface.co/docs/trl/en/jobs_training)\n- [TRL Jobs Package](https://github.com/huggingface/trl-jobs)\n- [HF Jobs Documentation](https://huggingface.co/docs/huggingface_hub/guides/jobs)\n- [TRL Example Scripts](https://github.com/huggingface/trl/tree/main/examples/scripts)\n- [UV Scripts Guide](https://docs.astral.sh/uv/guides/scripts/)\n- [UV Scripts Organization](https://huggingface.co/uv-scripts)\n\n## Key Takeaways\n\n1. **Submit scripts inline** - The `script` parameter accepts Python code directly; no file saving required unless user requests\n2. **Jobs are asynchronous** - Don't wait/poll; let user check when ready\n3. **Always set timeout** - Default 30 min is insufficient; minimum 1-2 hours recommended\n4. **Always enable Hub push** - Environment is ephemeral; without push, all results lost\n5. **Include Trackio** - Use example scripts as templates for real-time monitoring\n6. **Offer cost estimation** - When parameters are known, use `scripts/estimate_cost.py`\n7. **Use UV scripts (Approach 1)** - Default to `hf_jobs(\"uv\", {...})` with inline scripts; TRL maintained scripts for standard training; avoid bash `trl-jobs` commands in Claude Code\n8. **Use hf_doc_fetch/hf_doc_search** for latest TRL documentation\n9. **Validate dataset format** before training with dataset inspector (see Dataset Validation section)\n10. **Choose appropriate hardware** for model size; use LoRA for models >7B\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** hugging-face-model-trainer\n\n**SKILL.md Content:**\n```markdown\n---\nname: hugging-face-model-trainer\ndescription: This skill should be used when users want to train or fine-tune language models using TRL (Transformer Reinforcement Learning) on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes guidance on the TRL Jobs package, UV scripts with PEP 723 format, dataset preparation and validation, hardware selection, cost estimation, Trackio monitoring, Hub authentication, and model persistence. Should be invoked for tasks involving cloud GPU training, GGUF conversion, or when users mention training on Hugging Face Jobs without local GPU setup.\nlicense: Complete terms in LICENSE.txt\n---\n\n# TRL Training on Hugging Face Jobs\n\n## Overview\n\nTrain language models using TRL (Transformer Reinforcement Learning) on fully managed Hugging Face infrastructure. No local GPU setup required—models train on cloud GPUs and results are automatically saved to the Hugging Face Hub.\n\n**TRL provides multiple training methods:**\n- **SFT** (Supervised Fine-Tuning) - Standard instruction tuning\n- **DPO** (Direct Preference Optimization) - Alignment from preference data\n- **GRPO** (Group Relative Policy Optimization) - Online RL training\n- **Reward Modeling** - Train reward models for RLHF\n\n**For detailed TRL method documentation:**\n```python\nhf_doc_search(\"your query\", product=\"trl\")\nhf_doc_fetch(\"https://huggingface.co/docs/trl/sft_trainer\")  # SFT\nhf_doc_fetch(\"https://huggingface.co/docs/trl/dpo_trainer\")  # DPO\n# etc.\n```\n\n**See also:** `references/training_methods.md` for method overviews and selection guidance\n\n## When to Use This Skill\n\nUse this skill when users want to:\n- Fine-tune language models on cloud GPUs without local infrastructure\n- Train with TRL methods (SFT, DPO, GRPO, etc.)\n- Run training jobs on Hugging Face Jobs infrastructure\n- Convert trained models to GGUF for local deployment (Ollama, LM Studio, llama.cpp)\n- Ensure trained models are permanently saved to the Hub\n- Use modern workflows with optimized defaults\n\n## Key Directives\n\nWhen assisting with training jobs:\n\n1. **ALWAYS use `hf_jobs()` MCP tool** - Submit jobs using `hf_jobs(\"uv\", {...})`, NOT bash `trl-jobs` commands. The `script` parameter accepts Python code directly. Do NOT save to local files unless the user explicitly requests it. Pass the script content as a string to `hf_jobs()`. If user asks to \"train a model\", \"fine-tune\", or similar requests, you MUST create the training script AND submit the job immediately using `hf_jobs()`.\n\n2. **Always include Trackio** - Every training script should include Trackio for real-time monitoring. Use example scripts in `scripts/` as templates.\n\n3. **Provide job details after submission** - After submitting, provide job ID, monitoring URL, estimated time, and note that the user can request status checks later.\n\n4. **Use example scripts as templates** - Reference `scripts/train_sft_example.py`, `scripts/train_dpo_example.py`, etc. as starting points.\n\n## Local Script Dependencies\n\nTo run scripts locally (like `estimate_cost.py`), install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Prerequisites Checklist\n\nBefore starting any training job, verify:\n\n### ✅ **Account & Authentication**\n- Hugging Face Account with [Pro](https://hf.co/pro), [Team](https://hf.co/enterprise), or [Enterprise](https://hf.co/enterprise) plan (Jobs require paid plan)\n- Authenticated login: Check with `hf_whoami()`\n- **HF_TOKEN for Hub Push** ⚠️ CRITICAL - Training environment is ephemeral, must push to Hub or ALL training results are lost\n- Token must have write permissions  \n- **MUST pass `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}` in job config** to make token available (the `$HF_TOKEN` syntax\n  references your actual token value)\n\n### ✅ **Dataset Requirements**\n- Dataset must exist on Hub or be loadable via `datasets.load_dataset()`\n- Format must match training method (SFT: \"messages\"/text/prompt-completion; DPO: chosen/rejected; GRPO: prompt-only)\n- **ALWAYS validate unknown datasets** before GPU training to prevent format failures (see Dataset Validation section below)\n- Size appropriate for hardware (Demo: 50-100 examples on t4-small; Production: 1K-10K+ on a10g-large/a100-large)\n\n### ⚠️ **Critical Settings**\n- **Timeout must exceed expected training time** - Default 30min is TOO SHORT for most training. Minimum recommended: 1-2 hours. Job fails and loses all progress if timeout is exceeded.\n- **Hub push must be enabled** - Config: `push_to_hub=True`, `hub_model_id=\"username/model-name\"`; Job: `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}`\n\n## Asynchronous Job Guidelines\n\n**⚠️ IMPORTANT: Training jobs run asynchronously and can take hours**\n\n### Action Required\n\n**When user requests training:**\n1. **Create the training script** with Trackio included (use `scripts/train_sft_example.py` as template)\n2. **Submit immediately** using `hf_jobs()` MCP tool with script content inline - don't save to file unless user requests\n3. **Report submission** with job ID, monitoring URL, and estimated time\n4. **Wait for user** to request status checks - don't poll automatically\n\n### Ground Rules\n- **Jobs run in background** - Submission returns immediately; training continues independently\n- **Initial logs delayed** - Can take 30-60 seconds for logs to appear\n- **User checks status** - Wait for user to request status updates\n- **Avoid polling** - Check logs only on user request; provide monitoring links instead\n\n### After Submission\n\n**Provide to user:**\n- ✅ Job ID and monitoring URL\n- ✅ Expected completion time\n- ✅ Trackio dashboard URL\n- ✅ Note that user can request status checks later\n\n**Example Response:**\n```\n✅ Job submitted successfully!\n\nJob ID: abc123xyz\nMonitor: https://huggingface.co/jobs/username/abc123xyz\n\nExpected time: ~2 hours\nEstimated cost: ~$10\n\nThe job is running in the background. Ask me to check status/logs when ready!\n```\n\n## Quick Start: Three Approaches\n\n**💡 Tip for Demos:** For quick demos on smaller GPUs (t4-small), omit `eval_dataset` and `eval_strategy` to save ~40% memory. You'll still see training loss and learning progress.\n\n### Sequence Length Configuration\n\n**TRL config classes use `max_length` (not `max_seq_length`)** to control tokenized sequence length:\n\n```python\n# ✅ CORRECT - If you need to set sequence length\nSFTConfig(max_length=512)   # Truncate sequences to 512 tokens\nDPOConfig(max_length=2048)  # Longer context (2048 tokens)\n\n# ❌ WRONG - This parameter doesn't exist\nSFTConfig(max_seq_length=512)  # TypeError!\n```\n\n**Default behavior:** `max_length=1024` (truncates from right). This works well for most training.\n\n**When to override:**\n- **Longer context**: Set higher (e.g., `max_length=2048`)\n- **Memory constraints**: Set lower (e.g., `max_length=512`)\n- **Vision models**: Set `max_length=None` (prevents cutting image tokens)\n\n**Usually you don't need to set this parameter at all** - the examples below use the sensible default.\n\n### Approach 1: UV Scripts (Recommended—Default Choice)\n\nUV scripts use PEP 723 inline dependencies for clean, self-contained training. **This is the primary approach for Claude Code.**\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"\"\"\n# /// script\n# dependencies = [\"trl>=0.12.0\", \"peft>=0.7.0\", \"trackio\"]\n# ///\n\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom trl import SFTTrainer, SFTConfig\nimport trackio\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\n# Create train/eval split for monitoring\ndataset_split = dataset.train_test_split(test_size=0.1, seed=42)\n\ntrainer = SFTTrainer(\n    model=\"Qwen/Qwen2.5-0.5B\",\n    train_dataset=dataset_split[\"train\"],\n    eval_dataset=dataset_split[\"test\"],\n    peft_config=LoraConfig(r=16, lora_alpha=32),\n    args=SFTConfig(\n        output_dir=\"my-model\",\n        push_to_hub=True,\n        hub_model_id=\"username/my-model\",\n        num_train_epochs=3,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        report_to=\"trackio\",\n        project=\"meaningful_prject_name\", # project name for the training name (trackio)\n        run_name=\"meaningful_run_name\",   # descriptive name for the specific training run (trackio)\n    )\n)\n\ntrainer.train()\ntrainer.push_to_hub()\n\"\"\",\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"2h\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}\n})\n```\n\n**Benefits:** Direct MCP tool usage, clean code, dependencies declared inline (PEP 723), no file saving required, full control\n**When to use:** Default choice for all training tasks in Claude Code, custom training logic, any scenario requiring `hf_jobs()`\n\n#### Working with Scripts\n\n⚠️ **Important:** The `script` parameter accepts either inline code (as shown above) OR a URL. **Local file paths do NOT work.**\n\n**Why local paths don't work:**\nJobs run in isolated Docker containers without access to your local filesystem. Scripts must be:\n- Inline code (recommended for custom training)\n- Publicly accessible URLs\n- Private repo URLs (with HF_TOKEN)\n\n**Common mistakes:**\n```python\n# ❌ These will all fail\nhf_jobs(\"uv\", {\"script\": \"train.py\"})\nhf_jobs(\"uv\", {\"script\": \"./scripts/train.py\"})\nhf_jobs(\"uv\", {\"script\": \"/path/to/train.py\"})\n```\n\n**Correct approaches:**\n```python\n# ✅ Inline code (recommended)\nhf_jobs(\"uv\", {\"script\": \"# /// script\\n# dependencies = [...]\\n# ///\\n\\n<your code>\"})\n\n# ✅ From Hugging Face Hub\nhf_jobs(\"uv\", {\"script\": \"https://huggingface.co/user/repo/resolve/main/train.py\"})\n\n# ✅ From GitHub\nhf_jobs(\"uv\", {\"script\": \"https://raw.githubusercontent.com/user/repo/main/train.py\"})\n\n# ✅ From Gist\nhf_jobs(\"uv\", {\"script\": \"https://gist.githubusercontent.com/user/id/raw/train.py\"})\n```\n\n**To use local scripts:** Upload to HF Hub first:\n```bash\nhuggingface-cli repo create my-training-scripts --type model\nhuggingface-cli upload my-training-scripts ./train.py train.py\n# Use: https://huggingface.co/USERNAME/my-training-scripts/resolve/main/train.py\n```\n\n### Approach 2: TRL Maintained Scripts (Official Examples)\n\nTRL provides battle-tested scripts for all methods. Can be run from URLs:\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py\",\n    \"script_args\": [\n        \"--model_name_or_path\", \"Qwen/Qwen2.5-0.5B\",\n        \"--dataset_name\", \"trl-lib/Capybara\",\n        \"--output_dir\", \"my-model\",\n        \"--push_to_hub\",\n        \"--hub_model_id\", \"username/my-model\"\n    ],\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"2h\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}\n})\n```\n\n**Benefits:** No code to write, maintained by TRL team, production-tested\n**When to use:** Standard TRL training, quick experiments, don't need custom code\n**Available:** Scripts are available from https://github.com/huggingface/trl/tree/main/examples/scripts\n\n### Finding More UV Scripts on Hub\n\nThe `uv-scripts` organization provides ready-to-use UV scripts stored as datasets on Hugging Face Hub:\n\n```python\n# Discover available UV script collections\ndataset_search({\"author\": \"uv-scripts\", \"sort\": \"downloads\", \"limit\": 20})\n\n# Explore a specific collection\nhub_repo_details([\"uv-scripts/classification\"], repo_type=\"dataset\", include_readme=True)\n```\n\n**Popular collections:** ocr, classification, synthetic-data, vllm, dataset-creation\n\n### Approach 3: HF Jobs CLI (Direct Terminal Commands)\n\nWhen the `hf_jobs()` MCP tool is unavailable, use the `hf jobs` CLI directly.\n\n**⚠️ CRITICAL: CLI Syntax Rules**\n\n```bash\n# ✅ CORRECT syntax - flags BEFORE script URL\nhf jobs uv run --flavor a10g-large --timeout 2h --secrets HF_TOKEN \"https://example.com/train.py\"\n\n# ❌ WRONG - \"run uv\" instead of \"uv run\"\nhf jobs run uv \"https://example.com/train.py\" --flavor a10g-large\n\n# ❌ WRONG - flags AFTER script URL (will be ignored!)\nhf jobs uv run \"https://example.com/train.py\" --flavor a10g-large\n\n# ❌ WRONG - \"--secret\" instead of \"--secrets\" (plural)\nhf jobs uv run --secret HF_TOKEN \"https://example.com/train.py\"\n```\n\n**Key syntax rules:**\n1. Command order is `hf jobs uv run` (NOT `hf jobs run uv`)\n2. All flags (`--flavor`, `--timeout`, `--secrets`) must come BEFORE the script URL\n3. Use `--secrets` (plural), not `--secret`\n4. Script URL must be the last positional argument\n\n**Complete CLI example:**\n```bash\nhf jobs uv run \\\n  --flavor a10g-large \\\n  --timeout 2h \\\n  --secrets HF_TOKEN \\\n  \"https://huggingface.co/user/repo/resolve/main/train.py\"\n```\n\n**Check job status via CLI:**\n```bash\nhf jobs ps                        # List all jobs\nhf jobs logs <job-id>             # View logs\nhf jobs inspect <job-id>          # Job details\nhf jobs cancel <job-id>           # Cancel a job\n```\n\n### Approach 4: TRL Jobs Package (Simplified Training)\n\nThe `trl-jobs` package provides optimized defaults and one-liner training.\n\n```bash\n# Install\npip install trl-jobs\n\n# Train with SFT (simplest possible)\ntrl-jobs sft \\\n  --model_name Qwen/Qwen2.5-0.5B \\\n  --dataset_name trl-lib/Capybara\n```\n\n**Benefits:** Pre-configured settings, automatic Trackio integration, automatic Hub push, one-line commands\n**When to use:** User working in terminal directly (not Claude Code context), quick local experimentation\n**Repository:** https://github.com/huggingface/trl-jobs\n\n⚠️ **In Claude Code context, prefer using `hf_jobs()` MCP tool (Approach 1) when available.**\n\n## Hardware Selection\n\n| Model Size | Recommended Hardware | Cost (approx/hr) | Use Case |\n|------------|---------------------|------------------|----------|\n| <1B params | `t4-small` | ~$0.75 | Demos, quick tests only without eval steps |\n| 1-3B params | `t4-medium`, `l4x1` | ~$1.50-2.50 | Development |\n| 3-7B params | `a10g-small`, `a10g-large` | ~$3.50-5.00 | Production training |\n| 7-13B params | `a10g-large`, `a100-large` | ~$5-10 | Large models (use LoRA) |\n| 13B+ params | `a100-large`, `a10g-largex2` | ~$10-20 | Very large (use LoRA) |\n\n**GPU Flavors:** cpu-basic/upgrade/performance/xl, t4-small/medium, l4x1/x4, a10g-small/large/largex2/largex4, a100-large, h100/h100x8\n\n**Guidelines:**\n- Use **LoRA/PEFT** for models >7B to reduce memory\n- Multi-GPU automatically handled by TRL/Accelerate\n- Start with smaller hardware for testing\n\n**See:** `references/hardware_guide.md` for detailed specifications\n\n## Critical: Saving Results to Hub\n\n**⚠️ EPHEMERAL ENVIRONMENT—MUST PUSH TO HUB**\n\nThe Jobs environment is temporary. All files are deleted when the job ends. If the model isn't pushed to Hub, **ALL TRAINING IS LOST**.\n\n### Required Configuration\n\n**In training script/config:**\n```python\nSFTConfig(\n    push_to_hub=True,\n    hub_model_id=\"username/model-name\",  # MUST specify\n    hub_strategy=\"every_save\",  # Optional: push checkpoints\n)\n```\n\n**In job submission:**\n```python\n{\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"}  # Enables authentication\n}\n```\n\n### Verification Checklist\n\nBefore submitting:\n- [ ] `push_to_hub=True` set in config\n- [ ] `hub_model_id` includes username/repo-name\n- [ ] `secrets` parameter includes HF_TOKEN\n- [ ] User has write access to target repo\n\n**See:** `references/hub_saving.md` for detailed troubleshooting\n\n## Timeout Management\n\n**⚠️ DEFAULT: 30 MINUTES—TOO SHORT FOR TRAINING**\n\n### Setting Timeouts\n\n```python\n{\n    \"timeout\": \"2h\"   # 2 hours (formats: \"90m\", \"2h\", \"1.5h\", or seconds as integer)\n}\n```\n\n### Timeout Guidelines\n\n| Scenario | Recommended | Notes |\n|----------|-------------|-------|\n| Quick demo (50-100 examples) | 10-30 min | Verify setup |\n| Development training | 1-2 hours | Small datasets |\n| Production (3-7B model) | 4-6 hours | Full datasets |\n| Large model with LoRA | 3-6 hours | Depends on dataset |\n\n**Always add 20-30% buffer** for model/dataset loading, checkpoint saving, Hub push operations, and network delays.\n\n**On timeout:** Job killed immediately, all unsaved progress lost, must restart from beginning\n\n## Cost Estimation\n\n**Offer to estimate cost when planning jobs with known parameters.** Use `scripts/estimate_cost.py`:\n\n```bash\nuv run scripts/estimate_cost.py \\\n  --model meta-llama/Llama-2-7b-hf \\\n  --dataset trl-lib/Capybara \\\n  --hardware a10g-large \\\n  --dataset-size 16000 \\\n  --epochs 3\n```\n\nOutput includes estimated time, cost, recommended timeout (with buffer), and optimization suggestions.\n\n**When to offer:** User planning a job, asks about cost/time, choosing hardware, job will run >1 hour or cost >$5\n\n## Example Training Scripts\n\n**Production-ready templates with all best practices:**\n\nLoad these scripts for correctly:\n\n- **`scripts/train_sft_example.py`** - Complete SFT training with Trackio, LoRA, checkpoints\n- **`scripts/train_dpo_example.py`** - DPO training for preference learning\n- **`scripts/train_grpo_example.py`** - GRPO training for online RL\n\nThese scripts demonstrate proper Hub saving, Trackio integration, checkpoint management, and optimized parameters. Pass their content inline to `hf_jobs()` or use as templates for custom scripts.\n\n## Monitoring and Tracking\n\n**Trackio** provides real-time metrics visualization. See `references/trackio_guide.md` for complete setup guide.\n\n**Key points:**\n- Add `trackio` to dependencies\n- Configure trainer with `report_to=\"trackio\" and run_name=\"meaningful_name\"`\n\n### Trackio Configuration Defaults\n\n**Use sensible defaults unless user specifies otherwise.** When generating training scripts with Trackio:\n\n**Default Configuration:**\n- **Space ID**: `{username}/trackio` (use \"trackio\" as default space name)\n- **Run naming**: Unless otherwise specified, name the run in a way the user will recognize (e.g., descriptive of the task, model, or purpose)\n- **Config**: Keep minimal - only include hyperparameters and model/dataset info\n- **Project Name**: Use a Project Name to associate runs with a particular Project \n\n**User overrides:** If user requests specific trackio configuration (custom space, run naming, grouping, or additional config), apply their preferences instead of defaults.\n\n\nThis is useful for managing multiple jobs with the same configuration or keeping training scripts portable.\n\nSee `references/trackio_guide.md` for complete documentation including grouping runs for experiments.\n\n### Check Job Status\n\n```python\n# List all jobs\nhf_jobs(\"ps\")\n\n# Inspect specific job\nhf_jobs(\"inspect\", {\"job_id\": \"your-job-id\"})\n\n# View logs\nhf_jobs(\"logs\", {\"job_id\": \"your-job-id\"})\n```\n\n**Remember:** Wait for user to request status checks. Avoid polling repeatedly.\n\n## Dataset Validation\n\n**Validate dataset format BEFORE launching GPU training to prevent the #1 cause of training failures: format mismatches.**\n\n### Why Validate\n\n- 50%+ of training failures are due to dataset format issues\n- DPO especially strict: requires exact column names (`prompt`, `chosen`, `rejected`)\n- Failed GPU jobs waste $1-10 and 30-60 minutes\n- Validation on CPU costs ~$0.01 and takes <1 minute\n\n### When to Validate\n\n**ALWAYS validate for:**\n- Unknown or custom datasets\n- DPO training (CRITICAL - 90% of datasets need mapping)\n- Any dataset not explicitly TRL-compatible\n\n**Skip validation for known TRL datasets:**\n- `trl-lib/ultrachat_200k`, `trl-lib/Capybara`, `HuggingFaceH4/ultrachat_200k`, etc.\n\n### Usage\n\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py\",\n    \"script_args\": [\"--dataset\", \"username/dataset-name\", \"--split\", \"train\"]\n})\n```\n\nThe script is fast, and will usually complete synchronously.\n\n### Reading Results\n\nThe output shows compatibility for each training method:\n\n- **`✓ READY`** - Dataset is compatible, use directly\n- **`✗ NEEDS MAPPING`** - Compatible but needs preprocessing (mapping code provided)\n- **`✗ INCOMPATIBLE`** - Cannot be used for this method\n\nWhen mapping is needed, the output includes a **\"MAPPING CODE\"** section with copy-paste ready Python code.\n\n### Example Workflow\n\n```python\n# 1. Inspect dataset (costs ~$0.01, <1 min on CPU)\nhf_jobs(\"uv\", {\n    \"script\": \"https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py\",\n    \"script_args\": [\"--dataset\", \"argilla/distilabel-math-preference-dpo\", \"--split\", \"train\"]\n})\n\n# 2. Check output markers:\n#    ✓ READY → proceed with training\n#    ✗ NEEDS MAPPING → apply mapping code below\n#    ✗ INCOMPATIBLE → choose different method/dataset\n\n# 3. If mapping needed, apply before training:\ndef format_for_dpo(example):\n    return {\n        'prompt': example['instruction'],\n        'chosen': example['chosen_response'],\n        'rejected': example['rejected_response'],\n    }\ndataset = dataset.map(format_for_dpo, remove_columns=dataset.column_names)\n\n# 4. Launch training job with confidence\n```\n\n### Common Scenario: DPO Format Mismatch\n\nMost DPO datasets use non-standard column names. Example:\n\n```\nDataset has: instruction, chosen_response, rejected_response\nDPO expects: prompt, chosen, rejected\n```\n\nThe validator detects this and provides exact mapping code to fix it.\n\n## Converting Models to GGUF\n\nAfter training, convert models to **GGUF format** for use with llama.cpp, Ollama, LM Studio, and other local inference tools.\n\n**What is GGUF:**\n- Optimized for CPU/GPU inference with llama.cpp\n- Supports quantization (4-bit, 5-bit, 8-bit) to reduce model size\n- Compatible with Ollama, LM Studio, Jan, GPT4All, llama.cpp\n- Typically 2-8GB for 7B models (vs 14GB unquantized)\n\n**When to convert:**\n- Running models locally with Ollama or LM Studio\n- Reducing model size with quantization\n- Deploying to edge devices\n- Sharing models for local-first use\n\n**See:** `references/gguf_conversion.md` for complete conversion guide, including production-ready conversion script, quantization options, hardware requirements, usage examples, and troubleshooting.\n\n**Quick conversion:**\n```python\nhf_jobs(\"uv\", {\n    \"script\": \"<see references/gguf_conversion.md for complete script>\",\n    \"flavor\": \"a10g-large\",\n    \"timeout\": \"45m\",\n    \"secrets\": {\"HF_TOKEN\": \"$HF_TOKEN\"},\n    \"env\": {\n        \"ADAPTER_MODEL\": \"username/my-finetuned-model\",\n        \"BASE_MODEL\": \"Qwen/Qwen2.5-0.5B\",\n        \"OUTPUT_REPO\": \"username/my-model-gguf\"\n    }\n})\n```\n\n## Common Training Patterns\n\nSee `references/training_patterns.md` for detailed examples including:\n- Quick demo (5-10 minutes)\n- Production with checkpoints\n- Multi-GPU training\n- DPO training (preference learning)\n- GRPO training (online RL)\n\n## Common Failure Modes\n\n### Out of Memory (OOM)\n\n**Fix (try in order):**\n1. Reduce batch size: `per_device_train_batch_size=1`, increase `gradient_accumulation_steps=8`. Effective batch size is `per_device_train_batch_size` x `gradient_accumulation_steps`. For best performance keep effective batch size close to 128. \n2. Enable: `gradient_checkpointing=True`\n3. Upgrade hardware: t4-small → l4x1, a10g-small → a10g-large etc. \n\n### Dataset Misformatted\n\n**Fix:**\n1. Validate first with dataset inspector:\n   ```bash\n   uv run https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py \\\n     --dataset name --split train\n   ```\n2. Check output for compatibility markers (✓ READY, ✗ NEEDS MAPPING, ✗ INCOMPATIBLE)\n3. Apply mapping code from inspector output if needed\n\n### Job Timeout\n\n**Fix:**\n1. Check logs for actual runtime: `hf_jobs(\"logs\", {\"job_id\": \"...\"})`\n2. Increase timeout with buffer: `\"timeout\": \"3h\"` (add 30% to estimated time)\n3. Or reduce training: lower `num_train_epochs`, use smaller dataset, enable `max_steps`\n4. Save checkpoints: `save_strategy=\"steps\"`, `save_steps=500`, `hub_strategy=\"every_save\"`\n\n**Note:** Default 30min is insufficient for real training. Minimum 1-2 hours.\n\n### Hub Push Failures\n\n**Fix:**\n1. Add to job: `secrets={\"HF_TOKEN\": \"$HF_TOKEN\"}`\n2. Add to config: `push_to_hub=True`, `hub_model_id=\"username/model-name\"`\n3. Verify auth: `mcp__huggingface__hf_whoami()`\n4. Check token has write permissions and repo exists (or set `hub_private_repo=True`)\n\n### Missing Dependencies\n\n**Fix:**\nAdd to PEP 723 header:\n```python\n# /// script\n# dependencies = [\"trl>=0.12.0\", \"peft>=0.7.0\", \"trackio\", \"missing-package\"]\n# ///\n```\n\n## Troubleshooting\n\n**Common issues:**\n- Job times out → Increase timeout, reduce epochs/dataset, use smaller model/LoRA\n- Model not saved to Hub → Check push_to_hub=True, hub_model_id, secrets=HF_TOKEN\n- Out of Memory (OOM) → Reduce batch size, increase gradient accumulation, enable LoRA, use larger GPU\n- Dataset format error → Validate with dataset inspector (see Dataset Validation section)\n- Import/module errors → Add PEP 723 header with dependencies, verify format\n- Authentication errors → Check `mcp__huggingface__hf_whoami()`, token permissions, secrets parameter\n\n**See:** `references/troubleshooting.md` for complete troubleshooting guide\n\n## Resources\n\n### References (In This Skill)\n- `references/training_methods.md` - Overview of SFT, DPO, GRPO, KTO, PPO, Reward Modeling\n- `references/training_patterns.md` - Common training patterns and examples\n- `references/gguf_conversion.md` - Complete GGUF conversion guide\n- `references/trackio_guide.md` - Trackio monitoring setup\n- `references/hardware_guide.md` - Hardware specs and selection\n- `references/hub_saving.md` - Hub authentication troubleshooting\n- `references/troubleshooting.md` - Common issues and solutions\n\n### Scripts (In This Skill)\n- `scripts/train_sft_example.py` - Production SFT template\n- `scripts/train_dpo_example.py` - Production DPO template\n- `scripts/train_grpo_example.py` - Production GRPO template\n- `scripts/estimate_cost.py` - Estimate time and cost (offer when appropriate)\n- `scripts/convert_to_gguf.py` - Complete GGUF conversion script\n\n### External Scripts\n- [Dataset Inspector](https://huggingface.co/datasets/mcp-tools/skills/raw/main/dataset_inspector.py) - Validate dataset format before training (use via `uv run` or `hf_jobs`)\n\n### External Links\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [TRL Jobs Training Guide](https://huggingface.co/docs/trl/en/jobs_training)\n- [TRL Jobs Package](https://github.com/huggingface/trl-jobs)\n- [HF Jobs Documentation](https://huggingface.co/docs/huggingface_hub/guides/jobs)\n- [TRL Example Scripts](https://github.com/huggingface/trl/tree/main/examples/scripts)\n- [UV Scripts Guide](https://docs.astral.sh/uv/guides/scripts/)\n- [UV Scripts Organization](https://huggingface.co/uv-scripts)\n\n## Key Takeaways\n\n1. **Submit scripts inline** - The `script` parameter accepts Python code directly; no file saving required unless user requests\n2. **Jobs are asynchronous** - Don't wait/poll; let user check when ready\n3. **Always set timeout** - Default 30 min is insufficient; minimum 1-2 hours recommended\n4. **Always enable Hub push** - Environment is ephemeral; without push, all results lost\n5. **Include Trackio** - Use example scripts as templates for real-time monitoring\n6. **Offer cost estimation** - When parameters are known, use `scripts/estimate_cost.py`\n7. **Use UV scripts (Approach 1)** - Default to `hf_jobs(\"uv\", {...})` with inline scripts; TRL maintained scripts for standard training; avoid bash `trl-jobs` commands in Claude Code\n8. **Use hf_doc_fetch/hf_doc_search** for latest TRL documentation\n9. **Validate dataset format** before training with dataset inspector (see Dataset Validation section)\n10. **Choose appropriate hardware** for model size; use LoRA for models >7B\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - hugging-face-model-trainer\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "hugging-face-model-trainer",
          "skill_dir": "hugging-face-model-trainer",
          "skill_file_tree": "SKILL.md\nreferences/gguf_conversion.md\nreferences/hardware_guide.md\nreferences/hub_saving.md\nreferences/reliability_principles.md\nreferences/trackio_guide.md\nreferences/training_methods.md\nreferences/training_patterns.md\nreferences/troubleshooting.md\nscripts/convert_to_gguf.py\nscripts/dataset_inspector.py\nscripts/estimate_cost.py\nscripts/train_dpo_example.py\nscripts/train_grpo_example.py\nscripts/train_sft_example.py",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n1. **What domain knowledge does this skill teach?**\n   - Playwright-based web automation patterns\n   - Decision tree for static vs dynamic web apps\n   - The \"reconnaissance-then-action\" pattern for discovering selectors\n   - Critical timing considerations - waiting for `networkidle` before inspecting dynamic content\n   - Server lifecycle management for testing\n\n2. **What workflow decisions does it enable?**\n   - When to inspect HTML directly vs use browser automation\n   - When to wait for network/JS completion before inspecting DOM\n   - How to discover selectors through inspection rather than guessing\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - **Timing issues on dynamic apps** - the skill explicitly calls out the common pitfall of inspecting DOM before JavaScript finishes rendering\n   - Not knowing to use the reconnaissance-then-action pattern\n   - Race conditions between page load and element interaction\n\nThe key practical insight is the `networkidle` wait before DOM inspection on dynamic apps - this is a real-world pain point that causes flaky tests.\n\n```yaml\nname: playwright-dynamic-app-timing\nskills:\n  - webapp-testing\nprompt: |\n  I'm writing a Playwright script to automate testing of a React single-page application. The app loads data from an API and renders a list of items with \"Add to Cart\" buttons.\n\n  My current script looks like this:\n\n  ```python\n  from playwright.sync_api import sync_playwright\n\n  with sync_playwright() as p:\n      browser = p.chromium.launch(headless=True)\n      page = browser.new_page()\n      page.goto('http://localhost:3000')\n      \n      # Find all Add to Cart buttons\n      buttons = page.locator('button:has-text(\"Add to Cart\")').all()\n      print(f\"Found {len(buttons)} buttons\")\n      \n      # Click the first one\n      if buttons:\n          buttons[0].click()\n      \n      browser.close()\n  ```\n\n  The problem is that `buttons` is always an empty list, even though I can see the buttons when I manually open the page in a browser. The buttons appear after the API call completes and React renders the product list.\n\n  What's wrong with my script and how should I fix it?\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly identifies and fixes the timing issue in the Playwright script for a dynamic React application.\n\n    The core problem: The script inspects the DOM immediately after navigation, before JavaScript/React has fetched data and rendered the content. The buttons don't exist yet when the locator runs.\n\n    Score 1.0 if the response:\n    - Correctly identifies the timing/race condition issue (DOM inspected before JS rendering completes)\n    - Recommends waiting for the page to fully load using one of these valid approaches:\n      - `page.wait_for_load_state('networkidle')` \n      - `page.wait_for_selector()` to wait for specific elements\n      - `page.wait_for_timeout()` as a less ideal but functional solution\n      - Any combination of appropriate waits\n    - Provides corrected code or clear instructions on where to add the wait\n\n    Score 0.5 if the response:\n    - Identifies it's a timing/async issue but gives incomplete or vague solutions\n    - OR provides a working solution but doesn't explain why the original failed\n\n    Score 0.0 if the response:\n    - Doesn't identify the timing issue\n    - Suggests the problem is with the selector syntax, browser settings, or other unrelated issues\n    - Gives completely wrong advice\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about async timing in web automation (non-obviousness: 9/10) - the networkidle wait pattern applies broadly across dynamic web apps, not just this skill. The grader is robust, accepting multiple valid wait strategies (clarity: 9/10). The scenario is highly realistic - debugging timing issues in SPA automation is a common practitioner problem (realism: 9/10). Minor deduction: the test could be slightly more open-ended to avoid telegraphing that it's specifically a 'timing' issue, though the realistic framing as a debugging scenario mitigates this."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "## Analysis\n\nLet me first analyze what this skill teaches:\n\n**1. What domain knowledge does this skill teach?**\n- Testing web applications using browser automation (Playwright)\n- The distinction between static HTML and dynamic JavaScript-rendered applications\n- DOM inspection and selector discovery strategies\n- Server lifecycle management for testing\n\n**2. What workflow decisions does it enable?**\n- **Critical decision**: When to wait for content to load vs. acting immediately\n- Whether to inspect rendered DOM vs. reading source HTML\n- The reconnaissance-then-action pattern: inspect first, identify selectors, then act\n- When to use helper scripts vs. writing custom automation\n\n**3. What would a practitioner actually struggle with without this skill?**\n- **The key pain point**: Timing issues with dynamic web applications - trying to interact with elements before JavaScript renders them. This causes flaky tests and \"element not found\" errors.\n- Not understanding that they need to inspect the *rendered* DOM after JavaScript execution\n- Skipping the reconnaissance step and guessing at selectors that may not exist yet\n\nThe most valuable insight is the **reconnaissance-then-action pattern** and understanding that **dynamic apps require waiting for rendering before inspection/interaction**.\n\n## Test Case\n\n```yaml\nname: dynamic-webapp-element-timing\nskills:\n  - webapp-testing\nprompt: |\n  You need to write a Playwright automation script to test a single-page application (SPA) that loads content dynamically with JavaScript. The application is a todo list that:\n  \n  - Runs on http://localhost:3000\n  - Initially shows a loading spinner\n  - After 2 seconds, JavaScript renders a list of todo items as <li> elements\n  - Has an \"Add Todo\" button that appears after the items load\n  \n  Your task: Write a Playwright script that navigates to the app, waits for it to fully load, captures a screenshot of the loaded page, and clicks the \"Add Todo\" button.\n  \n  The server is already running. What approach should you take to ensure your script works reliably?\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of dynamic web app testing patterns.\n    \n    Score 1.0 if the response:\n    - Explicitly waits for content to load before interacting (using networkidle, wait_for_selector, or explicit timeouts)\n    - Shows awareness that JavaScript-rendered content needs time to appear\n    - Uses a pattern of: navigate → wait → inspect/screenshot → interact\n    - Mentions waiting for the page to be ready before taking the screenshot or clicking\n    \n    Score 0.5 if the response:\n    - Includes some wait mechanism but doesn't clearly explain why it's needed\n    - Takes a screenshot and clicks but without proper sequencing or wait strategies\n    - Shows partial understanding of timing issues\n    \n    Score 0.0 if the response:\n    - Navigates and immediately tries to click without any waiting\n    - Doesn't acknowledge that dynamic content needs load time\n    - Would likely fail due to timing issues (clicking before elements exist)\n    \n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It effectively tests the reconnaissance-then-action pattern and timing awareness for dynamic webapps - genuine transferable knowledge that practitioners need. The grader is robust, accepting multiple valid wait strategies (networkidle, wait_for_selector, timeouts) rather than requiring exact implementation. The scenario is realistic (testing a SPA with delayed rendering). Minor deduction: The prompt could be slightly more open-ended to avoid telegraphing the solution (mentioning '2 seconds' and 'loading spinner' hints at timing issues), and the grader could more explicitly reward understanding of why inspection should happen after waiting. Overall, this is a high-quality test that measures practical domain knowledge rather than trivia."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this webapp-testing skill, I need to identify the core knowledge it teaches:\n\n1. **What domain knowledge does this skill teach?**\n   - Testing dynamic web applications with Playwright\n   - The critical difference between static HTML and JavaScript-rendered content\n   - The reconnaissance-then-action pattern for discovering selectors\n   - Managing server lifecycle during testing\n\n2. **What workflow decisions does it enable?**\n   - Deciding when to wait for JavaScript execution (`networkidle`) vs immediate DOM inspection\n   - When to use helper scripts for server management vs manual approaches\n   - How to approach selector discovery in dynamically rendered applications\n\n3. **What would a practitioner actually struggle with without this skill?**\n   - **Critical issue**: Trying to find elements before JavaScript has rendered them, leading to empty selectors or missing content\n   - Not knowing how to handle multiple server dependencies\n   - Writing brittle scripts that fail on dynamic content\n\nThe most important insight this skill teaches is waiting for JavaScript to fully execute before inspecting the DOM in dynamic applications. This is a real pain point developers face when automating modern SPAs.\n\n```yaml\nname: dynamic-webapp-element-discovery\nskills:\n  - webapp-testing\nprompt: |\n  I have a React application running on localhost:3000 that displays a list of products fetched from an API. \n  The products are rendered dynamically after the page loads.\n  \n  I need to write a Playwright script to:\n  1. Navigate to the application\n  2. Find and count how many product cards are displayed\n  3. Click on the first product's \"Add to Cart\" button\n  \n  The HTML structure uses divs with class=\"product-card\" for each product, and buttons with \n  class=\"add-to-cart\" for the cart buttons.\n  \n  My current script navigates to the page but can't find any products. What am I likely doing wrong \n  and how should I fix it?\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the response correctly identifies and solves the dynamic content loading issue.\n    \n    Score 1.0 if: The response identifies that the issue is likely not waiting for JavaScript/dynamic \n    content to load, AND provides a correct solution using page.wait_for_load_state('networkidle') or \n    equivalent waiting strategy before attempting to find elements, AND demonstrates the correct \n    pattern of: navigate -> wait for content -> find selectors -> interact with elements.\n    \n    Score 0.5 if: The response identifies the timing/loading issue but provides an incomplete or \n    suboptimal solution (e.g., using arbitrary timeouts instead of networkidle, or suggesting \n    wait_for_selector without explaining the broader pattern).\n    \n    Score 0.0 if: The response doesn't identify the core issue of waiting for dynamic content to \n    render, or provides solutions that wouldn't work for JavaScript-rendered applications.\n    \n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about waiting for dynamic content in JavaScript-rendered applications, which is a genuine practitioner pain point (non-obviousness: 9/10). The grader is mostly robust, accepting solutions that demonstrate understanding of the networkidle pattern or equivalent waiting strategies, though it could be slightly more explicit about accepting alternative valid approaches like wait_for_selector with proper conditions (clarity: 8/10). The scenario is highly realistic - debugging why elements aren't found in a React SPA is a common real-world problem developers encounter when writing automation scripts (realism: 9/10). Minor deduction for the grader being somewhat prescriptive about the exact solution pattern rather than focusing purely on whether the core concept is understood."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "Based on the skill document, the core value proposition is the **workflow for handling local server lifecycles** during testing. The document explicitly advises against writing custom server management logic inside Python scripts and instead promotes the `scripts/with_server.py` helper. It also emphasizes waiting for `networkidle` for dynamic apps.\n\nA good test case targets a scenario where a user needs to test a dynamic app that isn't currently running, checking if the model correctly adopts the helper script pattern rather than trying to `subprocess.Popen` the server manually or assuming it's already up.\n\n```yaml\nname: dynamic-webapp-automation-setup\nskills:\n  - webapp-testing\nprompt: |\n  I have a local React application located in the `./frontend` directory. It starts using `npm run dev` and runs on port 5173. \n  \n  I need to run a quick automation task to verify the app renders correctly. The server is currently stopped.\n  \n  Please provide:\n  1. A Python script using Playwright to navigate to the local app, wait for it to fully load, and take a screenshot named `home.png`.\n  2. The shell command to run this script, ensuring the server starts up before the test and shuts down afterwards.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate if the solution correctly applies the `webapp-testing` workflows.\n\n    Score 1.0 if:\n    - The shell command uses `python scripts/with_server.py` (or similar path) to manage the server lifecycle.\n    - The arguments for `with_server.py` correctly specify the server command (`npm run dev`) and port (`5173`).\n    - The Python script uses `sync_playwright` and `headless=True`.\n    - The Python script includes `page.wait_for_load_state('networkidle')` (or a very robust selector wait) to ensure the React app hydrates before the screenshot.\n\n    Score 0.5 if:\n    - The Python script is correct (includes networkidle/waits) but the shell command misses the `with_server.py` helper (e.g., tells the user to run npm start manually in a separate tab).\n    - Uses `with_server.py` correctly but the Python script fails to wait for dynamic content loading (missing `networkidle`).\n\n    Score 0.0 if:\n    - Writes custom Python code to start the server (e.g., using `subprocess`), violating the \"black box\" guideline.\n    - Fails to use Playwright.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about server lifecycle management and dynamic content loading patterns (non-obviousness: 9/10). The grader is mostly robust, accepting the core workflow while appropriately penalizing anti-patterns, though it could be slightly more explicit about alternative valid approaches (clarity: 8/10). The scenario is highly realistic - testing a local React app is a common real-world task (realism: 9/10). Minor deduction for the grader being somewhat prescriptive about exact patterns (e.g., 'or similar path', 'or a very robust selector wait' suggests some flexibility but could be clearer)."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "## Analysis\n\n1.  **What domain knowledge does this skill teach?**\n    The skill covers the lifecycle management of local web applications during automated testing. It emphasizes the difference between static and dynamic web content and the necessity of \"hydration\" (waiting for JavaScript to execute) before interacting with the DOM. It also teaches the \"Reconnaissance-Then-Action\" pattern for discovering selectors in a headless environment.\n\n2.  **What workflow decisions does it enable?**\n    It enables the decision to use a wrapper script (`with_server.py`) to manage server processes (startup, port-waiting, and shutdown) rather than trying to manage background processes manually in a shell. It also guides the practitioner to use `networkidle` for dynamic apps to prevent race conditions where scripts attempt to interact with elements that haven't been rendered yet.\n\n3.  **What would a practitioner actually struggle with without this skill?**\n    A practitioner would likely encounter \"Connection Refused\" errors (running the test before the server is ready), \"Element Not Found\" errors (querying a dynamic React/Vue app before the JS has rendered the UI), and \"Orphaned Processes\" (the server continues to run after the test script crashes or finishes).\n\n---\n\n```yaml\nname: test-dynamic-webapp-lifecycle\nskills:\n- webapp-testing\nprompt: |\n  I have a dynamic React application in the `./frontend` directory. \n  - The app starts with the command `npm run dev` and runs on port 5173.\n  - I need to automate a test that:\n    1. Navigates to the application.\n    2. Clicks a button that has the text \"Initialize\".\n    3. Waits for a message to appear in a `div` with the ID `status-display`.\n    4. Confirms the message contains the word \"Ready\".\n\n  Please provide:\n  1. The Python automation script (using Playwright).\n  2. The shell command to execute the test while ensuring the local server is started and stopped correctly using the project's helper utilities.\n\n  Note: The \"Initialize\" button is rendered dynamically by React after the initial page load.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the response based on the following criteria:\n\n    1. Server Management: Does the user provide a shell command using `scripts/with_server.py` with the correct arguments (`--server \"npm run dev\" --port 5173`)?\n    2. Lifecycle: Does the Python script use `sync_playwright` and properly close the browser?\n    3. Dynamic Content Handling: Does the script use `page.wait_for_load_state('networkidle')` or an equivalent robust wait (like `wait_for_selector`) before trying to click the \"Initialize\" button? This is crucial since the prompt mentions the button is dynamic.\n    4. Assertions: Does the script correctly locate `#status-display` and check for the text \"Ready\"?\n\n    Score 1.0 if:\n    - The shell command correctly uses `scripts/with_server.py` to wrap the python execution.\n    - The Python script uses `networkidle` or `wait_for_selector` for the dynamic button.\n    - The Playwright logic is syntactically correct and follows the \"Reconnaissance-Then-Action\" principle.\n\n    Score 0.5 if:\n    - The user provides the Playwright script but fails to use `with_server.py` for lifecycle management, OR\n    - The user uses `with_server.py` but fails to include a wait for dynamic content (networkidle), which would cause the test to flake.\n\n    Score 0.0 if:\n    - The user does not use Playwright or provides a solution that doesn't address the local server requirement.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (server lifecycle management, dynamic content handling with networkidle, reconnaissance-then-action pattern) and creates a realistic scenario. The grader is mostly robust, accepting multiple valid approaches (networkidle OR wait_for_selector). However, there's minor brittleness: the grader requires exact use of 'scripts/with_server.py' when practitioners might use alternative valid approaches (manual server management with proper waits, docker-compose, etc.). The non-obviousness is good - it tests transferable knowledge about dynamic web app testing rather than trivia. The realism is excellent - this represents a common real-world testing scenario. Overall, a high-quality test with minor room for improvement in grader flexibility."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "I need to analyze the skill document first to understand the domain knowledge and workflow decisions it teaches.\n\n## Analysis of the Skill Document\n\n**What domain knowledge does this skill teach?**\n- Web application testing using Playwright\n- Server lifecycle management for local development\n- Dynamic vs static web application testing approaches\n- DOM inspection and element discovery patterns\n- Browser automation best practices\n\n**What workflow decisions does it enable?**\n- When to treat an app as static vs dynamic\n- Whether to use helper scripts vs direct Playwright\n- Single vs multiple server setup decisions\n- When to wait for networkidle before inspection\n- Reconnaissance-then-action testing methodology\n\n**What would a practitioner struggle with without this skill?**\n- Knowing how to properly start and manage test servers\n- Understanding the critical importance of waiting for networkidle\n- Distinguishing between static HTML and dynamic webapp approaches\n- Using reconnaissance patterns to discover selectors effectively\n- Managing multiple servers for complex applications\n\nThe key insight here is that without this skill, a practitioner would likely:\n1. Try to inspect DOM elements before they're loaded (causing flaky tests)\n2. Not know about the helper scripts for server management\n3. Use inappropriate approaches for static vs dynamic apps\n4. Write manual server startup code instead of using provided tools\n\nNow I'll write a test case that tests this practical knowledge.\n\n```yaml\nname: dynamic-webapp-automation-testing\nskills:\n- webapp-testing\nprompt: |\n  You need to automate testing of a local React development server that runs on port 3000. The application has dynamic content that loads via API calls after initial page load. Your task is to:\n\n  1. Start the development server automatically\n  2. Navigate to the application\n  3. Wait for all dynamic content to load properly\n  4. Identify and interact with a button that appears after data loading\n  5. Take a screenshot showing the loaded state\n\n  Write a complete automation script that handles the server lifecycle and properly waits for dynamic content before attempting to interact with elements. The script should be robust and handle the fact that the button may not be visible immediately on page load.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the automation script based on these criteria:\n\n    Score 1.0 if: The response includes a complete script that demonstrates understanding of:\n    - Using the with_server.py helper script for server management\n    - Properly waiting for networkidle state before element inspection\n    - Including reconnaissance steps (screenshot/DOM inspection) \n    - Using appropriate waits for dynamic elements\n    - Proper browser lifecycle management (launch with close)\n\n    Score 0.5 if: The response includes some correct concepts but misses key elements like:\n    - Networkidle waiting OR server management, but not both\n    - Basic Playwright automation but lacking proper dynamic content handling\n    - Manual server startup instead of using helper scripts\n\n    Score 0.0 if: The response lacks understanding of core concepts:\n    - No server management approach mentioned\n    - No waiting for dynamic content loading\n    - Attempts to inspect DOM elements prematurely\n    - Missing basic Playwright structure or browser lifecycle management\n  extractor: last_assistant\n``````yaml\nname: dynamic-webapp-automation-testing\nskills:\n- webapp-testing\nprompt: |\n  You need to automate testing of a local React development server that runs on port 3000. The application has dynamic content that loads via API calls after initial page load. Your task is to:\n\n  1. Start the development server automatically\n  2. Navigate to the application\n  3. Wait for all dynamic content to load properly\n  4. Identify and interact with a button that appears after data loading\n  5. Take a screenshot showing the loaded state\n\n  Write a complete automation script that handles the server lifecycle and properly waits for dynamic content before attempting to interact with elements. The script should be robust and handle the fact that the button may not be visible immediately on page load.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the automation script based on these criteria:\n\n    Score 1.0 if: The response includes a complete script that demonstrates understanding of:\n    - Using the with_server.py helper script for server management\n    - Properly waiting for networkidle state before element inspection\n    - Including reconnaissance steps (screenshot/DOM inspection) \n    - Using appropriate waits for dynamic elements\n    - Proper browser lifecycle management (launch with close)\n\n    Score 0.5 if: The response includes some correct concepts but misses key elements like:\n    - Networkidle waiting OR server management, but not both\n    - Basic Playwright automation but lacking proper dynamic content handling\n    - Manual server startup instead of using helper scripts\n\n    Score 0.0 if: The response lacks understanding of core concepts:\n    - No server management approach mentioned\n    - No waiting for dynamic content loading\n    - Attempts to inspect DOM elements prematurely\n    - Missing basic Playwright structure or browser lifecycle management\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about dynamic web application testing, particularly the critical concept of waiting for networkidle before DOM inspection and using server lifecycle management tools. The grader is mostly robust, accepting multiple valid approaches while focusing on core concepts rather than exact implementation details. The task is highly realistic - automating tests for dynamic React applications is a common real-world scenario. Minor deductions: the grader could be slightly more explicit about accepting alternative server management approaches (though it does give partial credit), and the prompt could specify what command starts the server to make it fully executable. Overall, this is a strong test that measures genuine skill knowledge rather than trivia."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze the core domain knowledge:\n\n## Analysis\n\n**What domain knowledge does this teach?**\n- How to choose between approaches for static vs. dynamic web applications\n- When to use server lifecycle management helpers vs. direct Playwright scripts\n- The critical importance of waiting for dynamic content (`networkidle`) before inspecting/interacting with the DOM\n- The \"reconnaissance-then-action\" pattern: inspect rendered state → discover selectors → execute actions\n- Treating helper scripts as black boxes rather than reading their source\n\n**What workflow decisions does it enable?**\n- Is this HTML static or JavaScript-driven? (determines approach)\n- Does the server need to be managed? (determines script selection)\n- Should I inspect the DOM before or after waiting? (affects test reliability)\n- Should I read helper script source or use `--help`? (affects context efficiency)\n\n**What practitioners struggle with without this skill?**\n- Timing failures from inspecting DOM before dynamic content loads\n- Not knowing helper scripts exist, reimplementing server management\n- Taking random actions without first discovering actual selectors from rendered state\n- Bloating context by reading large helper scripts instead of using them as APIs\n\n---\n\n```yaml\nname: dynamic-webapp-reconnaissance-pattern\nskills:\n  - webapp-testing\nprompt: |\n  You need to test a locally-running React web application. The app displays a form that initially shows only a \"Submit\" button. When the button is clicked, the form loads additional fields dynamically via an API call (takes about 1-2 seconds), and then displays validation errors if any fields are invalid.\n\n  Your task is to write an automated test that:\n  1. Loads the form page\n  2. Clicks the \"Submit\" button\n  3. Waits for the dynamic fields to appear\n  4. Verifies that a specific validation error message is displayed\n\n  The application runs on `http://localhost:3000` and is already running.\n\n  Explain your testing approach. What would you do FIRST before attempting to interact with elements on the page? Why is this important for testing dynamic content?\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of the webapp-testing skill's core principles.\n\n    Score 1.0 if the response includes ALL of:\n    - Recognition that the application is dynamic (JavaScript-driven with async behavior)\n    - Explicit mention of waiting for page to stabilize (using networkidle or similar) BEFORE inspecting DOM or interacting\n    - The reconnaissance-then-action pattern: first inspect/screenshot the rendered page to discover selectors, then use those selectors for actions\n    - Correct reasoning about WHY this order matters (dynamic content not yet loaded if you don't wait)\n    - Use of Playwright with proper synchronous API (sync_playwright) and appropriate waits\n\n    Score 0.5 if the response includes:\n    - General understanding that waiting is needed for dynamic content\n    - Mentions of Playwright and page navigation\n    - BUT missing either: explicit reconnaissance step before action, or clear explanation of the timing issue, or specific wait strategies (networkidle)\n\n    Score 0.0 if the response:\n    - Attempts to interact with elements without any waiting strategy\n    - Suggests inspecting the HTML source code instead of the rendered page\n    - Shows no understanding of the difference between static and dynamic web testing\n    - Provides only generic web testing advice without Playwright-specific patterns\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (reconnaissance-then-action, networkidle waiting, dynamic vs static) and creates a realistic scenario. However, it has moderate clarity issues: the grader is overly prescriptive about implementation details (requiring 'sync_playwright' specifically, 'networkidle' by name) when the skill allows alternatives like wait_for_selector or wait_for_timeout. The test also asks for an explanation rather than executable code, making it somewhat less realistic than having the agent actually write and run the test. Non-obviousness is good (tests transferable timing/inspection patterns), but the grader's brittleness around specific API choices reduces overall quality."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (for test authoring)\n# 1) Domain knowledge taught:\n#    - How to use Playwright (Python) to test local webapps, especially dynamic JS apps.\n#    - The importance of waiting for the app to finish loading (e.g., `networkidle`) before inspecting DOM or interacting.\n#    - A reconnaissance-then-action workflow: navigate → wait → inspect (screenshot/DOM/logs) → pick selectors → act.\n#    - Managing local server lifecycle via a helper runner (start one or multiple servers, then run the Playwright script).\n#\n# 2) Workflow decisions enabled:\n#    - Static HTML vs dynamic webapp: whether you can choose selectors from raw HTML or must inspect rendered DOM.\n#    - Whether the server is already running: if not, use a server-lifecycle helper rather than manually orchestrating.\n#    - When to take screenshots / dump DOM (only after waiting), and how to choose stable selectors (role/text).\n#\n# 3) What practitioners struggle with without this skill (test target):\n#    - Flaky failures: selectors not found because DOM is inspected too early or JS hasn’t rendered yet.\n#    - Confusion about how to reliably start local dev servers for E2E runs (especially multi-server setups).\n#    - Lack of a systematic debug loop (screenshot + console logs + DOM dump) before attempting interactions.\n\nname: dynamic-webapp-e2e-reconnaissance-with-server-helper\nskills:\n  - webapp-testing\nprompt: |\n  You need to create a one-off Playwright (Python) script to validate a *dynamic* local web app.\n\n  Scenario:\n  - Frontend dev server: runs on http://localhost:5173 (command: `npm run dev`)\n  - Backend API server: runs on http://localhost:3000 (command: `python server.py` from `backend/`)\n  - The app renders its main UI only after fetching `/api/me` and `/api/items`.\n  - Your automation goal: open the app, wait until it's fully loaded, click a button labeled \"New Item\",\n    fill an input with placeholder \"Item name\" with the text \"Test Widget\", click \"Save\", and then verify that\n    \"Test Widget\" appears somewhere on the page.\n\n  Requirements:\n  1) Assume the servers are NOT already running. The repo provides a helper script in `scripts/` that can manage\n     server lifecycles for one or multiple servers. Show how you'd use it (include the idea of running `--help` first).\n  2) Provide the Playwright script (sync API) that follows a reconnaissance-then-action approach:\n     - navigate\n     - wait for the right load state for a dynamic app before inspecting or selecting elements\n     - take a screenshot and/or print some useful inspection output to help choose selectors\n     - then perform the actions and assertions\n  3) Run Chromium headless and close the browser cleanly.\n  4) If the click/selectors fail, include at least one debugging technique (e.g., console logging or DOM dump)\n     in your script.\n\n  Output only the command(s) you would run and the Python script contents.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates the practical workflow and key decisions for testing a dynamic local webapp with Playwright plus server lifecycle management.\n\n    Score 1.0 if the answer includes ALL of:\n    - Server orchestration: explicitly indicates using the provided helper in `scripts/` to start servers (multi-server is acceptable/expected here) and indicates running it with `--help` first (not necessarily exact output).\n    - Dynamic-app synchronization: the Playwright script uses an appropriate wait before inspection/actions (e.g., `page.wait_for_load_state('networkidle')` OR an equivalent robust dynamic wait such as waiting for a key element that appears after data fetch). It must be clear that inspection/actions happen after this wait.\n    - Reconnaissance-then-action: includes at least one inspection step prior to final selectors/actions (e.g., screenshot, `page.content()` dump, listing locators) and then proceeds to interact using discovered/stable selectors.\n    - Headless Chromium with sync Playwright and clean shutdown (launch headless, close browser).\n    - Includes at least one debugging aid for failures (console logging, DOM dump, screenshot on failure, etc.).\n    - Performs the requested actions (click \"New Item\", fill \"Item name\" with \"Test Widget\", click \"Save\") and verifies \"Test Widget\" appears (assertion via text/locator).\n\n    Score 0.5 if the answer is directionally correct but missing ONE OR TWO of the key elements above, such as:\n    - Provides a Playwright script but omits the server helper usage (or omits the `--help` step), OR\n    - Uses Playwright but fails to include an explicit dynamic wait before inspection, OR\n    - Lacks reconnaissance steps and jumps straight to brittle selectors without inspection, OR\n    - Omits any debugging technique, OR\n    - Omits a clear verification/assertion that \"Test Widget\" appears.\n\n    Score 0.0 if:\n    - No workable Playwright (Python) solution is provided, OR\n    - The approach would predictably be flaky for a dynamic app (e.g., inspects DOM and selects elements immediately after `goto` with no waiting strategy), OR\n    - Does not address server-not-running orchestration at all, OR\n    - Does not attempt the required user flow and verification.\n\n    Be flexible about exact command syntax and selector strategies (role/text/CSS/placeholder), as long as the workflow decisions and robustness are demonstrated.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (reconnaissance-then-action, dynamic waiting, server lifecycle management) and creates a realistic E2E testing scenario. However, it has moderate clarity issues: the grader is quite complex with many requirements that could be interpreted differently, and the test relies heavily on specific workflow details (like running --help first) that border on trivia. The multi-server setup and comprehensive debugging requirements make it realistic but potentially brittle for grading edge cases where valid alternative approaches exist."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "Here’s my reasoning before crafting the test case:\n\n1. **Domain knowledge taught:**  \n   - How to leverage Playwright for local webapp testing, especially dynamic apps.  \n   - Managing server lifecycles via helper scripts (`with_server.py`) rather than manually.  \n   - Importance of waiting for `networkidle` before DOM inspection.  \n   - Reconnaissance-first workflow (inspect, identify selectors, then act).  \n   - Using headless Chromium, proper selectors, and cleanup.\n\n2. **Workflow decisions enabled:**  \n   - Determining whether to treat a task as static HTML vs dynamic webapp.  \n   - Knowing when to run helper scripts to start backend/frontend servers.  \n   - Choosing reconnaissance steps (screenshots, DOM inspection) before interacting.  \n   - Ensuring correct wait strategies (`networkidle`, specific selectors).  \n\n3. **Likely practitioner struggles without skill:**  \n   - Forgetting to launch servers properly (especially multi-server setups).  \n   - Skipping `networkidle`, leading to race conditions with dynamic content.  \n   - Inspecting DOM prematurely or lacking a structured reconnaissance plan.  \n   - Not using helper scripts efficiently (e.g., forgetting to run `--help` or manage multiple servers).  \n\nA strong test should force the model to plan a realistic automation workflow for a dynamic multi-service app, explicitly requiring the helper script usage, network-idle waits, and reconnaissance-first pattern.\n\n---\n\n```yaml\nname: multi-server-recon-automation-plan\nskills:\n  - webapp-testing\nprompt: |\n  You’ve joined a team that maintains a dynamic analytics dashboard. The app consists of:\n    • A Python backend API that must run on http://localhost:3000\n    • A Vite-based frontend served on http://localhost:5173\n\n  Currently, neither server is running. Your task is to verify, via Playwright automation, that clicking the “Generate Report” button causes a “PDF ready” badge to appear on the page.\n\n  Describe the exact workflow you would follow using the provided tooling. Include:\n    1. How you would initialize the environment, including any helper scripts and the order you’d invoke them (remember to follow best practices about running helper scripts with --help before final usage).\n    2. How your Playwright script would be structured (language choices, browser mode, waits, reconnaissance steps such as screenshots or DOM inspection, selector strategy, and cleanup).\n    3. How you’d ensure dynamic content is fully loaded before inspection and action.\n    4. Any additional debugging or logging steps you’d plan if the badge fails to appear.\n\n  Provide concrete commands and illustrative code snippets where relevant, but keep the focus on process and decision-making rather than full boilerplate.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of the webapp-testing skill.\n\n    Score 1.0 if:\n      - It explicitly states running `python scripts/with_server.py --help` before composing the final command AND uses the helper to start both backend and frontend servers in one invocation.\n      - The Playwright plan/code includes: sync Playwright usage, headless Chromium, navigation to the frontend URL, waiting for `page.wait_for_load_state(\"networkidle\")` before DOM inspection, reconnaissance steps (e.g., screenshot or DOM content dump) prior to choosing selectors, and proper browser closure.\n      - It explains how selectors will be identified and used after reconnaissance, and ensures the “PDF ready” badge verification occurs after the click.\n      - It mentions contingency/debug steps such as capturing console logs or additional screenshots if the badge doesn’t appear.\n\n    Score 0.5 if the response shows partial understanding (e.g., mentions with_server.py but not the --help step, or includes Playwright usage without the required networkidle wait or reconnaissance plan).\n\n    Score 0.0 if it fails to reference the helper script workflow or lacks any of the core Playwright best practices (e.g., no networkidle wait, no reconnaissance, no discussion of server management).\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (with_server.py, networkidle waits, reconnaissance pattern) and creates a realistic multi-server scenario. However, the grading criteria are overly prescriptive about implementation details (e.g., requiring explicit mention of '--help' step, specific code structure) which could reject valid alternative approaches that demonstrate the same underlying knowledge. The test measures transferable domain knowledge well but has moderate brittleness in its grader that expects very specific workflow descriptions rather than accepting any approach that demonstrates proper server management, wait strategies, and reconnaissance patterns."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\nname: playwright-dynamic-webapp-recon-then-action\nskills:\n- webapp-testing\nprompt: |\n  You need to smoke-test a local, dynamic web application (React/Vite-style) whose UI elements appear only after client-side JS finishes fetching data.\n\n  Scenario:\n  - The app is NOT running yet.\n  - When the page loads, it initially shows a spinner and only later renders a button labeled \"Create project\".\n  - You want an automation script that reliably clicks \"Create project\" and then verifies a dialog appears containing the text \"New project\".\n  - You also want basic debugging artifacts if it fails (a screenshot and browser console logs).\n\n  Task:\n  1) Describe (briefly) the workflow you would use to avoid flaky selector issues on a dynamic app.\n  2) Provide a Python Playwright (sync) script that:\n     - Assumes the server lifecycle is handled externally by a provided helper wrapper (i.e., your script should contain only Playwright logic, not subprocess management).\n     - Navigates to http://localhost:5173\n     - Waits appropriately for the dynamic UI to be ready (avoid inspecting/clicking too early)\n     - Captures a full-page screenshot for reconnaissance/debugging\n     - Captures and prints browser console logs\n     - Clicks the \"Create project\" button using a robust selector strategy\n     - Verifies the \"New project\" dialog is visible\n     - Closes the browser\n  3) Show the one command you would run to start the server via the helper wrapper and execute your script (include the idea of running --help first; exact arguments may vary).\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates practical skill in testing a local dynamic webapp with Playwright, emphasizing correct workflow decisions that reduce flakiness.\n\n    Score 1.0 if the response includes ALL of the following:\n    - Clearly distinguishes dynamic app behavior and recommends a reconnaissance-then-action workflow (e.g., load → wait → inspect/screenshot → choose selectors → act).\n    - Uses Python Playwright synchronous API (sync_playwright) and launches Chromium in headless mode.\n    - Includes an explicit wait appropriate for dynamic apps (must include waiting for network to settle such as wait_for_load_state('networkidle') OR an equivalent robust readiness wait) AND does this BEFORE DOM inspection/click attempts.\n    - Takes a screenshot (preferably full_page=True) as a debugging/recon artifact.\n    - Captures console logs (e.g., page.on(\"console\", ...) or equivalent) and prints them.\n    - Clicks the \"Create project\" button using a robust selector strategy (get_by_role with name, text= selector, or similarly stable locator) rather than fragile absolute CSS/XPath.\n    - Verifies the \"New project\" dialog/text is visible using an assertion or wait_for_selector/locator check.\n    - Properly closes the browser (and ideally context) to avoid resource leaks.\n    - Provides a realistic command showing use of the provided server-lifecycle helper wrapper to run the script, and mentions running the helper with --help first (exact script name/flags can vary as long as the concept is correct).\n\n    Score 0.5 if the response is mostly correct Playwright automation but is missing ONE OR TWO key best-practice elements above\n    (e.g., no console log capture, no screenshot, no external helper invocation mention, or waits are present but not clearly ordered before inspection/actions).\n\n    Score 0.0 if the response does not provide a workable Playwright-based approach/script for a dynamic local webapp,\n    or omits critical synchronization (no meaningful waits) such that it would be fundamentally flaky/incorrect.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of Playwright testing patterns and includes realistic requirements (reconnaissance-then-action, networkidle waits, console logs, screenshots). However, it suffers from moderate clarity issues: the grader is overly prescriptive about implementation details (requiring specific API calls like 'get_by_role', 'page.on(\"console\")', exact wait methods) which could reject valid alternative approaches. The test also borders on being a checklist of skill-specific details rather than testing transferable domain knowledge about dynamic webapp testing. The realism is strong (8/10) as this represents actual webapp testing scenarios, but non-obviousness (6/10) and clarity (5/10) are compromised by the rigid grading criteria that focuses too much on specific Playwright APIs rather than the underlying concepts of proper synchronization and debugging practices."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis (as YAML comments)\n# 1) Domain knowledge: Dynamic SPAs need explicit waiting for JS/render completion before DOM inspection;\n#    Playwright can validate UI flows, capture screenshots, and surface browser console logs.\n# 2) Workflow decisions: Decide static vs dynamic; if dynamic and server not running, use a server-lifecycle\n#    helper to avoid flaky “start server separately” flows; use a reconnaissance-then-action loop to discover\n#    selectors from the rendered state.\n# 3) Likely struggle without this skill: Flaky tests that click too early, use brittle selectors, lack\n#    debugging artifacts (screenshots/DOM/console), and forget to manage the dev server reliably in CI.\n\nname: dynamic-spa-smoketest-wait-networkidle-and-recon\nskills:\n  - webapp-testing\nprompt: |\n  You need to write a reliable end-to-end smoke test for a local single-page web app.\n\n  Context:\n  - The app is a React/Vite-style SPA served at http://localhost:5173.\n  - On initial load it performs API calls; the main UI (including a button labeled \"Create account\") only appears AFTER the JS finishes and network activity settles.\n  - A previous test was flaky because it tried to inspect/click too early.\n  - In CI, the dev server is NOT running initially.\n  - You have access to a helper runner at `scripts/with_server.py` that can start the server and run your Playwright script in one command.\n\n  Task: Provide BOTH of the following:\n\n  (A) The shell commands you would run:\n      1) first, show that you would check the helper's usage (e.g. with --help),\n      2) then, show a command that starts the dev server (`npm run dev`) on port 5173 and runs your Playwright script.\n\n  (B) A single Python Playwright script (synchronous) that:\n      - launches Chromium in headless mode,\n      - navigates to http://localhost:5173,\n      - waits appropriately for a dynamic SPA to finish loading BEFORE inspecting the DOM (avoid flakiness),\n      - performs a brief reconnaissance step helpful for selector discovery/debugging (e.g., screenshot and/or dump rendered HTML and/or list button texts),\n      - clicks the \"Create account\" button using a robust selector strategy (role/text preferred over brittle CSS),\n      - captures browser console logs (at least errors) and prints them to stdout,\n      - asserts that a confirmation toast/message like \"Account created\" appears after the click,\n      - closes the browser even if an assertion fails.\n\n  Return the answer as command(s) plus the Python code.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant produced a realistic, robust Playwright-based approach for testing a local dynamic SPA and managing the server lifecycle.\n\n    Score 1.0 if ALL of the following are true:\n    - Provides commands that (i) run the helper with `--help` (or explicitly says to check usage first) AND (ii) use the helper to start `npm run dev` on port 5173 and then run the Python script (any equivalent invocation structure is acceptable).\n    - Python solution uses Playwright in Python and is synchronous (e.g., `from playwright.sync_api import sync_playwright` and `with sync_playwright() as p:`).\n    - Launches Chromium headless (e.g., `p.chromium.launch(headless=True)`).\n    - Demonstrates an appropriate dynamic-app wait strategy BEFORE inspection/actions; full credit requires an explicit `page.wait_for_load_state(\"networkidle\")` (or clearly equivalent) prior to reconnaissance and clicking.\n    - Includes at least one reconnaissance/debug step after the wait (e.g., screenshot to a path, printing `page.content()`, listing buttons, etc.).\n    - Clicks \"Create account\" using robust selectors (role/text/get_by_* or `text=`), not only fragile absolute CSS/XPath.\n    - Captures/prints browser console output (at least errors) via `page.on(\"console\", ...)` or an equivalent hook.\n    - Includes an assertion that \"Account created\" (or a clearly described confirmation toast/message) appears.\n    - Ensures browser closure even on failure (try/finally or context-managed closure).\n\n    Score 0.5 if:\n    - Provides a plausible Playwright script that attempts to wait for the UI (e.g., `wait_for_selector`) and performs the click/assertion and closes the browser, BUT misses one or more key robustness elements (e.g., missing explicit networkidle, missing console log capture, missing reconnaissance, or missing the server-helper command).\n\n    Score 0.0 if:\n    - Does not provide a workable Playwright testing approach (wrong tool/language), or omits waiting such that the solution would obviously remain flaky for a dynamic SPA, or provides no actionable code/commands.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of Playwright testing patterns and includes realistic requirements (networkidle wait, reconnaissance, console logging). However, it suffers from significant clarity issues: the grader is overly prescriptive, requiring exact implementation details like `page.wait_for_load_state('networkidle')` rather than accepting equivalent approaches (e.g., `wait_for_selector` with appropriate timeout). The test also demands multiple specific steps (9 bullet points) that could be satisfied in various valid ways, making it brittle. The realism is moderate - while smoke testing SPAs is realistic, the specific combination of all required elements (reconnaissance + console logs + toast assertion + try/finally) feels somewhat contrived for a basic smoke test. Non-obviousness is reasonable as it tests transferable concepts about async UI testing, though some requirements veer toward implementation trivia."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\n1. **Domain knowledge taught:** The skill focuses on effectively testing local web applications using Playwright within Python. It emphasizes managing server lifecycles via provided helper scripts, understanding when an app is static versus dynamic, and the importance of waiting for networkidle before inspecting the DOM. It also covers best practices like using synchronous Playwright context managers, descriptive selectors, appropriate waits, and treating bundled scripts as black boxes invoked via `--help` first.\n\n2. **Workflow decisions enabled:** The decision tree in the skill guides whether to read static HTML directly or treat the app as dynamic, and if the server is running or needs to be launched using `scripts/with_server.py`. It also lays out the reconnaissance-then-action pattern: navigate, wait for networkidle, inspect (screenshot/DOM), identify selectors, and then perform actions. It encourages launching Chromium headless, closing browsers, and managing server life cycles through helper scripts.\n\n3. **Practitioner struggles without the skill:** Without this knowledge, one might attempt to inspect DOM before the page fully loads, leading to missing selectors or stale elements. They might not know to launch helper scripts via `with_server.py` or incorrectly manage servers. They may fail to adopt the reconnaissance-then-action flow, causing unreliable automation. They might also neglect best practices such as closing browsers or launching headless, impairing repeatable testing.\n\n---\n\n```yaml\nname: dynamic-reconnaissance-test\nskills:\n  - webapp-testing\nprompt: |\n  You're assigned to automate a smoke test of a local dynamic web application that requires both backend and frontend servers. The frontend is powered by Vite and the backend is a Flask API; there is no static HTML to inspect ahead of time. Describe your plan for:\n  1. Starting the necessary servers using the provided helper script following the documented practice.\n  2. Ensuring Playwright discovers stable selectors before interacting with the UI.\n  3. Performing an action (e.g., clicking a button or filling a form) and verifying a rendered change.\n  Reference the reconnaissance-then-action pattern and waiting for the appropriate load state in your explanation. No actual code execution is required, but the plan should reflect how you would implement the Playwright script and helper invocation based on the skill description.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if: The response outlines using scripts/with_server.py (with --help first) to launch both backend and frontend servers, references waiting for networkidle before inspecting the DOM, describes taking DOM inspection or screenshots to identify selectors, and explains executing an action (like clicking or filling) followed by verification of a UI change.\n    Score 0.5 if: The response mentions either server orchestration or reconnaissance steps but omits a clear plan tying both together or neglects the critical wait-for-load-state before acting.\n    Score 0.0 if: The response skips using the helper script pattern, fails to mention the reconnaissance-then-action approach, or lacks any mention of waiting for networkidle prior to interacting with the page.\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of the skill's core concepts (reconnaissance-then-action, networkidle waiting, with_server.py usage) and tests transferable knowledge rather than trivia. However, it has significant clarity issues: it asks for a 'plan' or 'explanation' rather than actual execution, making it more of a knowledge-check than a capability test. The grader accepts descriptions of approaches rather than verifying the agent can actually implement them, which reduces robustness. The realism is moderate - while testing dynamic webapps is realistic, asking for a plan description rather than implementation is somewhat artificial. A stronger test would require the agent to write actual Playwright code that demonstrates these concepts in practice."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this webapp-testing skill document, I need to analyze the core domain knowledge and create a test case that measures whether having this skill helps an LLM succeed.\n\n## Analysis\n\n**Domain Knowledge:** This skill teaches web application testing using Playwright, focusing on:\n- Decision-making between static HTML and dynamic webapp approaches\n- Server lifecycle management for local development\n- Reconnaissance-then-action testing patterns\n- Proper timing and synchronization (networkidle waits)\n\n**Key Workflow Decisions:**\n- When to read HTML directly vs use browser automation\n- When to start new servers vs connect to existing ones\n- How to structure DOM inspection before action execution\n\n**What Practitioners Struggle With:** Understanding the decision tree, proper timing for DOM inspection, and server management for local testing environments.\n\nNow I'll create a realistic test case:\n```yaml\nname: local-webapp-functional-testing\nskills:\n  - webapp-testing\nprompt: |\n  You have a local web application running on port 3000 that needs functional testing. The application is a simple todo list with add/remove functionality, but you're not sure if it's fully loaded or what the exact DOM structure looks like.\n\n  Your task:\n  1. Connect to the application at http://localhost:3000\n  2. Wait for it to be fully loaded and ready\n  3. Take a screenshot to inspect the current state\n  4. Identify the DOM elements needed to test the todo functionality\n  5. Write a Playwright script that:\n     - Adds a new todo item\n     - Marks it as complete\n     - Removes another item\n     - Takes a final screenshot showing the results\n\n  Focus on the proper testing workflow: reconnaissance first, then action execution.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates proper webapp testing methodology.\n\n    Score 1.0 if: \n    - Uses proper Playwright initialization with sync_playwright() and chromium\n    - Waits for networkidle or equivalent before DOM inspection\n    - Takes reconnaissance screenshots or DOM inspection before executing actions\n    - Identifies selectors through inspection rather than guessing\n    - Executes realistic todo list actions (add, complete, remove)\n    - Properly manages browser lifecycle (launch and close)\n    - Shows understanding of the reconnaissance-then-action pattern\n\n    Score 0.5 if:\n    - Uses Playwright but misses some best practices (e.g., no networkidle wait)\n    - Attempts todo functionality but with incomplete or incorrect approach\n    - Shows partial understanding of the testing workflow\n\n    Score 0.0 if:\n    - Doesn't use Playwright at all\n    - No functional testing of the webapp\n    - Missing critical steps in the testing workflow\n    - No demonstration of proper browser automation principles\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates good understanding of webapp testing concepts (reconnaissance-then-action, networkidle waits, DOM inspection) and is realistic. However, it has a critical clarity issue: it assumes a server is already running on port 3000, but provides no setup instructions or server code. The grader would need to either provide the todo app or accept solutions that acknowledge the missing server. The non-obviousness is moderate - it tests transferable patterns (wait for load, inspect before acting) rather than trivia, though the specific todo list scenario is somewhat generic. The realism is strong as functional testing of local webapps is common practice."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: test-webapp-testing-decision-tree\nskills:\n  - webapp-testing\nprompt: |\n  You are tasked with automating the testing of a dynamic web application. The application consists of a backend server and a frontend server. The backend server is started with the command `cd backend && python server.py` and runs on port 3000. The frontend server is also running, started with the command `cd frontend && npm run dev` and is accessible on port 5173.\n\n  Write a script that does the following:\n  1. Ensures both the backend and frontend servers are running.\n  2. Navigates to the frontend application.\n  3. Waits for the page to be fully loaded (network idle).\n  4. Takes a screenshot of the page.\n  5. Identifies and clicks a button with the text \"Submit\".\n\n  Provide the necessary commands and Python script to accomplish this task.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria for scoring the response.\n\n    Score 1.0 if:\n    - The response correctly uses the `with_server.py` script to manage both the backend and frontend servers.\n    - The response includes a Python script that navigates to the frontend application, waits for the page to be fully loaded, takes a screenshot, identifies a button with the text \"Submit\", and clicks it.\n\n    Score 0.5 if:\n    - The response partially addresses the task, such as correctly managing the servers but missing one or more steps in the automation script.\n    - The response provides a correct approach but contains minor errors or omissions.\n\n    Score 0.0 if:\n    - The response does not use the `with_server.py` script to manage the servers.\n    - The response fails to include the necessary steps in the automation script, such as waiting for the page to be fully loaded, taking a screenshot, or clicking the button.\n  extractor: last_assistant\n```",
          "score": 0.4,
          "rationale": "The test case has significant issues across all dimensions. For non-obviousness (score: 3/10), it explicitly requires knowing the exact script name 'with_server.py' and specific command syntax, making it trivia rather than testing transferable knowledge about web automation patterns. For clarity (score: 4/10), the grader is overly prescriptive - it penalizes valid alternatives like manually starting servers or using other server management approaches, and requires the exact 'with_server.py' script when the underlying skill is web automation, not script memorization. For realism (score: 5/10), while testing a web app is realistic, the specific requirement to use a particular helper script and the contrived multi-server setup feels artificial - real practitioners would use whatever server management approach works for their context. The test conflates 'knowing how to automate web testing' with 'memorizing this specific skill's helper scripts.'"
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: automating-webapp-testing\nskills:\n- webapp-testing\nprompt: |\n  You are tasked with automating a web application that has a dynamic frontend and a backend API. The frontend is built using React and runs on port 5173, while the backend API is built using Node.js and runs on port 3000. Your task is to write a Playwright script that launches both servers, navigates to the frontend, waits for the network to become idle, and then clicks on a button with the text \"Login\". Provide the complete script.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes a complete and correct Playwright script that launches both servers, navigates to the frontend, waits for the network to become idle, and then clicks on the \"Login\" button. Score 0.5 if the script is mostly correct but has some minor errors. Score 0.0 if the script is incomplete, incorrect, or does not follow the reconnaissance-then-action pattern.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. For non-obviousness (score: 3/10), it asks for specific implementation details like 'port 5173' and 'port 3000' rather than testing transferable knowledge about the reconnaissance-then-action pattern. For clarity (score: 4/10), the grader is problematic because it asks to 'launch both servers' in the Playwright script itself, which contradicts the skill's core teaching that `with_server.py` manages servers externally - a correct solution would NOT launch servers in the Playwright script. The grader would reject the correct approach shown in the skill examples. For realism (score: 4/10), while testing web apps is realistic, the specific task of writing a script to click a login button without any actual verification or assertion is contrived and doesn't represent a meaningful testing scenario that practitioners would encounter."
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Kubernetes Troubleshooting",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 13,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Kubernetes Troubleshooting\n\n**SKILL.md Content:**\n```markdown\n---\nname: Kubernetes Troubleshooting\ndescription: Diagnose and mitigate common Kubernetes issues (CrashLoopBackOff, OOMKilled, probe failures, bad configs) using kubectl and a systematic runbook.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Kubernetes Troubleshooting\n\n## Overview\n\nMost K8s outages are not “Kubernetes is broken” — they’re misconfigurations, missing dependencies, or resource constraints. The key is to distinguish:\n- **The scheduler can’t start it** (ImagePullBackOff, Pending)\n- **It starts but crashes** (CrashLoopBackOff)\n- **It runs but isn’t ready** (readiness probe failures)\n\n## CrashLoopBackOff Runbook\n\n1. Identify scope:\n   - Is it one pod, one node, or all replicas?\n2. `kubectl describe pod <pod>`:\n   - Look at Events: pull errors, probe failures, OOMKilled, restart count.\n3. `kubectl logs <pod> -c <container>`:\n   - If it restarted, also check: `kubectl logs --previous`.\n4. Check exit reasons:\n   - Exit code 137 often implies OOM kill.\n   - Exit code 1 is generic; rely on logs and events.\n5. Validate configuration:\n   - Env vars, ConfigMaps, Secrets, mounted paths, command/args.\n6. Probe sanity:\n   - Liveness should not kill a slow-starting app.\n   - Readiness should reflect “can serve traffic”, not “process is alive”.\n7. Mitigate safely:\n   - Roll back image, scale down, or disable liveness temporarily (with explicit follow-up).\n\n## Multi-Container Pods\n\nAlways specify container names. Many “logs are empty” issues are because you’re reading the sidecar container.\n\n## Common Gotchas\n\n- Readiness depends on a downstream service that’s unavailable → never becomes ready.\n- Probes hit the wrong port/path and kill the pod continuously.\n- Missing Secret/ConfigMap key: app crashes instantly at startup.\n- Resource limits too low → OOM kills under load spikes.\n\n## Checklist\n\n- [ ] Describe pod and read Events\n- [ ] Logs + previous logs for the correct container\n- [ ] Check exit code / OOMKilled\n- [ ] Validate probes and config mounts\n- [ ] Apply smallest safe mitigation and verify recovery\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Kubernetes Troubleshooting\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Kubernetes Troubleshooting\n\n**SKILL.md Content:**\n```markdown\n---\nname: Kubernetes Troubleshooting\ndescription: Diagnose and mitigate common Kubernetes issues (CrashLoopBackOff, OOMKilled, probe failures, bad configs) using kubectl and a systematic runbook.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Kubernetes Troubleshooting\n\n## Overview\n\nMost K8s outages are not “Kubernetes is broken” — they’re misconfigurations, missing dependencies, or resource constraints. The key is to distinguish:\n- **The scheduler can’t start it** (ImagePullBackOff, Pending)\n- **It starts but crashes** (CrashLoopBackOff)\n- **It runs but isn’t ready** (readiness probe failures)\n\n## CrashLoopBackOff Runbook\n\n1. Identify scope:\n   - Is it one pod, one node, or all replicas?\n2. `kubectl describe pod <pod>`:\n   - Look at Events: pull errors, probe failures, OOMKilled, restart count.\n3. `kubectl logs <pod> -c <container>`:\n   - If it restarted, also check: `kubectl logs --previous`.\n4. Check exit reasons:\n   - Exit code 137 often implies OOM kill.\n   - Exit code 1 is generic; rely on logs and events.\n5. Validate configuration:\n   - Env vars, ConfigMaps, Secrets, mounted paths, command/args.\n6. Probe sanity:\n   - Liveness should not kill a slow-starting app.\n   - Readiness should reflect “can serve traffic”, not “process is alive”.\n7. Mitigate safely:\n   - Roll back image, scale down, or disable liveness temporarily (with explicit follow-up).\n\n## Multi-Container Pods\n\nAlways specify container names. Many “logs are empty” issues are because you’re reading the sidecar container.\n\n## Common Gotchas\n\n- Readiness depends on a downstream service that’s unavailable → never becomes ready.\n- Probes hit the wrong port/path and kill the pod continuously.\n- Missing Secret/ConfigMap key: app crashes instantly at startup.\n- Resource limits too low → OOM kills under load spikes.\n\n## Checklist\n\n- [ ] Describe pod and read Events\n- [ ] Logs + previous logs for the correct container\n- [ ] Check exit code / OOMKilled\n- [ ] Validate probes and config mounts\n- [ ] Apply smallest safe mitigation and verify recovery\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Kubernetes Troubleshooting\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Kubernetes Troubleshooting",
          "skill_dir": "kubernetes-troubleshooting",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": [
        {
          "model_name": "claude-opus-4-5-20251101",
          "submission": "## Analysis\n\n**Domain Knowledge:** This skill teaches Excel/spreadsheet manipulation with a strong emphasis on financial modeling best practices. Key concepts include:\n1. Using formulas instead of hardcoded calculated values to keep spreadsheets dynamic\n2. Financial modeling color conventions (blue for inputs, black for formulas)\n3. The critical limitation that openpyxl doesn't evaluate formulas - requiring recalculation\n4. Library selection: pandas for data analysis, openpyxl for formulas/formatting\n\n**Workflow Decisions:**\n- When to use pandas vs openpyxl\n- How to structure financial models with assumptions in separate cells\n- The necessary step of recalculating formulas after creating them with openpyxl\n\n**Practitioner Struggles Without This Skill:**\n- Would hardcode calculated values in Python instead of using Excel formulas, creating static non-updateable spreadsheets\n- Would not know that openpyxl formulas need recalculation to show values\n- Would mix up library choices for different tasks\n\nThe most impactful knowledge is the anti-pattern of calculating in Python and hardcoding results vs using Excel formulas. This is a common mistake that makes spreadsheets useless for end users who expect to modify inputs.\n\n```yaml\nname: dynamic-spreadsheet-formulas-vs-hardcodes\nskills:\n  - xlsx\nprompt: |\n  I'm using Python with openpyxl to create a sales summary spreadsheet. I have quarterly sales data and need to add a row showing the total and another showing the average.\n  \n  Here's my current approach:\n  \n  ```python\n  from openpyxl import Workbook\n  \n  wb = Workbook()\n  sheet = wb.active\n  \n  # Headers\n  sheet['A1'] = 'Quarter'\n  sheet['B1'] = 'Sales'\n  \n  # Data\n  sales_data = [('Q1', 15000), ('Q2', 22000), ('Q3', 18000), ('Q4', 25000)]\n  for i, (quarter, sales) in enumerate(sales_data, start=2):\n      sheet[f'A{i}'] = quarter\n      sheet[f'B{i}'] = sales\n  \n  # Calculate totals in Python\n  total_sales = sum(s[1] for s in sales_data)\n  avg_sales = total_sales / len(sales_data)\n  \n  sheet['A6'] = 'Total'\n  sheet['B6'] = total_sales  # 80000\n  sheet['A7'] = 'Average'\n  sheet['B7'] = avg_sales    # 20000\n  \n  wb.save('sales_summary.xlsx')\n  ```\n  \n  My colleague says this approach has a significant problem for the end users who will update the quarterly numbers. What's wrong with this approach and how should I fix it?\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly identifies and fixes the hardcoding problem in spreadsheet creation.\n\n    Score 1.0 if the response:\n    - Correctly identifies that calculating values in Python and writing them as static numbers makes the spreadsheet non-dynamic (users can't update Q1-Q4 and have totals recalculate automatically)\n    - Recommends using Excel formulas like =SUM(B2:B5) and =AVERAGE(B2:B5) instead of hardcoded values\n    - Shows corrected code using formula strings (e.g., sheet['B6'] = '=SUM(B2:B5)')\n\n    Score 0.5 if the response:\n    - Identifies the dynamic/updateable nature problem OR recommends using formulas\n    - But doesn't fully explain both the problem AND the solution with corrected code\n\n    Score 0.0 if the response:\n    - Doesn't identify that hardcoding calculated values prevents the spreadsheet from being dynamic\n    - Suggests irrelevant fixes (formatting, error handling) without addressing the formula issue\n    - Doesn't mention using Excel formulas instead of Python-calculated values\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable knowledge about dynamic spreadsheets vs. hardcoded values - a genuine gotcha that applies broadly beyond Excel (non-obviousness: 9/10). The grader is robust, accepting explanations that identify the problem and solution without requiring exact syntax (clarity: 9/10). The scenario is highly realistic - practitioners frequently make this mistake when first automating spreadsheet creation (realism: 9/10). Minor deduction: the test could be slightly more challenging by requiring the agent to also mention the recalc.py step, which is emphasized in the skill as mandatory when using formulas, though the core learning objective is well-tested without it."
        },
        {
          "model_name": "claude-sonnet-4-5-20250929",
          "submission": "Looking at this skill document, let me analyze what it teaches:\n\n## Analysis\n\n**Core Domain Knowledge:**\n- The fundamental principle that spreadsheets should contain formulas, not pre-calculated values from Python\n- Financial modeling conventions (color coding, number formatting, assumption separation)\n- Formula construction best practices and error prevention\n- The distinction between static data dumps vs. dynamic, recalculable models\n\n**Key Workflow Decision:**\nThe critical insight is recognizing when to use Excel formulas (dynamic, recalculable) versus hardcoded values (static, breaks when data changes). This is the \"when to use X vs Y\" decision that practitioners genuinely struggle with.\n\n**What practitioners struggle with:**\nWithout this knowledge, developers calculate values in pandas/Python and hardcode results into Excel, creating static files that break when inputs change. They don't recognize that spreadsheets should be living calculation engines.\n\n## Test Case\n\n```yaml\nname: dynamic-spreadsheet-with-formulas\nskills:\n  - xlsx\nprompt: |\n  Create an Excel spreadsheet that contains quarterly sales data and calculates key metrics. \n  \n  The spreadsheet should have:\n  - Q1 Sales: $50,000\n  - Q2 Sales: $62,000\n  - Q3 Sales: $58,000\n  - Q4 Sales: $71,000\n  \n  Calculate and include:\n  - Total Annual Sales\n  - Average Quarterly Sales\n  - Q4 Growth Rate (percentage change from Q1 to Q4)\n  \n  The spreadsheet should be dynamic - if someone changes a quarterly sales figure, all metrics should automatically update.\n  \n  Save it as 'sales_analysis.xlsx' and explain your approach.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of creating dynamic spreadsheets with formulas.\n    \n    Score 1.0 if:\n    - The solution uses Excel formulas (=SUM, =AVERAGE, growth rate formula) for all calculated metrics\n    - The response explicitly mentions or demonstrates using formulas rather than hardcoding calculated values\n    - The approach would create a spreadsheet that automatically recalculates when inputs change\n    - Code uses openpyxl or similar library and assigns formula strings to cells (e.g., sheet['B6'] = '=SUM(B2:B5)')\n    \n    Score 0.5 if:\n    - The solution uses some formulas but hardcodes other calculations\n    - OR the response shows awareness that formulas are better but doesn't fully implement them\n    - OR the code calculates values in Python but there's some indication they understand the limitation\n    \n    Score 0.0 if:\n    - The solution calculates all metrics in Python/pandas and hardcodes the results into cells\n    - The response shows no awareness that spreadsheets should use formulas for dynamic calculations\n    - The approach would create a static spreadsheet that doesn't recalculate when inputs change\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about dynamic vs. static spreadsheets - a genuine practitioner concern that transfers across contexts (non-obviousness: 9/10). The grader is robust, accepting any approach using formulas while rejecting hardcoded values, with clear scoring criteria (clarity: 9/10). The task is highly realistic - creating a sales analysis spreadsheet with automatic recalculation is a common real-world need (realism: 9/10). Minor deduction: the test could be slightly more challenging by requiring formula error checking or the recalc.py workflow, but it still effectively tests the core skill principle."
        },
        {
          "model_name": "claude-haiku-4-5-20251001",
          "submission": "Looking at this skill document, let me analyze what it's really teaching:\n\n## Analysis\n\n**Domain Knowledge:** The skill teaches when and why to use Excel formulas vs hardcoded values, how to structure financial models with separated assumptions, Excel-specific formatting conventions, and error prevention strategies.\n\n**Workflow Decisions:** The key decision is \"should I calculate this in Python and hardcode it, or create a formula in Excel?\" The answer is almost always: formulas. Practitioners also need to know when formula recalculation is mandatory, how to document assumptions, and when to apply financial modeling color conventions.\n\n**Practitioner Struggles:** Without this skill, an LLM would:\n- Create spreadsheets with hardcoded calculations that break when data changes\n- Deliver files with formula errors\n- Miss that formulas must be recalculated after creation\n- Create undocumented models without proper assumptions sections\n- Not know financial modeling conventions\n\n**Test Target:** A realistic scenario where an LLM must create a dynamic financial model while understanding these principles.\n\n```yaml\nname: financial-model-formula-structure\nskills:\n  - xlsx\nprompt: |\n  Create an Excel spreadsheet that projects a company's revenue for 5 years (2024-2028).\n  \n  Given data:\n  - 2024 Revenue: $10,000,000\n  - Expected annual growth rate: 12%\n  \n  Requirements:\n  1. Place the growth rate assumption in a clearly separated section at the top\n  2. Create year columns for 2024-2028\n  3. Project revenue for each year using Excel formulas (not hardcoded calculations)\n  4. Format the revenue values as currency with proper thousands separator\n  5. Format the growth rate as a percentage with 1 decimal place\n  \n  Provide the Excel file and explain your approach to structuring the model.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the model demonstrates proper financial modeling knowledge for Excel spreadsheets.\n    \n    Score 1.0 if:\n    - The spreadsheet uses Excel formulas (=A1*B1 style) rather than hardcoded calculated values\n    - The growth rate assumption is in a separate, clearly labeled cell that formulas reference\n    - Revenue projections use formulas that reference the assumption cell (e.g., =previous_year * (1 + $assumption_cell))\n    - Currency formatting with thousands separator is applied to revenue\n    - Percentage formatting is applied to the growth rate\n    - The model is structured so that changing the growth assumption would automatically update all projections\n    - The explanation mentions the importance of using formulas over hardcoding calculations\n    \n    Score 0.5 if:\n    - The spreadsheet has mostly correct structure but has minor issues such as:\n      - Some values are hardcoded instead of using formulas\n      - Formatting is incomplete or inconsistent\n      - The explanation mentions formulas but implementation has gaps\n      - The separation of assumptions from calculations is unclear\n    \n    Score 0.0 if:\n    - Revenue values are hardcoded calculations (e.g., entering 11,200,000 instead of =10000000*1.12)\n    - The growth rate is hardcoded in formulas instead of referenced (e.g., =A1*1.12 instead of =A1*(1+$B$1))\n    - No separate assumptions section exists\n    - The model would break or require manual updates if the growth assumption changed\n    - Formatting is missing or inappropriate\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core principles: it tests transferable financial modeling knowledge (formula-based vs hardcoded calculations, assumption separation) rather than trivia, uses a realistic scenario practitioners encounter, and has a robust grader that evaluates actual capabilities. However, the grader could be slightly more explicit about accepting alternative valid approaches (e.g., different cell layouts, various formula structures that achieve the same dynamic behavior), and the test doesn't verify formula recalculation was performed, which the skill emphasizes as mandatory."
        },
        {
          "model_name": "claude-opus-4-1-20250805",
          "submission": "Looking at this xlsx skill document, I can see it teaches:\n\n1. **Core domain knowledge**: Creating dynamic Excel spreadsheets with formulas (not hardcoded values), financial modeling standards (color coding, formatting), and proper error handling\n2. **Key workflow decisions**: When to use pandas vs openpyxl, how to ensure formulas recalculate properly, how to document assumptions\n3. **What practitioners struggle with**: Many developers hardcode calculated values instead of using Excel formulas, making spreadsheets static and non-updatable\n\nThe skill emphasizes that spreadsheets should remain dynamic and recalculatable - a critical distinction for real-world use.\n\nHere's a test case that measures whether an LLM has absorbed this knowledge:\n\n```yaml\nname: dynamic-financial-forecast-model\nskills:\n  - xlsx\nprompt: |\n  Create an Excel file called \"revenue_forecast.xlsx\" with a simple revenue forecast model for the next 3 years (2024-2026).\n  \n  Requirements:\n  - Start with 2023 base revenue of $10 million\n  - Create assumption cells for annual growth rate (use 15%) and EBITDA margin (use 25%)\n  - Calculate projected revenue for each year using the growth rate assumption\n  - Calculate EBITDA for each year using the margin assumption\n  - Add a total row that sums revenue and EBITDA across all years\n  \n  The model should be dynamic - if someone changes the growth rate or margin assumptions, all calculations should automatically update.\n\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the solution creates a truly dynamic Excel model with formulas, not hardcoded calculated values.\n    \n    Score 1.0 if ALL of these are true:\n    - Uses Excel formulas for calculations (e.g., =B2*(1+$B$5) for growth, =B3*$B$6 for EBITDA)\n    - Growth rate and EBITDA margin are in separate assumption cells that formulas reference\n    - Total row uses SUM formulas, not hardcoded sums\n    - The model would recalculate if assumptions change (formulas, not static values)\n    \n    Score 0.5 if:\n    - Some calculations use formulas but others are hardcoded\n    - OR assumptions exist but aren't properly referenced in all formulas\n    - OR the approach is partially dynamic but has some static elements\n    \n    Score 0.0 if:\n    - Values are calculated in Python and hardcoded into cells (e.g., sheet['B3'] = 11.5)\n    - No use of Excel formulas for calculations\n    - OR the model would not update when assumptions change\n\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable domain knowledge about dynamic vs. static spreadsheets - a genuine practitioner concern that applies broadly beyond this specific skill. The grader is robust, testing actual capability (formula usage) rather than implementation details, and accepts multiple valid approaches. The task is realistic (financial forecasting is common). However, it loses some points on non-obviousness because the prompt explicitly tells the model 'The model should be dynamic' and mentions 'automatically update,' which somewhat telegraphs the answer rather than testing whether the model independently knows to use formulas. A stronger test would be less explicit about the dynamic requirement and see if the model applies this knowledge unprompted."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain-skill analysis (what this skill teaches and what to test)\n\n1. **Domain knowledge:** How to correctly build/modify Excel workbooks programmatically while keeping them “Excel-native”: calculations must be expressed as Excel formulas (not precomputed in Python), formulas must avoid common Excel error states, and financial models should follow common formatting conventions (inputs vs formulas coloring, percent/currency formats, zeros as “-”, negatives in parentheses).\n2. **Workflow decisions enabled:** When to use pandas (data extraction/analysis) vs openpyxl (formulas/formatting/template edits); how to safely load/edit existing templates without destroying formulas; and the necessity of a post-process formula recalculation/validation step (since openpyxl doesn’t evaluate formulas).\n3. **What practitioners struggle with without this skill:** They often (a) hardcode computed numbers instead of writing formulas, (b) break templates/formatting, (c) introduce #DIV/0!/bad references and fail to validate, and (d) mistakenly save a workbook loaded with `data_only=True`, wiping formulas.\n\n```yaml\nname: extend-financial-model-template-with-formulas-and-error-proofing\nskills:\n- xlsx\nprompt: |\n  You are asked to update an existing Excel financial model template (already formatted) without breaking its conventions.\n\n  Workbook structure (already exists):\n  - Sheet \"Inputs\"\n    - B2 = Base year Revenue (number)\n    - B3 = Base year COGS as % of Revenue (percentage)\n    - B4 = Base year Opex as % of Revenue (percentage)\n    - B6 = Revenue Growth Assumption for next year (percentage input that users will change)\n  - Sheet \"P&L\"\n    - Column B is the base year, Column C should be added/filled as the next year.\n    - Row 6 is Revenue\n    - Row 7 is COGS (as a negative number)\n    - Row 8 is Gross Profit\n    - Row 9 is Opex (as a negative number)\n    - Row 10 is EBITDA\n    - Row 11 is EBITDA Margin\n\n  Task:\n  1) Add the next-year column (Column C) calculations on \"P&L\" using Excel formulas only:\n     - Next-year Revenue = prior-year Revenue * (1 + growth assumption from Inputs!B6)\n     - Next-year COGS = - (Next-year Revenue * Inputs!B3)\n     - Next-year Opex = - (Next-year Revenue * Inputs!B4)\n     - Gross Profit, EBITDA, and EBITDA Margin should be formulas consistent with the row definitions.\n  2) Prevent Excel errors:\n     - EBITDA Margin must not show #DIV/0! when Revenue is 0; use an error-safe formula.\n  3) Preserve the template:\n     - Do not “reformat everything”; only add what’s necessary.\n     - Respect common financial model conventions: user inputs in blue, formulas in black, links across sheets in green (you can implement this specifically for the linked cells).\n  4) Apply appropriate number formats for the new cells:\n     - Currency cells with thousands separators, negatives in parentheses, and zeros displayed as \"-\" (e.g., an equivalent of $#,##0;($#,##0);-)\n     - Percentages as 0.0% and also show zero as \"-\" if you choose a custom format.\n  5) Provide concise Python (openpyxl) code that performs the update on \"model.xlsx\" and saves \"model_updated.xlsx\".\n  6) Explain how you would ensure formulas are recalculated and validated for error codes after saving (openpyxl won’t compute them).\n\n  Do NOT compute any results in Python and paste values; the sheet must stay dynamic if Inputs!B6 changes.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the assistant demonstrates practical spreadsheet-editing expertise appropriate for programmatically updating an Excel financial model template.\n\n    Score 1.0 if the answer:\n    - Provides an openpyxl-based approach that edits an existing workbook and writes Excel formulas (strings beginning with '=') for ALL required next-year cells (Revenue, COGS, Gross Profit, Opex, EBITDA, EBITDA Margin).\n    - References assumptions from dedicated assumption cells (e.g., Inputs!B6, Inputs!B3, Inputs!B4) rather than hardcoding rates inside formulas.\n    - Includes explicit error prevention for EBITDA Margin that avoids #DIV/0! when revenue is zero (e.g., IF, IFERROR, or equivalent safe-guard).\n    - Addresses template preservation (minimal changes) and mentions the danger of data_only=True overwriting formulas or otherwise shows correct loading/saving practice.\n    - Applies appropriate number formats for currency and percent and mentions zeros displayed as \"-\" and negatives in parentheses (exact format string may vary but must meet intent).\n    - Addresses financial model color conventions (inputs blue, formulas black, cross-sheet links green) in a reasonable way (e.g., setting Font color on the relevant cells) without repainting the entire workbook.\n    - Mentions a post-save formula recalculation + error scan step using an Excel/LibreOffice recalculation workflow (script/automation name not required) and indicates checking for Excel error codes (#REF!, #DIV/0!, #VALUE!, #NAME?, etc.).\n\n    Score 0.5 if the answer:\n    - Uses formulas and references assumptions, but is missing ONE OR TWO of: robust div/0 protection, number formats/zero-as-dash, clear recalculation+validation step, or reasonable handling of template preservation/colors.\n    - Or provides correct conceptual steps but the code is incomplete in minor ways (e.g., some cell addresses not explicit) while still clearly implementing formula-based, assumption-driven modeling.\n\n    Score 0.0 if the answer:\n    - Computes values in Python and writes numbers instead of formulas, or\n    - Fails to reference assumptions and hardcodes rates into formulas, or\n    - Omits any discussion of recalculation/validation and ignores formula error prevention, or\n    - Proposes destructive operations that would likely break the template (e.g., re-creating sheets, blanket reformatting) without justification.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts: it tests formula-based (not hardcoded) calculations, assumption referencing, error prevention (#DIV/0!), template preservation, financial formatting conventions, and the recalculation/validation workflow. These are genuine practitioner concerns that transfer across contexts. The grader is robust and accepts multiple valid implementations while checking for key capabilities. However, the test is somewhat lengthy and prescriptive in its setup (specifying exact cell locations and sheet structure), which adds minor artificiality. The task itself is realistic—extending a financial model template is common work—though the detailed pre-specification of the exact workbook structure makes it slightly more contrived than a real-world scenario where the model structure would be discovered. Overall, this is a high-quality test that effectively measures transferable spreadsheet expertise rather than trivia."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "Domain knowledge this skill teaches:\n- Building spreadsheets as *models*: keep calculations as Excel formulas (not precomputed in Python) so the file stays dynamic.\n- Financial-model hygiene: separate assumptions, reference them consistently (absolute refs), and format outputs (currency/percent, negatives in parentheses, zeros shown as “-”).\n- Professional conventions: color-coding (inputs vs formulas vs links) and highlighting key assumptions.\n- Operational workflow: openpyxl writes formulas but doesn’t evaluate them—requires a recalculation step (e.g., LibreOffice/Excel automation) and an error scan to ensure zero #REF!/#DIV/0! etc.\n\nWorkflow decisions it enables:\n- When to use openpyxl (formulas/formatting/templates) vs pandas (analysis/bulk data).\n- How to structure a workbook (assumptions sheet → model sheet links) and prevent common formula failures (division-by-zero).\n- Knowing to run a formula recalculation/validation pass after writing and to iterate until no errors remain.\n\nWhat practitioners struggle with without this skill (test target):\n- They often hardcode computed results, breaking scenario analysis.\n- They forget to recalc/validate formulas, shipping workbooks with stale values or hidden Excel errors.\n- They mix assumptions into formulas, making models brittle and un-auditable.\n- They miss finance-standard formatting/color conventions and zero/negative formatting requirements.\n\n```yaml\nname: build-financial-projection-xlsx-with-formulas-formatting-and-recalc\nskills:\n- xlsx\nprompt: |\n  You’re automating a simple CFO-ready 5-year P&L projection workbook.\n\n  Write a minimal Python solution (use openpyxl) that creates an Excel file `pnl_model.xlsx` with TWO sheets: `Assumptions` and `P&L`, with the exact layout below. Use Excel formulas for ALL derived values (do not calculate projection numbers in Python), and ensure the model will not produce any Excel formula errors (especially avoid #DIV/0!).\n\n  Sheet: Assumptions\n  - A1: \"Assumptions\" (bold)\n  - A2: \"Base Revenue (USD mm)\"   B2: 120\n  - A3: \"Annual Growth Rate\"     B3: 8%\n  - A4: \"Gross Margin\"           B4: 35%\n  Formatting requirements:\n  - Inputs (B2:B4): blue font (user inputs) AND yellow fill (key assumptions)\n  - Percent inputs should display as 0.0%\n  \n  Sheet: P&L\n  - Row 1 headers (bold):\n    A1 \"Year\", B1 \"Revenue ($mm)\", C1 \"COGS ($mm)\", D1 \"Gross Profit ($mm)\", E1 \"Gross Margin\"\n  - Years in A2:A6: 2024–2028 as TEXT (not numbers)\n  - Formulas:\n    - Revenue:\n      B2 links to base revenue: =Assumptions!$B$2  (this cross-sheet link should be green font)\n      B3 = B2*(1+Assumptions!$B$3) and copy down through 2028\n    - COGS: =Revenue*(1-GrossMargin) using the assumption cell (copy down)\n    - Gross Profit: =Revenue-COGS (copy down)\n    - Gross Margin: must never throw #DIV/0! even if revenue is 0 (copy down)\n  Formatting requirements:\n  - Formula cells should be black font (except the pure cross-sheet link cell B2 which is green)\n  - Currency columns (Revenue/COGS/Gross Profit) use a format that:\n    - shows negatives in parentheses\n    - shows zeros as \"-\" (dash)\n  - Gross Margin column uses 0.0% and also shows zeros as \"-\" (dash)\n  - Set reasonable column widths so headers fit.\n\n  After the code, explain (briefly) how you would recalculate formulas after saving (since openpyxl doesn’t compute them) and how you would verify the workbook has zero Excel errors before sending it.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates professional spreadsheet-model creation per the skill.\n\n    Score 1.0 if the response includes ALL of the following:\n    - Uses openpyxl (or an equivalent Excel-writing library) to create an .xlsx with the specified two-sheet layout and cell addresses.\n    - Writes projection logic as Excel formulas (strings beginning with \"=\") referencing the Assumptions cells; does NOT compute the projected revenue/COGS/profit numbers in Python.\n    - Separates assumptions in dedicated cells (B2:B4) and uses absolute references (e.g., $B$3/$B$4) or otherwise clearly stable references in formulas.\n    - Prevents division-by-zero in Gross Margin (e.g., IF/IFERROR pattern that yields a numeric 0 when revenue is 0, not an Excel error and not #N/A).\n    - Applies finance-style number formats:\n      - Currency with negatives in parentheses and zeros shown as \"-\" (dash) (any equivalent custom format is acceptable).\n      - Percent as 0.0% with zeros shown as \"-\" (dash) (any equivalent custom format is acceptable).\n      - Years stored as text (strings and/or \"@\" number format).\n    - Applies the color conventions:\n      - Blue font + yellow fill for assumption input cells B2:B4.\n      - Green font for the pure cross-sheet link cell (Revenue 2024 =Assumptions!$B$2).\n      - Black font for other formula cells.\n    - Mentions a post-save formula recalculation step (e.g., running a LibreOffice/Excel-based recalculation utility) and explicitly mentions checking/ensuring zero Excel errors (#REF!, #DIV/0!, #VALUE!, etc.) and iterating if errors are found.\n\n    Score 0.5 if the response gets the core modeling workflow right (formulas not hardcoded + assumptions separated + mentions recalc/validation) but is missing one or two of: correct zero/negative formatting, the color conventions, explicit division-by-zero handling, or years-as-text.\n\n    Score 0.0 if it primarily hardcodes computed results instead of Excel formulas, omits recalculation/validation entirely, or produces/permits Excel error conditions (e.g., naive Gross Margin =D2/B2 with no protection).\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case effectively measures transferable spreadsheet modeling knowledge (formulas vs hardcoding, assumption separation, error prevention, recalculation workflow) rather than skill-specific trivia. The grader is mostly robust, accepting valid approaches while checking for core competencies. However, it has minor brittleness around exact color specifications (RGB values) and specific formatting strings that could reject valid alternatives. The task is highly realistic—building a financial projection model is a common real-world need. The test loses points primarily on clarity due to prescriptive formatting requirements that may reject functionally equivalent solutions (e.g., different but valid custom number formats, slightly different color codes)."
        },
        {
          "model_name": "gemini-3-flash-preview",
          "submission": "## Analysis: xlsx Skill\n\n**1. What domain knowledge does this skill teach?**\nThe skill focuses on **Spreadsheet Engineering** and **Financial Modeling Standards**. It moves beyond basic data entry into the realm of professional \"Investment Banking\" style Excel development. This includes the distinction between dynamic models (formulas) and static data, industry-standard color-coding (Blue/Black/Green/Red/Yellow), and rigorous formatting for professional readability (e.g., accounting for zeros, currency, and multiples).\n\n**2. What workflow decisions does it enable?**\n- **Library Selection:** Choosing `openpyxl` when formatting and formulas are required, versus `pandas` for pure data manipulation.\n- **Dynamic Modeling:** Deciding to reference an \"Assumption Cell\" (e.g., Growth Rate) rather than hardcoding a percentage inside a formula.\n- **Verification Loop:** Incorporating a recalculation step (`recalc.py`) to validate formula integrity before delivery.\n- **Documentation:** Deciding when and how to source-stamp hardcoded inputs to ensure auditability.\n\n**3. What would a practitioner actually struggle with without this skill?**\nA practitioner (or an LLM without this skill) would likely produce \"dead\" spreadsheets where results are pre-calculated in Python and pasted as static values. They would also likely use default formatting that is difficult for finance professionals to read (e.g., negative signs instead of parentheses, inconsistent decimal places) and fail to distinguish between user-editable inputs and calculated outputs, making the model dangerous to use in a professional setting.\n\n---\n\n```yaml\nname: professional-financial-projection-modeling\nskills:\n- xlsx\nprompt: |\n  Create a professional Excel financial model for \"Project Alpha\" based on the following data:\n  - 2024 Actual Revenue: $5,000,000\n  - 2024 Actual Expenses: $3,500,000\n  - Data Source: \"Annual Internal Audit, Jan 2025\"\n\n  Task:\n  1. Create a 3-year projection (2025, 2026, 2027).\n  2. Use a Revenue Growth assumption of 15% and an Expense Margin assumption of 70% of Revenue.\n  3. Requirements:\n     - All assumptions (Growth, Margin) must be in separate cells and referenced by formulas.\n     - Calculations MUST be dynamic Excel formulas (not hardcoded values calculated in Python).\n     - Apply industry-standard color coding: Blue for hardcoded inputs, Black for formulas.\n     - Use professional formatting: $#,##0 for currency, years as text, and (negative numbers) in parentheses.\n     - Include the source documentation for the 2024 actuals.\n  \n  Save the file as 'project_alpha_model.xlsx' and ensure you verify the formulas.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate the model based on professional Excel and financial modeling standards:\n    \n    1. **Dynamic Formulas**: Are the 2025-2027 projections calculated via Excel formulas (e.g., =B2*(1+Assump!B1)) rather than hardcoded numbers?\n    2. **Assumption Placement**: Are the 15% growth and 70% margin rates placed in their own cells and referenced, or are they buried inside formulas?\n    3. **Color Coding**: Are the 2024 inputs (5M, 3.5M) colored Blue (0,0,255) and the calculated projections colored Black?\n    4. **Formatting**: Are negative numbers in parentheses? Are years formatted as text strings? Is the $#,##0 format used?\n    5. **Documentation**: Is the source \"Annual Internal Audit, Jan 2025\" documented near the hardcoded inputs?\n    6. **Recalculation**: Did the model use a tool or script to verify there are no #REF! or #DIV/0! errors?\n\n    Score 1.0 if: The model uses dynamic formulas, separates assumptions, applies correct color-coding/formatting, and includes data sourcing.\n    Score 0.5 if: The model uses formulas but fails on color-coding/formatting, OR if it has correct formatting but hardcoded the projected values instead of using formulas.\n    Score 0.0 if: The model provides a static spreadsheet with hardcoded values and no professional formatting or assumption separation.\n  extractor: last_assistant\n```",
          "score": 0.85,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts. It effectively tests transferable financial modeling knowledge (dynamic formulas, assumption separation, professional formatting) rather than trivia. The grader is mostly robust, accepting solutions that demonstrate the key principles. However, there's minor brittleness in requiring exact RGB color codes (0,0,255 for blue) when the skill document doesn't specify exact values, and the grader could be more explicit about accepting alternative valid approaches (e.g., different cell layouts for assumptions). The task is highly realistic - creating a financial projection model is a common real-world need. The test successfully distinguishes between models that understand professional Excel practices versus those that just dump calculated values."
        },
        {
          "model_name": "gpt-5.2-2025-12-11",
          "submission": "```yaml\n# Analysis:\n# Domain knowledge: Building and editing Excel workbooks with dynamic, reference-based formulas; financial-model\n# formatting conventions (inputs vs formulas vs links); preventing common Excel formula errors; and understanding\n# that openpyxl writes formulas as strings without evaluated values.\n# Workflow decisions enabled: When to use openpyxl (formulas/formatting) vs pandas (analysis); when/why to\n# recalculate formulas using an external engine (e.g., LibreOffice/Excel) and then scan for errors; structuring\n# assumptions separately and linking instead of hardcoding.\n# Practitioner struggle without this skill: They often compute results in Python and paste values (breaking\n# spreadsheet dynamism), mix assumptions into formulas, forget to prevent #DIV/0!, apply inconsistent formatting,\n# or fail to recalculate/validate formulas after programmatic edits.\n\nname: dynamic-financial-forecast-xlsx-with-assumptions-and-error-proofing\nskills:\n- xlsx\nprompt: |\n  You are asked to deliver a small, professional, *dynamic* Excel forecast model (not a static report).\n\n  Write concise Python code (using openpyxl) that creates an .xlsx file with:\n  - Sheet \"Assumptions\":\n    - Base year label as text \"2024\" in A2\n    - Base Revenue input in B2 = 1000 (representing $mm)\n    - Revenue Growth input in B3 = 5.0% (user-editable)\n    - COGS % of Revenue input in B4 = 60.0% (user-editable)\n    - Mark user-editable inputs in BLUE text; highlight key assumptions (growth and COGS%) with YELLOW fill.\n  - Sheet \"Model\":\n    - A header row with years \"2024\", \"2025\", \"2026\", \"2027\" formatted as text\n    - Rows for Revenue, COGS, Gross Profit, and Gross Margin\n    - Revenue for 2025-2027 must be Excel formulas referencing the assumptions (no Python-calculated values)\n    - COGS must be formula based on Revenue and the COGS% assumption\n    - Gross Profit = Revenue - COGS (formula)\n    - Gross Margin = Gross Profit / Revenue with protection to avoid #DIV/0! when Revenue is 0\n    - Use BLACK text for formulas; use GREEN text for internal links that pull from \"Assumptions\"\n  - Apply number formats:\n    - Currency rows: $#,##0;($#,##0);-   (so zeros display as \"-\")\n    - Percent rows: 0.0%;(0.0%);-        (one decimal, zeros as \"-\")\n  - Save as \"forecast.xlsx\".\n\n  Finally, explain (briefly) how you would ensure formula cells have no Excel errors after creating the file,\n  given that openpyxl does not evaluate formulas.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the answer demonstrates strong spreadsheet (.xlsx) creation skill with dynamic formulas,\n    financial-model conventions, and formula validation awareness.\n\n    Score 1.0 if ALL of the following are satisfied:\n    - Provides openpyxl-based code that creates a workbook with TWO sheets named \"Assumptions\" and \"Model\".\n    - Uses Excel formulas (strings beginning with \"=\") for projections and calculations (Revenue forecast, COGS,\n      Gross Profit, Gross Margin) rather than computing those values in Python and hardcoding results.\n    - Places assumptions in dedicated assumption cells and references them from the model (cross-sheet references\n      like Assumptions!B3), showing clear separation of inputs vs calculations.\n    - Includes explicit protection against division-by-zero in Gross Margin (e.g., IF/IFERROR pattern) to avoid\n      #DIV/0!.\n    - Applies at least the key formatting conventions requested:\n      * BLUE font for user-editable inputs in Assumptions (and/or clearly identified input cells)\n      * BLACK font for formula cells in Model\n      * GREEN font for internal links pulling from Assumptions\n      * YELLOW fill for key assumptions (growth and COGS%)\n    - Applies the specified number formats for currency and percent rows, including a custom format that shows\n      zeros as \"-\".\n    - Explains a correct approach for post-generation formula validation: recognizes formulas aren't evaluated by\n      openpyxl and proposes recalculation via an external engine (e.g., LibreOffice/Excel) and scanning for error\n      codes (#REF!, #DIV/0!, #VALUE!, #NAME?, #N/A).\n\n    Score 0.5 if:\n    - Uses formulas and separates assumptions, but misses one major requirement (e.g., no divide-by-zero guard,\n      or no mention of external recalculation/verification, or minimal/incorrect formatting/number formats).\n\n    Score 0.0 if:\n    - Primarily hardcodes calculated outputs (does math in Python and writes numbers), OR\n    - Does not create a dynamic model with cross-sheet references, OR\n    - Fails to address formula error prevention/validation in any credible way.\n\n    Be flexible on exact cell locations and styling APIs, but require the conceptual elements above.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong domain knowledge testing (dynamic formulas vs hardcoded values, financial formatting conventions, formula error prevention, cross-sheet references) and is realistic. However, the grader has moderate brittleness issues: it requires very specific implementation details (exact sheet names 'Assumptions' and 'Model', specific color coding for different cell types, exact number format strings) that could reject valid alternative approaches. A practitioner might use different sheet names, alternative error-handling patterns (IFERROR vs IF), or equivalent formatting methods while still demonstrating the core skill. The non-obviousness is high (8-9) as it tests transferable spreadsheet modeling principles, clarity is moderate (6-7) due to prescriptive grading criteria, and realism is high (8-9) as building dynamic financial models is a common real-world task."
        },
        {
          "model_name": "gpt-5.1-codex-mini",
          "submission": "**Analysis**\n\n1. **Domain Knowledge**: The skill teaches how to work with Excel files using a combination of tools (pandas for data analysis, openpyxl for formulas and formatting), emphasizing dynamic models. Core concepts include: placing assumptions in dedicated cells, referencing those assumptions in formulas rather than hardcoding, applying industry-standard formatting (color-coding inputs vs formulas, currency/percent formats, zero display), and ensuring all formulas recalculate without errors (using LibreOffice’s `recalc.py`).\n\n2. **Workflow Decisions Enabled**: The skill helps decide when to use pandas (for reading and initial data analysis) versus openpyxl (for structured writing, formatting, adding formulas). It also guides when to trigger recalculation (any time formulas are added/changed) and how to approach edits versus new creations (preserve template styles). The emphasis on formulas shows when to build references instead of doing offline calculations, and how to structure assumption inputs to keep models flexible.\n\n3. **Practitioner Struggles Without the Skill**: Without this knowledge, an LLM might hardcode computed values, miss required formatting/color conventions, forget to recalc formulas (leading to stale values), or fail to structure assumptions so future updates are easy. They could also produce models with formula errors (#REF!, #DIV/0!) or inconsistent number formats, making the spreadsheet unreliable for stakeholders.\n\nNow, the test case evaluates these elements.\n\n```yaml\nname: dynamic-product-revenue-projection\nskills:\n  - xlsx\nprompt: |\n  You are building the “Product Revenue Projection” workbook for a product team. They provide a historical table of monthly performance and want you to deliver an Excel workbook that:\n  1. Includes an assumption block with one row for “Price Growth (%)” and one row for “Volume Growth (%)”. These assumption cells must be formatted with blue text (RGB 0,0,255) to signal hardcoded inputs.\n  2. Uses those assumptions via Excel formulas (no hardcoded growth factors) to project revenue for the next four months, based on the last historical month’s units and price. Revenue = Units × Price, with both units and price growing according to their respective assumption cells each period.\n  3. Adds summary metrics (current trailing four-month average price, total historical revenue, and projected revenue) calculated entirely with formulas referencing the appropriate tables/ranges.\n  4. Applies currency formatting ($#,##0) to all revenue figures, uses 0.0% for growth rates, and displays zero values as “-”.\n  5. Documents any hardcoded assumptions with source notes adjacent to the assumption block (e.g., “Source: Internal target, Feb 2026”).\n  6. Describes how you would recalculates formulas before delivering the file (mention recalc.py and LibreOffice requirement).\n\n  Provide the Python steps (using pandas/openpyxl as needed) or detailed instructions that would generate this workbook, explicitly showing how formulas reference the assumption cells and how the formatting/notes are applied.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluation criteria for the response:\n    \n    Score 1.0 if: The answer outlines creating/editing a workbook that builds a projection table referencing a dedicated assumption block (blue text) for price/volume growth, uses Excel formulas (not hardcoded values) to calculate projected revenue and summary metrics, applies the required currency/percentage/zero formatting, includes a note documenting the assumptions’ source, and explicitly mentions running recalc.py (LibreOffice) to refresh formulas.\n    \n    Score 0.5 if: The answer partially satisfies the requirements (e.g., mentions assumptions and formulas but omits formatting details or recalc), or still mentions formulas but lacks clear reference to the assumption cells or documentation note.\n    \n    Score 0.0 if: The answer relies on hardcoded calculations, fails to reference the assumptions in formulas, ignores formatting/documentation/recalc requirements, or cannot be followed to produce the requested workbook.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of core xlsx skill concepts (formulas vs hardcoding, assumption cells, formatting standards, recalc.py workflow) and creates a realistic financial modeling scenario. However, the grader has moderate brittleness: it requires mentioning specific implementation details like 'recalc.py' and 'LibreOffice' rather than testing whether the model understands the underlying concept of formula recalculation. A model could correctly explain the workflow using different but valid terminology and receive partial credit. The non-obviousness is good (tests transferable spreadsheet modeling patterns), and realism is high (common business task), but clarity suffers from the grader's specificity requirements."
        },
        {
          "model_name": "gemini-3-pro-preview",
          "submission": "```yaml\nname: create-dynamic-financial-model\nskills:\n  - xlsx\nprompt: |\n  Create an Excel financial model file named 'saas_projections.xlsx'.\n  \n  Setup the following Assumptions (Inputs) at the top of the sheet:\n  - Starting ARR: $1,000,000\n  - Annual Growth Rate: 20%\n  \n  Create a projection table for Years 1 through 5. Calculate the ARR for each year based on the growth rate assumption.\n  \n  Crucial Requirements:\n  1. The Excel file must be dynamic. If I change the Growth Rate cell in Excel, the projections must update automatically.\n  2. Follow standard financial modeling color conventions: Blue text for hardcoded inputs/assumptions, and Black text for formulas/calculations.\n  3. Ensure the file is saved and values are recalculated so I can read it immediately.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Analyze the Python code generated by the assistant.\n\n    Score 1.0 if the code demonstrates ALL of the following:\n    1. **Dynamic Formulas**: Uses Excel formulas (e.g., `=B5*(1+$B$2)`) for the projections. It must NOT calculate the values in Python and write them as static numbers.\n    2. **Assumptions Separation**: Places the inputs (Start ARR, Growth Rate) in distinct cells referenced by the formulas.\n    3. **Color Coding**: Applies Blue font color (RGB 0,0,255 or hex 'FF0000FF'/'0000FF') to the assumption cells and Black (or default) to the calculated cells.\n    4. **Recalculation**: Includes a step to run the `recalc.py` script (e.g., `subprocess.run`, `os.system`, or backticks) to calculate the formula values, as required by the skill for formula-heavy sheets.\n\n    Score 0.5 if:\n    - The model uses valid Excel formulas and separates assumptions, but fails to apply the specific Blue/Black color coding or forgets the `recalc.py` step.\n\n    Score 0.0 if:\n    - The model performs the math in Python and writes static integers/floats to the projection cells (Hardcoding).\n    - The model fails to generate a valid Excel file code.\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case effectively measures understanding of dynamic Excel formulas vs. hardcoded values, which is a genuine transferable concept in spreadsheet modeling. The grader is mostly robust, checking for formulas, color coding, and recalculation. However, it has minor brittleness: it only accepts blue color for inputs when the skill document states 'Unless otherwise stated by the user or existing template,' and the prompt doesn't explicitly require color coding (though it says 'Crucial Requirements'). The task is realistic - creating a simple financial projection model is a common real-world need. The test loses points for being somewhat focused on skill-specific details (exact color codes, recalc.py script name) rather than purely testing the conceptual understanding of dynamic vs. static spreadsheets."
        },
        {
          "model_name": "z-ai/glm-4.6",
          "submission": "\nI'll analyze the skill document and create a high-quality test case that measures whether having this specialized Excel/financial modeling knowledge helps an LLM succeed.\n\n## Analysis of the xlsx Skill\n\n**Domain Knowledge:**\n- Financial modeling standards (color coding, formatting, number conventions)\n- Excel formula construction and error prevention\n- Professional spreadsheet workflows (pandas vs openpyxl selection)\n- Formula recalculation and verification processes\n\n**Workflow Decisions:**\n- When to use formulas vs hardcoded values (critical distinction)\n- How to handle financial modeling conventions (colors, formatting, documentation)\n- Tool selection: pandas for analysis vs openpyxl for formatting/formulas\n- Error checking and recalculation workflows\n\n**Practitioner Struggles Without This Skill:**\n- Creating spreadsheets with hardcoded values instead of dynamic formulas\n- Missing industry standards for financial modeling formatting\n- Formula errors that make models unusable (#REF!, #DIV/0!, etc.)\n- Understanding the difference between data analysis and spreadsheet construction\n\nNow I'll create a test case that specifically targets the core distinction between using formulas versus hardcoded values, which is the most critical workflow decision taught in this skill.\n\n```yaml\nname: excel-formulas-vs-hardcoded-test\nskills:\n- xlsx\nprompt: |\n  You need to create a simple 3-year financial projection model in Excel. I have historical revenue data for 2021-2023 and need to project 2024-2026 with a 15% growth rate.\n\nHistorical Data:\n- 2021 Revenue: $1,000,000\n- 2022 Revenue: $1,150,000  \n- 2023 Revenue: $1,322,500\n\nRequirements:\n1. Create an Excel file with the historical data and 3-year projection\n2. Use EXCEL FORMULAS for all calculations (not hardcoded values)\n3. The growth rate should be in an assumption cell that references are made to\n4. Project 2024-2026 revenue by growing from 2023 base\n5. Include a total revenue summary that uses Excel formulas\n\nGenerate the complete Python code to create this Excel file. Make sure all revenue calculations use Excel formulas so the model updates automatically if I change the growth rate assumption.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response correctly uses Excel formulas instead of hardcoded values for all calculations.\n\n    Score 1.0 if:\n    - The code creates an Excel file with proper structure (historical + projected years)\n    - Uses Excel formulas for all projected revenue calculations (e.g., '=D5*(1+$B$2)' not hardcoded values)\n    - Places the growth rate in a separate assumption cell\n    - Uses Excel formulas for totals/summaries (e.g., '=SUM()' not hardcoded totals)\n    - Demonstrates understanding of cell references (absolute for assumptions, relative for projections)\n\n    Score 0.5 if:\n    - Creates the Excel file correctly but mixes some hardcoded values with formulas\n    - Uses formulas for projections but hardcodes totals or growth rate\n    - Has the right structure but missing proper cell reference usage\n\n    Score 0.0 if:\n    - Uses hardcoded values instead of formulas for calculations\n    - Calculates values in Python and writes them as static numbers\n    - Doesn't demonstrate understanding of dynamic spreadsheet principles\n    - Missing the core concept of using Excel formulas over hardcoded values\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concept (formulas vs hardcoded values) and creates a realistic financial modeling scenario. However, it has moderate clarity issues: the grader accepts solutions that 'mix some hardcoded values with formulas' for 0.5 points, which is vague and could lead to inconsistent scoring. The test also doesn't verify that the model actually uses openpyxl (could use pandas incorrectly) or includes the mandatory recalc.py step mentioned in the skill. Non-obviousness is good (8/10) - tests transferable principle of dynamic spreadsheets. Clarity is moderate (6/10) - grading criteria could be more precise about what constitutes 'mixing' values. Realism is strong (9/10) - very common real-world task."
        },
        {
          "model_name": "minimax/minimax-m2",
          "submission": "Looking at this skill document, I can see it teaches comprehensive Excel spreadsheet knowledge focused on financial modeling. Let me analyze the core domain knowledge and create a test case.\n\n## Analysis of the Skill\n\n**Domain Knowledge:**\n- Excel formula construction and error prevention\n- Financial modeling standards (color coding, formatting)\n- Tool selection (pandas vs openpyxl)\n- Formula-first approach (using Excel calculations instead of hardcoding)\n- Verification and debugging techniques\n\n**Key Workflow Decisions:**\n- When to use formulas vs hardcoded values (always use formulas)\n- Library selection for different tasks (pandas for data analysis, openpyxl for formatting)\n- Error prevention strategies and verification methods\n\n**Test Target:**\nPractitioners often struggle with creating dynamic Excel models that use proper formulas instead of hardcoded values, and knowing how to verify formula correctness.\n\n```yaml\nname: financial-projection-formula-creation\nskills:\n  - xlsx\nprompt: |\n  Create a 5-year quarterly revenue projection model for TechCorp Inc. with the following requirements:\n\n  Starting Data:\n  - Q1 2024 Revenue: $2,500,000\n  - Annual growth rate: 15% (apply quarterly)\n  - Tax rate: 25%\n  \n  Create an Excel file with:\n  1. A quarterly projection table with proper formulas (not hardcoded values)\n  2. Calculate quarterly growth from the annual rate\n  3. Show quarterly revenue for 5 years (Q1 2024 - Q4 2028)\n  4. Calculate after-tax quarterly revenue\n  5. Use proper number formatting (currency, no hardcoded calculations)\n  6. Include a summary section showing total 5-year revenue\n  \n  IMPORTANT: Use Excel formulas for ALL calculations. Do NOT calculate values in Python and hardcode them. The spreadsheet must be dynamic and recalculatable.\n  \n  Save the file as \"techcorp_revenue_model.xlsx\" and verify it has zero formula errors.\ntimeout: 300\n\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the model demonstrates the xlsx skill by checking:\n    \n    Score 1.0 if:\n    - Creates an Excel file with proper formulas for ALL calculations (no hardcoded values)\n    - Quarterly growth is calculated using formulas, not hardcoded\n    - After-tax revenue uses formulas to calculate from pre-tax revenue\n    - Total 5-year revenue is calculated with Excel formulas\n    - File is saved as \"techcorp_revenue_model.xlsx\"\n    - Shows understanding of formula-first approach vs hardcoding\n    \n    Score 0.5 if:\n    - Uses some formulas but hardcodes some calculated values\n    - Missing 1-2 key formula components\n    - File created but with significant formula issues\n    \n    Score 0.0 if:\n    - Hardcodes all calculated values instead of using formulas\n    - No Excel file created\n    - File created but contains no meaningful calculations\n    \n    Key indicators of skill mastery:\n    - Dynamic, recalculatable formulas instead of static numbers\n    - Proper formula construction for financial calculations\n    - Understanding that Excel should do the math, not Python hardcoding\n  extractor: last_assistant\n```",
          "score": 0.75,
          "rationale": "The test case demonstrates strong understanding of the skill's core concepts (formula-first approach, dynamic calculations) and tests transferable domain knowledge rather than trivia. However, it has moderate clarity issues: the grader relies on subjective interpretation of 'proper formulas' and 'ALL calculations' without specifying which exact calculations must be formulas vs acceptable hardcodes (e.g., the starting Q1 2024 revenue of $2,500,000 is inherently a hardcoded input). The realism is good - financial projections are common tasks. The non-obviousness is strong as it tests the conceptual understanding of dynamic spreadsheets. The grader could be more robust by explicitly listing the 5-7 specific formula cells it expects (quarterly growth calculation, revenue projections, after-tax calculations, total) rather than using broad language that could reject valid approaches."
        },
        {
          "model_name": "gpt-5.1-codex",
          "submission": "**Analysis**\n\n1. **Domain Knowledge Taught**  \n   The skill document imparts best practices for building and editing Excel workbooks, especially financial models: enforcing zero formula errors, respecting existing templates, maintaining financial modeling color conventions, applying standardized number formats, separating assumptions from calculations, rigorously using cell references instead of hardcoding, documenting hardcoded sources, and recalculating formulas via LibreOffice (`recalc.py`). It also covers selecting appropriate Python libraries (pandas vs. openpyxl) and emphasizes workflows that keep spreadsheets dynamic and error-free.\n\n2. **Workflow Decisions Enabled**  \n   Practitioners learn when to use pandas for bulk data analysis vs. openpyxl for precise formula/format control, when to place assumptions in dedicated cells, how to apply industry color coding, when to run `recalc.py`, and how to decide between editing vs. preserving templates. They gain guidance on preventing formula errors, ensuring consistent formatting, and verifying outputs through structured checklists.\n\n3. **Likely Practitioner Struggles Without This Skill**  \n   Without these guidelines, an LLM might hardcode calculated values, forget to separate assumptions, ignore required formatting/color standards, miss running `recalc.py`, or overlook documenting sources—leading to brittle, noncompliant financial models with hidden errors. The test should therefore probe whether the model knows to prioritize formulas, assumption placement, color/format standards, documentation, and recalculation/error-check workflows.\n\n```yaml\nname: financial-model-update-plan\nskills:\n  - xlsx\nprompt: |\n  You are asked to update an existing multi-sheet financial model for a client. They want to add a “Downside” scenario that adjusts revenue growth rates and EBITDA margins over the next five years. You must describe, in detail, how you will modify the workbook so that it remains fully dynamic and compliant with professional modeling standards.\n\n  In your answer, explain:\n  - How you will introduce and reference the new scenario assumptions without breaking existing templates.\n  - The formula strategy you will use to ensure no hardcoded calculations, including how you’ll keep assumptions separate from calculations.\n  - The formatting and color-coding conventions you’ll apply for inputs, formulas, internal links, and any key assumption highlights.\n  - The number-formatting rules you’ll enforce for years, currency, percentages, multiples, zeros, and negatives.\n  - How you will document sources for any new hardcoded values.\n  - The exact steps you’ll take to recalc formulas and verify zero Excel errors after your edits.\n\n  Focus on the workflow and controls you would follow; do not provide actual Excel files.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates mastery of the xlsx skill requirements.\n\n    Score 1.0 if:\n    - Clearly explains separating scenario assumptions into dedicated cells and referencing them in formulas (no hardcoded calculations).\n    - Describes applying the specified financial-model color coding (blue inputs, black formulas, green intra-workbook links, red external links, yellow highlights).\n    - Mentions required number-format rules for years, currency, percentages, multiples, zeros, and negatives.\n    - Covers documenting sources for hardcoded values.\n    - Specifies running recalc.py (or LibreOffice recalculation) and checking for zero Excel errors.\n\n    Score 0.5 if:\n    - Shows partial understanding (e.g., mentions formulas vs. hardcoding and one other requirement) but omits multiple critical elements such as color coding, number formats, or recalculation/error checks.\n\n    Score 0.0 if:\n    - Fails to reference the essential workflow controls (assumption separation, formula usage, formatting conventions, and recalculation/error verification).\n  extractor: last_assistant\n```",
          "score": 0.65,
          "rationale": "The test case demonstrates moderate quality with significant weaknesses. **Non-obviousness (5/10)**: While it tests transferable financial modeling concepts (assumption separation, formula usage, formatting standards), it heavily emphasizes skill-specific details like exact color codes (RGB values), the specific recalc.py script name, and precise formatting conventions that border on trivia rather than transferable principles. **Clarity (7/10)**: The grader is reasonably robust, accepting explanations of workflow rather than requiring exact implementation, though it could be more flexible about which specific conventions are mentioned. **Realism (6/10)**: The scenario is plausible but somewhat contrived—asking for a detailed *description* of how to update a model rather than actually doing it is artificial; practitioners would simply perform the task. A more realistic test would involve actually creating or modifying a spreadsheet file with these requirements."
        },
        {
          "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
          "submission": "```yaml\nname: create-financial-model\nskills:\n  - xlsx\nprompt: |\n  You are a financial analyst and have been asked to create a financial model in an Excel spreadsheet for a hypothetical company. The model should include the following:\n\n  1. **Assumptions Section**: Include assumptions for revenue growth rate, operating margin, and tax rate. Assume a revenue growth rate of 10% per year, an operating margin of 20%, and a tax rate of 30%. Place these assumptions in separate cells and use blue text (RGB: 0,0,255).\n\n  2. **Revenue Projections**: Create a table for revenue projections for the next 5 years (2024-2028). Use the revenue growth rate assumption to calculate the projected revenue for each year. Ensure the formula is dynamic and can be easily updated if the growth rate changes.\n\n  3. **Operating Income Calculation**: Calculate the operating income for each year using the operating margin assumption. Use black text (RGB: 0,0,0) for all formulas and calculations.\n\n  4. **Net Income Calculation**: Calculate the net income for each year after applying the tax rate. Use black text (RGB: 0,0,0) for all formulas and calculations.\n\n  5. **Number Formatting**: Ensure that all currency values are formatted as $#,##0 (units in headers), and all percentages are formatted to one decimal place (0.0%).\n\n  6. **Save the Spreadsheet**: Save the spreadsheet as `financial_model.xlsx`.\n\n  7. **Recalculate Formulas**: Use the `recalc.py` script to recalculate formulas and verify that there are no formula errors (e.g., #REF!, #DIV/0!, #VALUE!, #N/A, #NAME?).\n\n  8. **Documentation**: Document the sources for the assumptions in comments or adjacent cells. Use the format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\".\n\n  **Note**: Ensure that the spreadsheet is well-organized and easy to read, with appropriate column widths and cell formatting.\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score the response based on the following criteria:\n\n    Score 1.0 if:\n    - The spreadsheet contains a correctly formatted assumptions section with blue text for the growth rate, operating margin, and tax rate.\n    - The revenue projections for the next 5 years are correctly calculated using the growth rate assumption.\n    - The operating income and net income are correctly calculated using the operating margin and tax rate assumptions.\n    - All currency values are formatted as $#,##0 with units in headers, and all percentages are formatted to one decimal place (0.0%).\n    - The spreadsheet is saved as `financial_model.xlsx`.\n    - The `recalc.py` script is used to recalculate formulas, and there are no formula errors.\n    - The sources for the assumptions are documented in comments or adjacent cells.\n\n    Score 0.5 if:\n    - Most of the requirements are met, but there are minor issues with formatting, documentation, or formula errors.\n\n    Score 0.0 if:\n    - The spreadsheet is not created, or the model does not meet the basic requirements.\n  extractor: last_assistant\n```",
          "score": 0.45,
          "rationale": "The test case has significant issues across all dimensions. **Non-obviousness (3/10)**: The test heavily focuses on skill-specific trivia like exact RGB color codes (0,0,255 for blue), specific formatting conventions ($#,##0), and the recalc.py script name - these are memorization details rather than transferable financial modeling knowledge. **Clarity (4/10)**: The grader is brittle, requiring exact compliance with arbitrary formatting details (blue text RGB values, specific number formats) that don't affect the model's functionality. It also requires using recalc.py by name rather than testing whether formulas work correctly. **Realism (6/10)**: While creating a financial model is realistic, the task is overly prescriptive about implementation details (exact colors, specific script usage) that practitioners wouldn't typically be constrained by. A real analyst would focus on correct calculations and clear presentation, not memorizing RGB codes."
        },
        {
          "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "submission": "```yaml\nname: create-spreadsheet-with-formulas\nskills:\n- xlsx\nprompt: |\n  Create a new Excel spreadsheet that calculates the total cost of products based on their prices and quantities. The spreadsheet should have two columns: \"Price\" and \"Quantity\". The total cost should be calculated using a formula that multiplies the price and quantity of each product and sums up the results. The formula should be placed in a separate column named \"Total Cost\". The spreadsheet should also include a header row with the column names.\n\n  Please provide the Python code to create this spreadsheet using the openpyxl library.\n\ntimeout: 300\ngrader:\n  kind: letta_judge\n  prompt: |\n    Score 1.0 if the response includes a complete and correct Python code snippet that creates the specified Excel spreadsheet with formulas. The code should use the openpyxl library and include a header row with column names, two columns for price and quantity, and a third column for the total cost calculated using a formula.\n\n    Score 0.5 if the response includes a partially correct Python code snippet that creates some parts of the spreadsheet but is missing key elements, such as the formula for calculating the total cost or the header row.\n\n    Score 0.0 if the response does not include a Python code snippet or if the code is completely incorrect and does not create the specified spreadsheet.\n  extractor: last_assistant\n```",
          "score": 0.3,
          "rationale": "The test has significant issues across all dimensions. **Non-obviousness (3/10)**: The task is too basic - creating a simple spreadsheet with formulas is trivial and doesn't test meaningful domain knowledge from the skill document (e.g., formula error prevention, color coding standards, recalculation workflow). **Clarity (2/10)**: The grader is fundamentally broken - it asks for Python code in the prompt but then evaluates whether code was provided, which any LLM can do without the skill. The test doesn't verify the code actually works, doesn't check if formulas are used (vs hardcoded values - a key skill requirement), and doesn't verify the mandatory recalc.py step. **Realism (5/10)**: While creating spreadsheets with formulas is realistic, asking for 'the Python code' makes it an artificial coding exercise rather than a practical spreadsheet task. A real practitioner would be asked to 'create a spreadsheet' not 'provide Python code to create a spreadsheet.'"
        },
        {
          "model_name": "deepseek-chat",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek-reasoner",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "deepseek/deepseek-chat-v3.1",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        },
        {
          "model_name": "mistralai/mistral-large-2512",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: Error code: 429 - {'error': 'Rate limited', 'reasons': ['model-unknown'], 'currentRequestFrequency': 0, 'currentTokenFrequency': 0, 'remainingMonthlyCredits': 0, 'remainingPurchasedCredits': 0}"
        }
      ]
    },
    {
      "id": "skill-test-Log Privacy & Redaction",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 14,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Log Privacy & Redaction\n\n**SKILL.md Content:**\n```markdown\n---\nname: Log Privacy & Redaction\ndescription: Avoid leaking secrets and PII in logs while keeping them useful: structured logging, safe fields, redaction patterns, and incident-safe practices.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Log Privacy & Redaction\n\n## Overview\n\nLogs are durable and widely accessible. Treat them as sensitive data. The goal is to:\n- Preserve debuggability (correlation IDs, event types, error codes)\n- Avoid sensitive exposure (tokens, passwords, personal data)\n- Make redaction consistent and testable\n\n## What to Log (Safe Defaults)\n\n- Request ID / trace ID / correlation ID\n- High-level event type (e.g., `payment_failed`)\n- Error class + stable error code\n- Timing/latency metrics\n- Sanitized identifiers (hashes) instead of raw PII where possible\n\n## What NOT to Log\n\n- Access tokens / refresh tokens / API keys\n- Passwords, secrets, session cookies\n- Full credit card numbers, SSNs, full addresses\n- Raw request bodies unless explicitly scrubbed\n\n## Workflow\n\n1. **Define a safe schema** for log fields (allowlist).\n2. **Redact at boundaries**:\n   - Incoming headers (Authorization)\n   - Query params (tokens)\n   - Error messages that echo input\n3. **Redaction rules**:\n   - Replace values with `[REDACTED:<TYPE>]`\n   - Keep minimal prefixes/suffixes only if needed for debugging\n4. **Test redaction**:\n   - Unit tests for known patterns\n   - Property tests that no secret patterns pass through\n5. **Incident mode**:\n   - Never “temporarily log everything”\n   - Add targeted, short-lived instrumentation with explicit cleanup\n\n## Common Patterns to Redact\n\n- `Authorization: Bearer <...>`\n- JWTs (`eyJ...`)\n- AWS keys (`AKIA...`)\n- Database URLs with credentials (`postgres://user:pass@...`)\n\n## Checklist\n\n- [ ] Allowlist schema for log fields\n- [ ] Centralized redaction middleware\n- [ ] Tests for redaction patterns\n- [ ] No raw request/response bodies in logs by default\n- [ ] Correlation IDs present for debugging\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Log Privacy & Redaction\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Log Privacy & Redaction\n\n**SKILL.md Content:**\n```markdown\n---\nname: Log Privacy & Redaction\ndescription: Avoid leaking secrets and PII in logs while keeping them useful: structured logging, safe fields, redaction patterns, and incident-safe practices.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Log Privacy & Redaction\n\n## Overview\n\nLogs are durable and widely accessible. Treat them as sensitive data. The goal is to:\n- Preserve debuggability (correlation IDs, event types, error codes)\n- Avoid sensitive exposure (tokens, passwords, personal data)\n- Make redaction consistent and testable\n\n## What to Log (Safe Defaults)\n\n- Request ID / trace ID / correlation ID\n- High-level event type (e.g., `payment_failed`)\n- Error class + stable error code\n- Timing/latency metrics\n- Sanitized identifiers (hashes) instead of raw PII where possible\n\n## What NOT to Log\n\n- Access tokens / refresh tokens / API keys\n- Passwords, secrets, session cookies\n- Full credit card numbers, SSNs, full addresses\n- Raw request bodies unless explicitly scrubbed\n\n## Workflow\n\n1. **Define a safe schema** for log fields (allowlist).\n2. **Redact at boundaries**:\n   - Incoming headers (Authorization)\n   - Query params (tokens)\n   - Error messages that echo input\n3. **Redaction rules**:\n   - Replace values with `[REDACTED:<TYPE>]`\n   - Keep minimal prefixes/suffixes only if needed for debugging\n4. **Test redaction**:\n   - Unit tests for known patterns\n   - Property tests that no secret patterns pass through\n5. **Incident mode**:\n   - Never “temporarily log everything”\n   - Add targeted, short-lived instrumentation with explicit cleanup\n\n## Common Patterns to Redact\n\n- `Authorization: Bearer <...>`\n- JWTs (`eyJ...`)\n- AWS keys (`AKIA...`)\n- Database URLs with credentials (`postgres://user:pass@...`)\n\n## Checklist\n\n- [ ] Allowlist schema for log fields\n- [ ] Centralized redaction middleware\n- [ ] Tests for redaction patterns\n- [ ] No raw request/response bodies in logs by default\n- [ ] Correlation IDs present for debugging\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Log Privacy & Redaction\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Log Privacy & Redaction",
          "skill_dir": "log-privacy-redaction",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-OAuth/OIDC Practical Debugging",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 15,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** OAuth/OIDC Practical Debugging\n\n**SKILL.md Content:**\n```markdown\n---\nname: OAuth/OIDC Practical Debugging\ndescription: Diagnose common OAuth2/OIDC failures in authorization code + PKCE flows, validate tokens correctly, and debug production-only auth issues safely.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# OAuth/OIDC Practical Debugging\n\n## Overview\n\nMost OAuth/OIDC issues come from mismatched inputs between steps (redirect URI, PKCE verifier, client settings) or incorrect token validation. The fastest path is to trace the full flow and compare exact values.\n\n## Flow Checklist (Auth Code + PKCE)\n\n1. **/authorize request**\n   - `client_id`, `redirect_uri`, `scope`, `response_type=code`\n   - Generate `state` (CSRF) and `nonce` (OIDC) and persist them\n   - Generate `code_verifier` and derived `code_challenge`\n2. **Redirect callback**\n   - Verify `state`\n   - Capture `code`\n3. **/token exchange**\n   - Send the same `redirect_uri`\n   - Send `code_verifier` for PKCE\n4. **Validate tokens**\n   - Access token: used to call APIs (often opaque)\n   - ID token: validate signature + claims (`iss`, `aud`, `exp`, `nonce`)\n\n## Debugging invalid_grant\n\nCommon causes:\n- Redirect URI mismatch (must match exactly)\n- Code expired or reused\n- Wrong `code_verifier` (lost between steps or wrong user/session)\n- Clock skew when server enforces narrow time windows\n\nActionable checks:\n- Compare the exact `redirect_uri` string from /authorize and /token\n- Confirm the code is exchanged once\n- Confirm verifier stored per user session and retrieved correctly\n\n## Token Validation Pitfalls\n\n- Validate `iss` and `aud` against the correct environment (dev vs prod tenant).\n- Cache JWKS but handle key rotation (respect cache headers; retry on kid mismatch).\n- Never accept an ID token without verifying signature and `exp`.\n\n## Safe Logging\n\nNever log:\n- auth codes, access tokens, refresh tokens, client secrets\n\nLog instead:\n- request IDs, correlation IDs, error codes, and redacted token fingerprints\n\n## Checklist\n\n- [ ] Trace /authorize → callback → /token end-to-end\n- [ ] Compare redirect_uri and PKCE verifier values\n- [ ] Validate ID token signature + iss/aud/exp/nonce\n- [ ] Check env-specific settings (issuer, allowed origins, redirect URIs)\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - OAuth/OIDC Practical Debugging\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** OAuth/OIDC Practical Debugging\n\n**SKILL.md Content:**\n```markdown\n---\nname: OAuth/OIDC Practical Debugging\ndescription: Diagnose common OAuth2/OIDC failures in authorization code + PKCE flows, validate tokens correctly, and debug production-only auth issues safely.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# OAuth/OIDC Practical Debugging\n\n## Overview\n\nMost OAuth/OIDC issues come from mismatched inputs between steps (redirect URI, PKCE verifier, client settings) or incorrect token validation. The fastest path is to trace the full flow and compare exact values.\n\n## Flow Checklist (Auth Code + PKCE)\n\n1. **/authorize request**\n   - `client_id`, `redirect_uri`, `scope`, `response_type=code`\n   - Generate `state` (CSRF) and `nonce` (OIDC) and persist them\n   - Generate `code_verifier` and derived `code_challenge`\n2. **Redirect callback**\n   - Verify `state`\n   - Capture `code`\n3. **/token exchange**\n   - Send the same `redirect_uri`\n   - Send `code_verifier` for PKCE\n4. **Validate tokens**\n   - Access token: used to call APIs (often opaque)\n   - ID token: validate signature + claims (`iss`, `aud`, `exp`, `nonce`)\n\n## Debugging invalid_grant\n\nCommon causes:\n- Redirect URI mismatch (must match exactly)\n- Code expired or reused\n- Wrong `code_verifier` (lost between steps or wrong user/session)\n- Clock skew when server enforces narrow time windows\n\nActionable checks:\n- Compare the exact `redirect_uri` string from /authorize and /token\n- Confirm the code is exchanged once\n- Confirm verifier stored per user session and retrieved correctly\n\n## Token Validation Pitfalls\n\n- Validate `iss` and `aud` against the correct environment (dev vs prod tenant).\n- Cache JWKS but handle key rotation (respect cache headers; retry on kid mismatch).\n- Never accept an ID token without verifying signature and `exp`.\n\n## Safe Logging\n\nNever log:\n- auth codes, access tokens, refresh tokens, client secrets\n\nLog instead:\n- request IDs, correlation IDs, error codes, and redacted token fingerprints\n\n## Checklist\n\n- [ ] Trace /authorize → callback → /token end-to-end\n- [ ] Compare redirect_uri and PKCE verifier values\n- [ ] Validate ID token signature + iss/aud/exp/nonce\n- [ ] Check env-specific settings (issuer, allowed origins, redirect URIs)\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - OAuth/OIDC Practical Debugging\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "OAuth/OIDC Practical Debugging",
          "skill_dir": "oauth-oidc-debugging",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-pdf",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 16,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** pdf\n\n**SKILL.md Content:**\n```markdown\n---\nname: pdf\ndescription: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n\n```\n\n## Additional Skill Files\n\n**reference.md:**\n```markdown\n# PDF Processing Advanced Reference\n\nThis document contains advanced PDF processing features, detailed examples, and additional libraries not covered in the main skill instructions.\n\n## pypdfium2 Library (Apache/BSD License)\n\n### Overview\npypdfium2 is a Python binding for PDFium (Chromium's PDF library). It's excellent for fast PDF rendering, image generation, and serves as a PyMuPDF replacement.\n\n### Render PDF to Images\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\n\n# Load PDF\npdf = pdfium.PdfDocument(\"document.pdf\")\n\n# Render page to image\npage = pdf[0]  # First page\nbitmap = page.render(\n    scale=2.0,  # Higher resolution\n    rotation=0  # No rotation\n)\n\n# Convert to PIL Image\nimg = bitmap.to_pil()\nimg.save(\"page_1.png\", \"PNG\")\n\n# Process multiple pages\nfor i, page in enumerate(pdf):\n    bitmap = page.render(scale=1.5)\n    img = bitmap.to_pil()\n    img.save(f\"page_{i+1}.jpg\", \"JPEG\", quality=90)\n```\n\n### Extract Text with pypdfium2\n```python\nimport pypdfium2 as pdfium\n\npdf = pdfium.PdfDocument(\"document.pdf\")\nfor i, page in enumerate(pdf):\n    text = page.get_text()\n    print(f\"Page {i+1} text length: {len(text)} chars\")\n```\n\n## JavaScript Libraries\n\n### pdf-lib (MIT License)\n\npdf-lib is a powerful JavaScript library for creating and modifying PDF documents in any JavaScript environment.\n\n#### Load and Manipulate Existing PDF\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function manipulatePDF() {\n    // Load existing PDF\n    const existingPdfBytes = fs.readFileSync('input.pdf');\n    const pdfDoc = await PDFDocument.load(existingPdfBytes);\n\n    // Get page count\n    const pageCount = pdfDoc.getPageCount();\n    console.log(`Document has ${pageCount} pages`);\n\n    // Add new page\n    const newPage = pdfDoc.addPage([600, 400]);\n    newPage.drawText('Added by pdf-lib', {\n        x: 100,\n        y: 300,\n        size: 16\n    });\n\n    // Save modified PDF\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('modified.pdf', pdfBytes);\n}\n```\n\n#### Create Complex PDFs from Scratch\n```javascript\nimport { PDFDocument, rgb, StandardFonts } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function createPDF() {\n    const pdfDoc = await PDFDocument.create();\n\n    // Add fonts\n    const helveticaFont = await pdfDoc.embedFont(StandardFonts.Helvetica);\n    const helveticaBold = await pdfDoc.embedFont(StandardFonts.HelveticaBold);\n\n    // Add page\n    const page = pdfDoc.addPage([595, 842]); // A4 size\n    const { width, height } = page.getSize();\n\n    // Add text with styling\n    page.drawText('Invoice #12345', {\n        x: 50,\n        y: height - 50,\n        size: 18,\n        font: helveticaBold,\n        color: rgb(0.2, 0.2, 0.8)\n    });\n\n    // Add rectangle (header background)\n    page.drawRectangle({\n        x: 40,\n        y: height - 100,\n        width: width - 80,\n        height: 30,\n        color: rgb(0.9, 0.9, 0.9)\n    });\n\n    // Add table-like content\n    const items = [\n        ['Item', 'Qty', 'Price', 'Total'],\n        ['Widget', '2', '$50', '$100'],\n        ['Gadget', '1', '$75', '$75']\n    ];\n\n    let yPos = height - 150;\n    items.forEach(row => {\n        let xPos = 50;\n        row.forEach(cell => {\n            page.drawText(cell, {\n                x: xPos,\n                y: yPos,\n                size: 12,\n                font: helveticaFont\n            });\n            xPos += 120;\n        });\n        yPos -= 25;\n    });\n\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('created.pdf', pdfBytes);\n}\n```\n\n#### Advanced Merge and Split Operations\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function mergePDFs() {\n    // Create new document\n    const mergedPdf = await PDFDocument.create();\n\n    // Load source PDFs\n    const pdf1Bytes = fs.readFileSync('doc1.pdf');\n    const pdf2Bytes = fs.readFileSync('doc2.pdf');\n\n    const pdf1 = await PDFDocument.load(pdf1Bytes);\n    const pdf2 = await PDFDocument.load(pdf2Bytes);\n\n    // Copy pages from first PDF\n    const pdf1Pages = await mergedPdf.copyPages(pdf1, pdf1.getPageIndices());\n    pdf1Pages.forEach(page => mergedPdf.addPage(page));\n\n    // Copy specific pages from second PDF (pages 0, 2, 4)\n    const pdf2Pages = await mergedPdf.copyPages(pdf2, [0, 2, 4]);\n    pdf2Pages.forEach(page => mergedPdf.addPage(page));\n\n    const mergedPdfBytes = await mergedPdf.save();\n    fs.writeFileSync('merged.pdf', mergedPdfBytes);\n}\n```\n\n### pdfjs-dist (Apache License)\n\nPDF.js is Mozilla's JavaScript library for rendering PDFs in the browser.\n\n#### Basic PDF Loading and Rendering\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configure worker (important for performance)\npdfjsLib.GlobalWorkerOptions.workerSrc = './pdf.worker.js';\n\nasync function renderPDF() {\n    // Load PDF\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    console.log(`Loaded PDF with ${pdf.numPages} pages`);\n\n    // Get first page\n    cons\n... (truncated)\n```\n\n**forms.md:**\n```markdown\n**CRITICAL: You MUST complete these steps in order. Do not skip ahead to writing code.**\n\nIf you need to fill out a PDF form, first check to see if the PDF has fillable form fields. Run this script from this file's directory:\n `python scripts/check_fillable_fields <file.pdf>`, and depending on the result go to either the \"Fillable fields\" or \"Non-fillable fields\" and follow those instructions.\n\n# Fillable fields\nIf the PDF has fillable form fields:\n- Run this script from this file's directory: `python scripts/extract_form_field_info.py <input.pdf> <field_info.json>`. It will create a JSON file with a list of fields in this format:\n```\n[\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"rect\": ([left, bottom, right, top] bounding box in PDF coordinates, y=0 is the bottom of the page),\n    \"type\": (\"text\", \"checkbox\", \"radio_group\", or \"choice\"),\n  },\n  // Checkboxes have \"checked_value\" and \"unchecked_value\" properties:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"checkbox\",\n    \"checked_value\": (Set the field to this value to check the checkbox),\n    \"unchecked_value\": (Set the field to this value to uncheck the checkbox),\n  },\n  // Radio groups have a \"radio_options\" list with the possible choices.\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"radio_group\",\n    \"radio_options\": [\n      {\n        \"value\": (set the field to this value to select this radio option),\n        \"rect\": (bounding box for the radio button for this option)\n      },\n      // Other radio options\n    ]\n  },\n  // Multiple choice fields have a \"choice_options\" list with the possible choices:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"choice\",\n    \"choice_options\": [\n      {\n        \"value\": (set the field to this value to select this option),\n        \"text\": (display text of the option)\n      },\n      // Other choice options\n    ],\n  }\n]\n```\n- Convert the PDF to PNGs (one image for each page) with this script (run from this file's directory):\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThen analyze the images to determine the purpose of each form field (make sure to convert the bounding box PDF coordinates to image coordinates).\n- Create a `field_values.json` file in this format with the values to be entered for each field:\n```\n[\n  {\n    \"field_id\": \"last_name\", // Must match the field_id from `extract_form_field_info.py`\n    \"description\": \"The user's last name\",\n    \"page\": 1, // Must match the \"page\" value in field_info.json\n    \"value\": \"Simpson\"\n  },\n  {\n    \"field_id\": \"Checkbox12\",\n    \"description\": \"Checkbox to be checked if the user is 18 or over\",\n    \"page\": 1,\n    \"value\": \"/On\" // If this is a checkbox, use its \"checked_value\" value to check it. If it's a radio button group, use one of the \"value\" values in \"radio_options\".\n  },\n  // more fields\n]\n```\n- Run the `fill_fillable_fields.py` script from this file's directory to create a filled-in PDF:\n`python scripts/fill_fillable_fields.py <input pdf> <field_values.json> <output pdf>`\nThis script will verify that the field IDs and values you provide are valid; if it prints error messages, correct the appropriate fields and try again.\n\n# Non-fillable fields\nIf the PDF doesn't have fillable form fields, you'll need to visually determine where the data should be added and create text annotations. Follow the below steps *exactly*. You MUST perform all of these steps to ensure that the the form is accurately completed. Details for each step are below.\n- Convert the PDF to PNG images and determine field bounding boxes.\n- Create a JSON file with field information and validation images showing the bounding boxes.\n- Validate the the bounding boxes.\n- Use the bounding boxes to fill in the form.\n\n## Step 1: Visual Analysis (REQUIRED)\n- Convert the PDF to PNG images. Run this script from this file's directory:\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThe script will create a PNG image for each page in the PDF.\n- Carefully examine each PNG image and identify all form fields and areas where the user should enter data. For each form field where the user should enter text, determine bounding boxes for both the form field label, and the area where the user should enter text. The label and entry bounding boxes MUST NOT INTERSECT; the text entry box should only include the area where data should be entered. Usually this area will be immediately to the side, above, or below its label. Entry bounding boxes must be tall and wide enough to contain their text.\n\nThese are some examples of form structures that you might see:\n\n*Label inside box*\n```\n┌────────────────────────┐\n│ Name:                  │\n└────────────────────────┘\n```\nThe input area should be to the right of the \"Name\" label and extend to the edge of the box.\n\n*Label before line*\n```\nEmail: ___\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - pdf\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** pdf\n\n**SKILL.md Content:**\n```markdown\n---\nname: pdf\ndescription: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n\n```\n\n## Additional Skill Files\n\n**reference.md:**\n```markdown\n# PDF Processing Advanced Reference\n\nThis document contains advanced PDF processing features, detailed examples, and additional libraries not covered in the main skill instructions.\n\n## pypdfium2 Library (Apache/BSD License)\n\n### Overview\npypdfium2 is a Python binding for PDFium (Chromium's PDF library). It's excellent for fast PDF rendering, image generation, and serves as a PyMuPDF replacement.\n\n### Render PDF to Images\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\n\n# Load PDF\npdf = pdfium.PdfDocument(\"document.pdf\")\n\n# Render page to image\npage = pdf[0]  # First page\nbitmap = page.render(\n    scale=2.0,  # Higher resolution\n    rotation=0  # No rotation\n)\n\n# Convert to PIL Image\nimg = bitmap.to_pil()\nimg.save(\"page_1.png\", \"PNG\")\n\n# Process multiple pages\nfor i, page in enumerate(pdf):\n    bitmap = page.render(scale=1.5)\n    img = bitmap.to_pil()\n    img.save(f\"page_{i+1}.jpg\", \"JPEG\", quality=90)\n```\n\n### Extract Text with pypdfium2\n```python\nimport pypdfium2 as pdfium\n\npdf = pdfium.PdfDocument(\"document.pdf\")\nfor i, page in enumerate(pdf):\n    text = page.get_text()\n    print(f\"Page {i+1} text length: {len(text)} chars\")\n```\n\n## JavaScript Libraries\n\n### pdf-lib (MIT License)\n\npdf-lib is a powerful JavaScript library for creating and modifying PDF documents in any JavaScript environment.\n\n#### Load and Manipulate Existing PDF\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function manipulatePDF() {\n    // Load existing PDF\n    const existingPdfBytes = fs.readFileSync('input.pdf');\n    const pdfDoc = await PDFDocument.load(existingPdfBytes);\n\n    // Get page count\n    const pageCount = pdfDoc.getPageCount();\n    console.log(`Document has ${pageCount} pages`);\n\n    // Add new page\n    const newPage = pdfDoc.addPage([600, 400]);\n    newPage.drawText('Added by pdf-lib', {\n        x: 100,\n        y: 300,\n        size: 16\n    });\n\n    // Save modified PDF\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('modified.pdf', pdfBytes);\n}\n```\n\n#### Create Complex PDFs from Scratch\n```javascript\nimport { PDFDocument, rgb, StandardFonts } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function createPDF() {\n    const pdfDoc = await PDFDocument.create();\n\n    // Add fonts\n    const helveticaFont = await pdfDoc.embedFont(StandardFonts.Helvetica);\n    const helveticaBold = await pdfDoc.embedFont(StandardFonts.HelveticaBold);\n\n    // Add page\n    const page = pdfDoc.addPage([595, 842]); // A4 size\n    const { width, height } = page.getSize();\n\n    // Add text with styling\n    page.drawText('Invoice #12345', {\n        x: 50,\n        y: height - 50,\n        size: 18,\n        font: helveticaBold,\n        color: rgb(0.2, 0.2, 0.8)\n    });\n\n    // Add rectangle (header background)\n    page.drawRectangle({\n        x: 40,\n        y: height - 100,\n        width: width - 80,\n        height: 30,\n        color: rgb(0.9, 0.9, 0.9)\n    });\n\n    // Add table-like content\n    const items = [\n        ['Item', 'Qty', 'Price', 'Total'],\n        ['Widget', '2', '$50', '$100'],\n        ['Gadget', '1', '$75', '$75']\n    ];\n\n    let yPos = height - 150;\n    items.forEach(row => {\n        let xPos = 50;\n        row.forEach(cell => {\n            page.drawText(cell, {\n                x: xPos,\n                y: yPos,\n                size: 12,\n                font: helveticaFont\n            });\n            xPos += 120;\n        });\n        yPos -= 25;\n    });\n\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('created.pdf', pdfBytes);\n}\n```\n\n#### Advanced Merge and Split Operations\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function mergePDFs() {\n    // Create new document\n    const mergedPdf = await PDFDocument.create();\n\n    // Load source PDFs\n    const pdf1Bytes = fs.readFileSync('doc1.pdf');\n    const pdf2Bytes = fs.readFileSync('doc2.pdf');\n\n    const pdf1 = await PDFDocument.load(pdf1Bytes);\n    const pdf2 = await PDFDocument.load(pdf2Bytes);\n\n    // Copy pages from first PDF\n    const pdf1Pages = await mergedPdf.copyPages(pdf1, pdf1.getPageIndices());\n    pdf1Pages.forEach(page => mergedPdf.addPage(page));\n\n    // Copy specific pages from second PDF (pages 0, 2, 4)\n    const pdf2Pages = await mergedPdf.copyPages(pdf2, [0, 2, 4]);\n    pdf2Pages.forEach(page => mergedPdf.addPage(page));\n\n    const mergedPdfBytes = await mergedPdf.save();\n    fs.writeFileSync('merged.pdf', mergedPdfBytes);\n}\n```\n\n### pdfjs-dist (Apache License)\n\nPDF.js is Mozilla's JavaScript library for rendering PDFs in the browser.\n\n#### Basic PDF Loading and Rendering\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configure worker (important for performance)\npdfjsLib.GlobalWorkerOptions.workerSrc = './pdf.worker.js';\n\nasync function renderPDF() {\n    // Load PDF\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    console.log(`Loaded PDF with ${pdf.numPages} pages`);\n\n    // Get first page\n    cons\n... (truncated)\n```\n\n**forms.md:**\n```markdown\n**CRITICAL: You MUST complete these steps in order. Do not skip ahead to writing code.**\n\nIf you need to fill out a PDF form, first check to see if the PDF has fillable form fields. Run this script from this file's directory:\n `python scripts/check_fillable_fields <file.pdf>`, and depending on the result go to either the \"Fillable fields\" or \"Non-fillable fields\" and follow those instructions.\n\n# Fillable fields\nIf the PDF has fillable form fields:\n- Run this script from this file's directory: `python scripts/extract_form_field_info.py <input.pdf> <field_info.json>`. It will create a JSON file with a list of fields in this format:\n```\n[\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"rect\": ([left, bottom, right, top] bounding box in PDF coordinates, y=0 is the bottom of the page),\n    \"type\": (\"text\", \"checkbox\", \"radio_group\", or \"choice\"),\n  },\n  // Checkboxes have \"checked_value\" and \"unchecked_value\" properties:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"checkbox\",\n    \"checked_value\": (Set the field to this value to check the checkbox),\n    \"unchecked_value\": (Set the field to this value to uncheck the checkbox),\n  },\n  // Radio groups have a \"radio_options\" list with the possible choices.\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"radio_group\",\n    \"radio_options\": [\n      {\n        \"value\": (set the field to this value to select this radio option),\n        \"rect\": (bounding box for the radio button for this option)\n      },\n      // Other radio options\n    ]\n  },\n  // Multiple choice fields have a \"choice_options\" list with the possible choices:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"choice\",\n    \"choice_options\": [\n      {\n        \"value\": (set the field to this value to select this option),\n        \"text\": (display text of the option)\n      },\n      // Other choice options\n    ],\n  }\n]\n```\n- Convert the PDF to PNGs (one image for each page) with this script (run from this file's directory):\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThen analyze the images to determine the purpose of each form field (make sure to convert the bounding box PDF coordinates to image coordinates).\n- Create a `field_values.json` file in this format with the values to be entered for each field:\n```\n[\n  {\n    \"field_id\": \"last_name\", // Must match the field_id from `extract_form_field_info.py`\n    \"description\": \"The user's last name\",\n    \"page\": 1, // Must match the \"page\" value in field_info.json\n    \"value\": \"Simpson\"\n  },\n  {\n    \"field_id\": \"Checkbox12\",\n    \"description\": \"Checkbox to be checked if the user is 18 or over\",\n    \"page\": 1,\n    \"value\": \"/On\" // If this is a checkbox, use its \"checked_value\" value to check it. If it's a radio button group, use one of the \"value\" values in \"radio_options\".\n  },\n  // more fields\n]\n```\n- Run the `fill_fillable_fields.py` script from this file's directory to create a filled-in PDF:\n`python scripts/fill_fillable_fields.py <input pdf> <field_values.json> <output pdf>`\nThis script will verify that the field IDs and values you provide are valid; if it prints error messages, correct the appropriate fields and try again.\n\n# Non-fillable fields\nIf the PDF doesn't have fillable form fields, you'll need to visually determine where the data should be added and create text annotations. Follow the below steps *exactly*. You MUST perform all of these steps to ensure that the the form is accurately completed. Details for each step are below.\n- Convert the PDF to PNG images and determine field bounding boxes.\n- Create a JSON file with field information and validation images showing the bounding boxes.\n- Validate the the bounding boxes.\n- Use the bounding boxes to fill in the form.\n\n## Step 1: Visual Analysis (REQUIRED)\n- Convert the PDF to PNG images. Run this script from this file's directory:\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThe script will create a PNG image for each page in the PDF.\n- Carefully examine each PNG image and identify all form fields and areas where the user should enter data. For each form field where the user should enter text, determine bounding boxes for both the form field label, and the area where the user should enter text. The label and entry bounding boxes MUST NOT INTERSECT; the text entry box should only include the area where data should be entered. Usually this area will be immediately to the side, above, or below its label. Entry bounding boxes must be tall and wide enough to contain their text.\n\nThese are some examples of form structures that you might see:\n\n*Label inside box*\n```\n┌────────────────────────┐\n│ Name:                  │\n└────────────────────────┘\n```\nThe input area should be to the right of the \"Name\" label and extend to the edge of the box.\n\n*Label before line*\n```\nEmail: ___\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - pdf\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "pdf",
          "skill_dir": "pdf",
          "skill_file_tree": "LICENSE.txt\nSKILL.md\nforms.md\nreference.md\nscripts/check_bounding_boxes.py\nscripts/check_bounding_boxes_test.py\nscripts/check_fillable_fields.py\nscripts/convert_pdf_to_images.py\nscripts/create_validation_image.py\nscripts/extract_form_field_info.py\nscripts/fill_fillable_fields.py\nscripts/fill_pdf_form_with_annotations.py",
          "additional_file_names": [
            "reference.md",
            "forms.md"
          ]
        }
      },
      "results": []
    },
    {
      "id": "skill-test-PostgreSQL Indexing & EXPLAIN",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 17,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** PostgreSQL Indexing & EXPLAIN\n\n**SKILL.md Content:**\n```markdown\n---\nname: PostgreSQL Indexing & EXPLAIN\ndescription: Use EXPLAIN ANALYZE to find bottlenecks and choose the right indexes (btree/gin/partial/covering) with safe production rollout practices.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PostgreSQL Indexing & EXPLAIN\n\n## Overview\n\nIndexing is about matching your query patterns. A “missing index” isn’t always the issue — sometimes the query shape, statistics, or join strategy is the bottleneck. Use EXPLAIN ANALYZE to ground decisions in evidence.\n\n## Workflow\n\n1. Capture the slow query + parameters.\n2. Run `EXPLAIN (ANALYZE, BUFFERS)` in a safe environment (or on prod for read-only queries).\n3. Identify the primary cost center:\n   - Full table scans (Seq Scan)\n   - Large sorts\n   - Nested loops with huge row counts\n   - Hash joins spilling to disk\n4. Decide the fix:\n   - Add/adjust indexes\n   - Rewrite query (predicate pushdown, reduce rows early)\n   - Refresh stats (`ANALYZE`)\n5. Roll out safely:\n   - Use `CREATE INDEX CONCURRENTLY` in production to avoid blocking writes.\n   - Monitor build time and IO impact.\n\n## Index Selection Cheatsheet\n\n- **B-tree**: equality + range filters, ordering, most common.\n- **Composite index**: when queries filter on multiple columns; order columns by selectivity and query patterns.\n- **Partial index**: index only rows that match a predicate (e.g., `WHERE deleted_at IS NULL`).\n- **Covering index**: include extra columns to avoid heap fetches (via `INCLUDE`).\n- **GIN**: arrays/JSONB/text search.\n\n## Pitfalls\n\n- Low-selectivity columns (e.g., boolean) rarely benefit alone.\n- Index doesn’t help if predicate doesn’t match index order.\n- Too many indexes slow writes and bloat storage.\n\n## Checklist\n\n- [ ] Use EXPLAIN ANALYZE to confirm bottleneck\n- [ ] Choose index type based on predicate and data shape\n- [ ] Prefer CONCURRENTLY for prod\n- [ ] Measure before/after and watch write overhead\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - PostgreSQL Indexing & EXPLAIN\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** PostgreSQL Indexing & EXPLAIN\n\n**SKILL.md Content:**\n```markdown\n---\nname: PostgreSQL Indexing & EXPLAIN\ndescription: Use EXPLAIN ANALYZE to find bottlenecks and choose the right indexes (btree/gin/partial/covering) with safe production rollout practices.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PostgreSQL Indexing & EXPLAIN\n\n## Overview\n\nIndexing is about matching your query patterns. A “missing index” isn’t always the issue — sometimes the query shape, statistics, or join strategy is the bottleneck. Use EXPLAIN ANALYZE to ground decisions in evidence.\n\n## Workflow\n\n1. Capture the slow query + parameters.\n2. Run `EXPLAIN (ANALYZE, BUFFERS)` in a safe environment (or on prod for read-only queries).\n3. Identify the primary cost center:\n   - Full table scans (Seq Scan)\n   - Large sorts\n   - Nested loops with huge row counts\n   - Hash joins spilling to disk\n4. Decide the fix:\n   - Add/adjust indexes\n   - Rewrite query (predicate pushdown, reduce rows early)\n   - Refresh stats (`ANALYZE`)\n5. Roll out safely:\n   - Use `CREATE INDEX CONCURRENTLY` in production to avoid blocking writes.\n   - Monitor build time and IO impact.\n\n## Index Selection Cheatsheet\n\n- **B-tree**: equality + range filters, ordering, most common.\n- **Composite index**: when queries filter on multiple columns; order columns by selectivity and query patterns.\n- **Partial index**: index only rows that match a predicate (e.g., `WHERE deleted_at IS NULL`).\n- **Covering index**: include extra columns to avoid heap fetches (via `INCLUDE`).\n- **GIN**: arrays/JSONB/text search.\n\n## Pitfalls\n\n- Low-selectivity columns (e.g., boolean) rarely benefit alone.\n- Index doesn’t help if predicate doesn’t match index order.\n- Too many indexes slow writes and bloat storage.\n\n## Checklist\n\n- [ ] Use EXPLAIN ANALYZE to confirm bottleneck\n- [ ] Choose index type based on predicate and data shape\n- [ ] Prefer CONCURRENTLY for prod\n- [ ] Measure before/after and watch write overhead\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - PostgreSQL Indexing & EXPLAIN\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "PostgreSQL Indexing & EXPLAIN",
          "skill_dir": "postgres-indexing",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-pptx",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 18,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** pptx\n\n**SKILL.md Content:**\n```markdown\n---\nname: pptx\ndescription: \"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n- ✅ State your content-informed design approach BEFORE writing code\n- ✅ Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n- ✅ Create clear visual hierarchy through size, weight, and color\n- ✅ Ensure readability: strong contrast, appropriately sized text, clean alignment\n- ✅ Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90° or 270°\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (3×3, 4×4 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt × 405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (•, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (5×6)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)\n```\n\n## Additional Skill Files\n\n**ooxml.md:**\n```markdown\n# Office Open XML Technical Reference for PowerPoint\n\n**Important: Read this entire document before starting.** Critical XML schema rules and formatting requirements are covered throughout. Incorrect implementation can create invalid PPTX files that PowerPoint cannot open.\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<p:txBody>`**: `<a:bodyPr>`, `<a:lstStyle>`, `<a:p>`\n- **Whitespace**: Add `xml:space='preserve'` to `<a:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/`, reference in slide XML, set dimensions to fit slide bounds\n- **Relationships**: Update `ppt/slides/_rels/slideN.xml.rels` for each slide's resources\n- **Dirty attribute**: Add `dirty=\"0\"` to `<a:rPr>` and `<a:endParaRPr>` elements to indicate clean state\n\n## Presentation Structure\n\n### Basic Slide Structure\n```xml\n<!-- ppt/slides/slide1.xml -->\n<p:sld>\n  <p:cSld>\n    <p:spTree>\n      <p:nvGrpSpPr>...</p:nvGrpSpPr>\n      <p:grpSpPr>...</p:grpSpPr>\n      <!-- Shapes go here -->\n    </p:spTree>\n  </p:cSld>\n</p:sld>\n```\n\n### Text Box / Shape with Text\n```xml\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"2\" name=\"Title\"/>\n    <p:cNvSpPr>\n      <a:spLocks noGrp=\"1\"/>\n    </p:cNvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"838200\" y=\"365125\"/>\n      <a:ext cx=\"7772400\" cy=\"1470025\"/>\n    </a:xfrm>\n  </p:spPr>\n  <p:txBody>\n    <a:bodyPr/>\n    <a:lstStyle/>\n    <a:p>\n      <a:r>\n        <a:t>Slide Title</a:t>\n      </a:r>\n    </a:p>\n  </p:txBody>\n</p:sp>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<a:r>\n  <a:rPr b=\"1\"/>\n  <a:t>Bold Text</a:t>\n</a:r>\n\n<!-- Italic -->\n<a:r>\n  <a:rPr i=\"1\"/>\n  <a:t>Italic Text</a:t>\n</a:r>\n\n<!-- Underline -->\n<a:r>\n  <a:rPr u=\"sng\"/>\n  <a:t>Underlined</a:t>\n</a:r>\n\n<!-- Highlight -->\n<a:r>\n  <a:rPr>\n    <a:highlight>\n      <a:srgbClr val=\"FFFF00\"/>\n    </a:highlight>\n  </a:rPr>\n  <a:t>Highlighted Text</a:t>\n</a:r>\n\n<!-- Font and Size -->\n<a:r>\n  <a:rPr sz=\"2400\" typeface=\"Arial\">\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Colored Arial 24pt</a:t>\n</a:r>\n\n<!-- Complete formatting example -->\n<a:r>\n  <a:rPr lang=\"en-US\" sz=\"1400\" b=\"1\" dirty=\"0\">\n    <a:solidFill>\n      <a:srgbClr val=\"FAFAFA\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Formatted text</a:t>\n</a:r>\n```\n\n### Lists\n```xml\n<!-- Bullet list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buChar char=\"•\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First bullet point</a:t>\n  </a:r>\n</a:p>\n\n<!-- Numbered list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buAutoNum type=\"arabicPeriod\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First numbered item</a:t>\n  </a:r>\n</a:p>\n\n<!-- Second level indent -->\n<a:p>\n  <a:pPr lvl=\"1\">\n    <a:buChar char=\"•\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>Indented bullet</a:t>\n  </a:r>\n</a:p>\n```\n\n### Shapes\n```xml\n<!-- Rectangle -->\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"3\" name=\"Rectangle\"/>\n    <p:cNvSpPr/>\n    <p:nvPr/>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n    <a:ln w=\"25400\">\n      <a:solidFill>\n        <a:srgbClr val=\"000000\"/>\n      </a:solidFill>\n    </a:ln>\n  </p:spPr>\n</p:sp>\n\n<!-- Rounded Rectangle -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"roundRect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n\n<!-- Circle/Ellipse -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"ellipse\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n```\n\n### Images\n```xml\n<p:pic>\n  <p:nvPicPr>\n    <p:cNvPr id=\"4\" name=\"Picture\">\n      <a:hlinkClick r:id=\"\" action=\"ppaction://media\"/>\n    </p:cNvPr>\n    <p:cNvPicPr>\n      <a:picLocks noChangeAspect=\"1\"/>\n    </p:cNvPicPr>\n    <p:nvPr/>\n  </p:nvPicPr>\n  <p:blipFill>\n    <a:blip r:embed=\"rId2\"/>\n    <a:stretch>\n      <a:fillRect/>\n    </a:stretch>\n  </p:blipFill>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:pic>\n```\n\n### Tables\n```xml\n<p:graphicFrame>\n  <p:nvGraphicFramePr>\n    <p:cNvPr id=\"5\" name=\"Table\"/>\n    <p:cNvGraphicFramePr>\n      <a:graphicFrameLocks noGrp=\"1\"/>\n    </p:cNvGraphicFramePr>\n    <p:nvPr/>\n  </p:nvGraphicFramePr>\n  <p:xfrm>\n    <a:off x=\"1000000\" y=\"1000000\"/>\n    <a:ext cx=\"6000000\" cy=\"2000000\"/>\n  </p:xfrm>\n  <a:graphic>\n    <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/table\">\n      <a:tbl>\n        <a:tblGrid>\n          <a:gridCol w=\"3000000\"/>\n          <a:gridCol w=\"3000000\"/>\n        </a:tblGrid>\n        <a:tr h=\"500000\">\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 1</a:t>\n            \n... (truncated)\n```\n\n**html2pptx.md:**\n```markdown\n# HTML to PowerPoint Guide\n\nConvert HTML slides to PowerPoint presentations with accurate positioning using the `html2pptx.js` library.\n\n## Table of Contents\n\n1. [Creating HTML Slides](#creating-html-slides)\n2. [Using the html2pptx Library](#using-the-html2pptx-library)\n3. [Using PptxGenJS](#using-pptxgenjs)\n\n---\n\n## Creating HTML Slides\n\nEvery HTML slide must include proper body dimensions:\n\n### Layout Dimensions\n\n- **16:9** (default): `width: 720pt; height: 405pt`\n- **4:3**: `width: 720pt; height: 540pt`\n- **16:10**: `width: 720pt; height: 450pt`\n\n### Supported Elements\n\n- `<p>`, `<h1>`-`<h6>` - Text with styling\n- `<ul>`, `<ol>` - Lists (never use manual bullets •, -, *)\n- `<b>`, `<strong>` - Bold text (inline formatting)\n- `<i>`, `<em>` - Italic text (inline formatting)\n- `<u>` - Underlined text (inline formatting)\n- `<span>` - Inline formatting with CSS styles (bold, italic, underline, color)\n- `<br>` - Line breaks\n- `<div>` with bg/border - Becomes shape\n- `<img>` - Images\n- `class=\"placeholder\"` - Reserved space for charts (returns `{ id, x, y, w, h }`)\n\n### Critical Text Rules\n\n**ALL text MUST be inside `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags:**\n- ✅ Correct: `<div><p>Text here</p></div>`\n- ❌ Wrong: `<div>Text here</div>` - **Text will NOT appear in PowerPoint**\n- ❌ Wrong: `<span>Text</span>` - **Text will NOT appear in PowerPoint**\n- Text in `<div>` or `<span>` without a text tag will be silently ignored\n\n**NEVER use manual bullet symbols (•, -, *, etc.)** - Use `<ul>` or `<ol>` lists instead\n\n**ONLY use web-safe fonts that are universally available:**\n- ✅ Web-safe fonts: `Arial`, `Helvetica`, `Times New Roman`, `Georgia`, `Courier New`, `Verdana`, `Tahoma`, `Trebuchet MS`, `Impact`, `Comic Sans MS`\n- ❌ Wrong: `'Segoe UI'`, `'SF Pro'`, `'Roboto'`, custom fonts - **Might cause rendering issues**\n\n### Styling\n\n- Use `display: flex` on body to prevent margin collapse from breaking overflow validation\n- Use `margin` for spacing (padding included in size)\n- Inline formatting: Use `<b>`, `<i>`, `<u>` tags OR `<span>` with CSS styles\n  - `<span>` supports: `font-weight: bold`, `font-style: italic`, `text-decoration: underline`, `color: #rrggbb`\n  - `<span>` does NOT support: `margin`, `padding` (not supported in PowerPoint text runs)\n  - Example: `<span style=\"font-weight: bold; color: #667eea;\">Bold blue text</span>`\n- Flexbox works - positions calculated from rendered layout\n- Use hex colors with `#` prefix in CSS\n- **Text alignment**: Use CSS `text-align` (`center`, `right`, etc.) when needed as a hint to PptxGenJS for text formatting if text lengths are slightly off\n\n### Shape Styling (DIV elements only)\n\n**IMPORTANT: Backgrounds, borders, and shadows only work on `<div>` elements, NOT on text elements (`<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`)**\n\n- **Backgrounds**: CSS `background` or `background-color` on `<div>` elements only\n  - Example: `<div style=\"background: #f0f0f0;\">` - Creates a shape with background\n- **Borders**: CSS `border` on `<div>` elements converts to PowerPoint shape borders\n  - Supports uniform borders: `border: 2px solid #333333`\n  - Supports partial borders: `border-left`, `border-right`, `border-top`, `border-bottom` (rendered as line shapes)\n  - Example: `<div style=\"border-left: 8pt solid #E76F51;\">`\n- **Border radius**: CSS `border-radius` on `<div>` elements for rounded corners\n  - `border-radius: 50%` or higher creates circular shape\n  - Percentages <50% calculated relative to shape's smaller dimension\n  - Supports px and pt units (e.g., `border-radius: 8pt;`, `border-radius: 12px;`)\n  - Example: `<div style=\"border-radius: 25%;\">` on 100x200px box = 25% of 100px = 25px radius\n- **Box shadows**: CSS `box-shadow` on `<div>` elements converts to PowerPoint shadows\n  - Supports outer shadows only (inset shadows are ignored to prevent corruption)\n  - Example: `<div style=\"box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3);\">`\n  - Note: Inset/inner shadows are not supported by PowerPoint and will be skipped\n\n### Icons & Gradients\n\n- **CRITICAL: Never use CSS gradients (`linear-gradient`, `radial-gradient`)** - They don't convert to PowerPoint\n- **ALWAYS create gradient/icon PNGs FIRST using Sharp, then reference in HTML**\n- For gradients: Rasterize SVG to PNG background images\n- For icons: Rasterize react-icons SVG to PNG images\n- All visual effects must be pre-rendered as raster images before HTML rendering\n\n**Rasterizing Icons with Sharp:**\n\n```javascript\nconst React = require('react');\nconst ReactDOMServer = require('react-dom/server');\nconst sharp = require('sharp');\nconst { FaHome } = require('react-icons/fa');\n\nasync function rasterizeIconPng(IconComponent, color, size = \"256\", filename) {\n  const svgString = ReactDOMServer.renderToStaticMarkup(\n    React.createElement(IconComponent, { color: `#${color}`, size: size })\n  );\n\n  // Convert SVG to PNG using Sharp\n  await sharp(Buffer.from(svgString))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Rasterize icon b\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - pptx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** pptx\n\n**SKILL.md Content:**\n```markdown\n---\nname: pptx\ndescription: \"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n- ✅ State your content-informed design approach BEFORE writing code\n- ✅ Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n- ✅ Create clear visual hierarchy through size, weight, and color\n- ✅ Ensure readability: strong contrast, appropriately sized text, clean alignment\n- ✅ Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90° or 270°\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (3×3, 4×4 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt × 405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (•, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (5×6)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)\n```\n\n## Additional Skill Files\n\n**ooxml.md:**\n```markdown\n# Office Open XML Technical Reference for PowerPoint\n\n**Important: Read this entire document before starting.** Critical XML schema rules and formatting requirements are covered throughout. Incorrect implementation can create invalid PPTX files that PowerPoint cannot open.\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<p:txBody>`**: `<a:bodyPr>`, `<a:lstStyle>`, `<a:p>`\n- **Whitespace**: Add `xml:space='preserve'` to `<a:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/`, reference in slide XML, set dimensions to fit slide bounds\n- **Relationships**: Update `ppt/slides/_rels/slideN.xml.rels` for each slide's resources\n- **Dirty attribute**: Add `dirty=\"0\"` to `<a:rPr>` and `<a:endParaRPr>` elements to indicate clean state\n\n## Presentation Structure\n\n### Basic Slide Structure\n```xml\n<!-- ppt/slides/slide1.xml -->\n<p:sld>\n  <p:cSld>\n    <p:spTree>\n      <p:nvGrpSpPr>...</p:nvGrpSpPr>\n      <p:grpSpPr>...</p:grpSpPr>\n      <!-- Shapes go here -->\n    </p:spTree>\n  </p:cSld>\n</p:sld>\n```\n\n### Text Box / Shape with Text\n```xml\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"2\" name=\"Title\"/>\n    <p:cNvSpPr>\n      <a:spLocks noGrp=\"1\"/>\n    </p:cNvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"838200\" y=\"365125\"/>\n      <a:ext cx=\"7772400\" cy=\"1470025\"/>\n    </a:xfrm>\n  </p:spPr>\n  <p:txBody>\n    <a:bodyPr/>\n    <a:lstStyle/>\n    <a:p>\n      <a:r>\n        <a:t>Slide Title</a:t>\n      </a:r>\n    </a:p>\n  </p:txBody>\n</p:sp>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<a:r>\n  <a:rPr b=\"1\"/>\n  <a:t>Bold Text</a:t>\n</a:r>\n\n<!-- Italic -->\n<a:r>\n  <a:rPr i=\"1\"/>\n  <a:t>Italic Text</a:t>\n</a:r>\n\n<!-- Underline -->\n<a:r>\n  <a:rPr u=\"sng\"/>\n  <a:t>Underlined</a:t>\n</a:r>\n\n<!-- Highlight -->\n<a:r>\n  <a:rPr>\n    <a:highlight>\n      <a:srgbClr val=\"FFFF00\"/>\n    </a:highlight>\n  </a:rPr>\n  <a:t>Highlighted Text</a:t>\n</a:r>\n\n<!-- Font and Size -->\n<a:r>\n  <a:rPr sz=\"2400\" typeface=\"Arial\">\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Colored Arial 24pt</a:t>\n</a:r>\n\n<!-- Complete formatting example -->\n<a:r>\n  <a:rPr lang=\"en-US\" sz=\"1400\" b=\"1\" dirty=\"0\">\n    <a:solidFill>\n      <a:srgbClr val=\"FAFAFA\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Formatted text</a:t>\n</a:r>\n```\n\n### Lists\n```xml\n<!-- Bullet list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buChar char=\"•\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First bullet point</a:t>\n  </a:r>\n</a:p>\n\n<!-- Numbered list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buAutoNum type=\"arabicPeriod\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First numbered item</a:t>\n  </a:r>\n</a:p>\n\n<!-- Second level indent -->\n<a:p>\n  <a:pPr lvl=\"1\">\n    <a:buChar char=\"•\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>Indented bullet</a:t>\n  </a:r>\n</a:p>\n```\n\n### Shapes\n```xml\n<!-- Rectangle -->\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"3\" name=\"Rectangle\"/>\n    <p:cNvSpPr/>\n    <p:nvPr/>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n    <a:ln w=\"25400\">\n      <a:solidFill>\n        <a:srgbClr val=\"000000\"/>\n      </a:solidFill>\n    </a:ln>\n  </p:spPr>\n</p:sp>\n\n<!-- Rounded Rectangle -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"roundRect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n\n<!-- Circle/Ellipse -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"ellipse\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n```\n\n### Images\n```xml\n<p:pic>\n  <p:nvPicPr>\n    <p:cNvPr id=\"4\" name=\"Picture\">\n      <a:hlinkClick r:id=\"\" action=\"ppaction://media\"/>\n    </p:cNvPr>\n    <p:cNvPicPr>\n      <a:picLocks noChangeAspect=\"1\"/>\n    </p:cNvPicPr>\n    <p:nvPr/>\n  </p:nvPicPr>\n  <p:blipFill>\n    <a:blip r:embed=\"rId2\"/>\n    <a:stretch>\n      <a:fillRect/>\n    </a:stretch>\n  </p:blipFill>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:pic>\n```\n\n### Tables\n```xml\n<p:graphicFrame>\n  <p:nvGraphicFramePr>\n    <p:cNvPr id=\"5\" name=\"Table\"/>\n    <p:cNvGraphicFramePr>\n      <a:graphicFrameLocks noGrp=\"1\"/>\n    </p:cNvGraphicFramePr>\n    <p:nvPr/>\n  </p:nvGraphicFramePr>\n  <p:xfrm>\n    <a:off x=\"1000000\" y=\"1000000\"/>\n    <a:ext cx=\"6000000\" cy=\"2000000\"/>\n  </p:xfrm>\n  <a:graphic>\n    <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/table\">\n      <a:tbl>\n        <a:tblGrid>\n          <a:gridCol w=\"3000000\"/>\n          <a:gridCol w=\"3000000\"/>\n        </a:tblGrid>\n        <a:tr h=\"500000\">\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 1</a:t>\n            \n... (truncated)\n```\n\n**html2pptx.md:**\n```markdown\n# HTML to PowerPoint Guide\n\nConvert HTML slides to PowerPoint presentations with accurate positioning using the `html2pptx.js` library.\n\n## Table of Contents\n\n1. [Creating HTML Slides](#creating-html-slides)\n2. [Using the html2pptx Library](#using-the-html2pptx-library)\n3. [Using PptxGenJS](#using-pptxgenjs)\n\n---\n\n## Creating HTML Slides\n\nEvery HTML slide must include proper body dimensions:\n\n### Layout Dimensions\n\n- **16:9** (default): `width: 720pt; height: 405pt`\n- **4:3**: `width: 720pt; height: 540pt`\n- **16:10**: `width: 720pt; height: 450pt`\n\n### Supported Elements\n\n- `<p>`, `<h1>`-`<h6>` - Text with styling\n- `<ul>`, `<ol>` - Lists (never use manual bullets •, -, *)\n- `<b>`, `<strong>` - Bold text (inline formatting)\n- `<i>`, `<em>` - Italic text (inline formatting)\n- `<u>` - Underlined text (inline formatting)\n- `<span>` - Inline formatting with CSS styles (bold, italic, underline, color)\n- `<br>` - Line breaks\n- `<div>` with bg/border - Becomes shape\n- `<img>` - Images\n- `class=\"placeholder\"` - Reserved space for charts (returns `{ id, x, y, w, h }`)\n\n### Critical Text Rules\n\n**ALL text MUST be inside `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags:**\n- ✅ Correct: `<div><p>Text here</p></div>`\n- ❌ Wrong: `<div>Text here</div>` - **Text will NOT appear in PowerPoint**\n- ❌ Wrong: `<span>Text</span>` - **Text will NOT appear in PowerPoint**\n- Text in `<div>` or `<span>` without a text tag will be silently ignored\n\n**NEVER use manual bullet symbols (•, -, *, etc.)** - Use `<ul>` or `<ol>` lists instead\n\n**ONLY use web-safe fonts that are universally available:**\n- ✅ Web-safe fonts: `Arial`, `Helvetica`, `Times New Roman`, `Georgia`, `Courier New`, `Verdana`, `Tahoma`, `Trebuchet MS`, `Impact`, `Comic Sans MS`\n- ❌ Wrong: `'Segoe UI'`, `'SF Pro'`, `'Roboto'`, custom fonts - **Might cause rendering issues**\n\n### Styling\n\n- Use `display: flex` on body to prevent margin collapse from breaking overflow validation\n- Use `margin` for spacing (padding included in size)\n- Inline formatting: Use `<b>`, `<i>`, `<u>` tags OR `<span>` with CSS styles\n  - `<span>` supports: `font-weight: bold`, `font-style: italic`, `text-decoration: underline`, `color: #rrggbb`\n  - `<span>` does NOT support: `margin`, `padding` (not supported in PowerPoint text runs)\n  - Example: `<span style=\"font-weight: bold; color: #667eea;\">Bold blue text</span>`\n- Flexbox works - positions calculated from rendered layout\n- Use hex colors with `#` prefix in CSS\n- **Text alignment**: Use CSS `text-align` (`center`, `right`, etc.) when needed as a hint to PptxGenJS for text formatting if text lengths are slightly off\n\n### Shape Styling (DIV elements only)\n\n**IMPORTANT: Backgrounds, borders, and shadows only work on `<div>` elements, NOT on text elements (`<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`)**\n\n- **Backgrounds**: CSS `background` or `background-color` on `<div>` elements only\n  - Example: `<div style=\"background: #f0f0f0;\">` - Creates a shape with background\n- **Borders**: CSS `border` on `<div>` elements converts to PowerPoint shape borders\n  - Supports uniform borders: `border: 2px solid #333333`\n  - Supports partial borders: `border-left`, `border-right`, `border-top`, `border-bottom` (rendered as line shapes)\n  - Example: `<div style=\"border-left: 8pt solid #E76F51;\">`\n- **Border radius**: CSS `border-radius` on `<div>` elements for rounded corners\n  - `border-radius: 50%` or higher creates circular shape\n  - Percentages <50% calculated relative to shape's smaller dimension\n  - Supports px and pt units (e.g., `border-radius: 8pt;`, `border-radius: 12px;`)\n  - Example: `<div style=\"border-radius: 25%;\">` on 100x200px box = 25% of 100px = 25px radius\n- **Box shadows**: CSS `box-shadow` on `<div>` elements converts to PowerPoint shadows\n  - Supports outer shadows only (inset shadows are ignored to prevent corruption)\n  - Example: `<div style=\"box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3);\">`\n  - Note: Inset/inner shadows are not supported by PowerPoint and will be skipped\n\n### Icons & Gradients\n\n- **CRITICAL: Never use CSS gradients (`linear-gradient`, `radial-gradient`)** - They don't convert to PowerPoint\n- **ALWAYS create gradient/icon PNGs FIRST using Sharp, then reference in HTML**\n- For gradients: Rasterize SVG to PNG background images\n- For icons: Rasterize react-icons SVG to PNG images\n- All visual effects must be pre-rendered as raster images before HTML rendering\n\n**Rasterizing Icons with Sharp:**\n\n```javascript\nconst React = require('react');\nconst ReactDOMServer = require('react-dom/server');\nconst sharp = require('sharp');\nconst { FaHome } = require('react-icons/fa');\n\nasync function rasterizeIconPng(IconComponent, color, size = \"256\", filename) {\n  const svgString = ReactDOMServer.renderToStaticMarkup(\n    React.createElement(IconComponent, { color: `#${color}`, size: size })\n  );\n\n  // Convert SVG to PNG using Sharp\n  await sharp(Buffer.from(svgString))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Rasterize icon b\n... (truncated)\n```\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - pptx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "pptx",
          "skill_dir": "pptx",
          "skill_file_tree": "LICENSE.txt\nSKILL.md\nhtml2pptx.md\nooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd\nooxml/schemas/ISO-IEC29500-4_2016/pml.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd\nooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "additional_file_names": [
            "ooxml.md",
            "html2pptx.md"
          ]
        }
      },
      "results": []
    },
    {
      "id": "skill-test-Python Packaging (pyproject)",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 19,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Python Packaging (pyproject)\n\n**SKILL.md Content:**\n```markdown\n---\nname: Python Packaging (pyproject)\ndescription: Package Python projects correctly with pyproject.toml, build wheels, manage editable installs, and avoid common dependency/versioning pitfalls.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Python Packaging (pyproject)\n\n## Overview\n\nModern Python packaging centers on `pyproject.toml`. The common goals are:\n- Reproducible installs (dev vs CI)\n- Clear build backend configuration\n- Correct versioning and distribution artifacts (sdist/wheel)\n\n## Workflow\n\n1. Define project metadata in `pyproject.toml`:\n   - name, version, dependencies, optional extras\n2. Choose a build backend (e.g., setuptools, hatchling, poetry-core).\n3. For development:\n   - Use a virtualenv\n   - Install editable: `pip install -e .` (if supported by backend)\n4. For release:\n   - Build: `python -m build`\n   - Verify artifacts: install the wheel into a clean env and run imports/tests\n5. Pin dependencies appropriately:\n   - Use lockfiles for apps, more flexible ranges for libraries\n\n## Common Pitfalls\n\n- Confusing an app vs a library: apps can lock tightly; libraries should be compatible across versions.\n- Editable install surprises with PEP 660: backend must support it.\n- Import/package name mismatch: distribution name can differ from module import name.\n- Missing package data: ensure non-.py files are included in the build.\n\n## Checklist\n\n- [ ] `pyproject.toml` defines metadata and dependencies\n- [ ] Clean build produces wheel + sdist\n- [ ] Wheel installs and imports in a fresh environment\n- [ ] Versioning strategy is consistent (tags, changelog, semver)\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Python Packaging (pyproject)\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Python Packaging (pyproject)\n\n**SKILL.md Content:**\n```markdown\n---\nname: Python Packaging (pyproject)\ndescription: Package Python projects correctly with pyproject.toml, build wheels, manage editable installs, and avoid common dependency/versioning pitfalls.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Python Packaging (pyproject)\n\n## Overview\n\nModern Python packaging centers on `pyproject.toml`. The common goals are:\n- Reproducible installs (dev vs CI)\n- Clear build backend configuration\n- Correct versioning and distribution artifacts (sdist/wheel)\n\n## Workflow\n\n1. Define project metadata in `pyproject.toml`:\n   - name, version, dependencies, optional extras\n2. Choose a build backend (e.g., setuptools, hatchling, poetry-core).\n3. For development:\n   - Use a virtualenv\n   - Install editable: `pip install -e .` (if supported by backend)\n4. For release:\n   - Build: `python -m build`\n   - Verify artifacts: install the wheel into a clean env and run imports/tests\n5. Pin dependencies appropriately:\n   - Use lockfiles for apps, more flexible ranges for libraries\n\n## Common Pitfalls\n\n- Confusing an app vs a library: apps can lock tightly; libraries should be compatible across versions.\n- Editable install surprises with PEP 660: backend must support it.\n- Import/package name mismatch: distribution name can differ from module import name.\n- Missing package data: ensure non-.py files are included in the build.\n\n## Checklist\n\n- [ ] `pyproject.toml` defines metadata and dependencies\n- [ ] Clean build produces wheel + sdist\n- [ ] Wheel installs and imports in a fresh environment\n- [ ] Versioning strategy is consistent (tags, changelog, semver)\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Python Packaging (pyproject)\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Python Packaging (pyproject)",
          "skill_dir": "python-packaging",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-React Performance Tuning",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 20,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** React Performance Tuning\n\n**SKILL.md Content:**\n```markdown\n---\nname: React Performance Tuning\ndescription: Identify and fix common React performance issues (unnecessary re-renders, expensive computations, list rendering) using profiling and memoization patterns.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# React Performance Tuning\n\n## Overview\n\nReact slowness is usually caused by unnecessary renders, expensive render work, or rendering too much UI. The goal is to:\n- Reduce render frequency\n- Reduce work per render\n- Reduce the amount of DOM rendered\n\n## Workflow\n\n1. Reproduce the slowdown and capture a baseline (which interaction is slow).\n2. Profile:\n   - Use React DevTools Profiler to find components that render often or take long.\n3. Identify common causes:\n   - Parent state changes re-render large subtrees\n   - Inline object/array props break memoization\n   - Unstable callback props cause child re-renders\n   - Large lists without virtualization\n4. Apply fixes:\n   - `React.memo` for pure components\n   - `useMemo`/`useCallback` for stable props (only when it helps)\n   - Lift state appropriately or split components\n   - Virtualize big lists\n5. Re-profile and verify the change helped.\n\n## Common Pitfalls\n\n- Overusing memoization: it adds complexity and can make things worse.\n- Incorrect keys in lists: causes remounts and lost state.\n- Passing new objects each render: `style={{...}}`, `{}` in props.\n\n## Checklist\n\n- [ ] Profile before and after\n- [ ] Fix unstable props (objects/functions) when causing churn\n- [ ] Use correct list keys (stable IDs)\n- [ ] Consider virtualization for big lists\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - React Performance Tuning\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** React Performance Tuning\n\n**SKILL.md Content:**\n```markdown\n---\nname: React Performance Tuning\ndescription: Identify and fix common React performance issues (unnecessary re-renders, expensive computations, list rendering) using profiling and memoization patterns.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# React Performance Tuning\n\n## Overview\n\nReact slowness is usually caused by unnecessary renders, expensive render work, or rendering too much UI. The goal is to:\n- Reduce render frequency\n- Reduce work per render\n- Reduce the amount of DOM rendered\n\n## Workflow\n\n1. Reproduce the slowdown and capture a baseline (which interaction is slow).\n2. Profile:\n   - Use React DevTools Profiler to find components that render often or take long.\n3. Identify common causes:\n   - Parent state changes re-render large subtrees\n   - Inline object/array props break memoization\n   - Unstable callback props cause child re-renders\n   - Large lists without virtualization\n4. Apply fixes:\n   - `React.memo` for pure components\n   - `useMemo`/`useCallback` for stable props (only when it helps)\n   - Lift state appropriately or split components\n   - Virtualize big lists\n5. Re-profile and verify the change helped.\n\n## Common Pitfalls\n\n- Overusing memoization: it adds complexity and can make things worse.\n- Incorrect keys in lists: causes remounts and lost state.\n- Passing new objects each render: `style={{...}}`, `{}` in props.\n\n## Checklist\n\n- [ ] Profile before and after\n- [ ] Fix unstable props (objects/functions) when causing churn\n- [ ] Use correct list keys (stable IDs)\n- [ ] Consider virtualization for big lists\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - React Performance Tuning\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "React Performance Tuning",
          "skill_dir": "react-performance",
          "skill_file_tree": "SKILL.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-Skill Development",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 21,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Skill Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Skill Development\ndescription: This skill should be used when the user wants to \"create a skill\", \"add a skill to plugin\", \"write a new skill\", \"improve skill description\", \"organize skill content\", or needs guidance on skill structure, progressive disclosure, or skill development best practices for Claude Code plugins.\nversion: 0.1.0\n---\n\n# Skill Development for Claude Code Plugins\n\nThis skill provides guidance for creating effective skills for Claude Code plugins.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasks—they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n├── SKILL.md (required)\n│   ├── YAML frontmatter metadata (required)\n│   │   ├── name: (required)\n│   │   └── description: (required)\n│   └── Markdown instructions (required)\n└── Bundled Resources (optional)\n    ├── scripts/          - Executable code (Python/Bash/etc.)\n    ├── references/       - Documentation intended to be loaded into context as needed\n    └── assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\n**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. \"This skill should be used when...\" instead of \"Use this skill when...\").\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited*)\n\n*Unlimited because scripts can be executed without reading into context window.\n\n## Skill Creation Process\n\nTo create a skill, follow the \"Skill Creation Process\" in order, skipping steps only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\n**For Claude Code plugins:** When building a hooks skill, the analysis shows:\n1. Developers repeatedly need to validate hooks.json and test hook scripts\n2. `scripts/validate-hook-schema.sh` and `scripts/test-hook.sh` utilities would be helpful\n3. `references/patterns.md` for detailed hook patterns to avoid bloating SKILL.md\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Create Skill Structure\n\nFor Claude Code plugins, create the skill directory structure:\n\n```bash\nmkdir -p plugin-name/skills/skill-name/{references,examples,scripts}\ntouch plugin-name/skills/skill-name/SKILL.md\n```\n\n**Note:** Unlike the generic skill-creator which uses `init_skill.py`, plugin skills are created directly in the plugin's `skills/` directory with a simpler manual structure.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-created or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAlso, delete any example files and directories not needed for the skill. Create only the directories you actually need (references/, examples/, scripts/).\n\n#### Update SKILL.md\n\n**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., \"To accomplish X, do Y\" rather than \"You should do X\" or \"If you need to do X\"). This maintains consistency and clarity for AI consumption.\n\n**Description (Frontmatter):** Use third-person format with specific trigger phrases:\n\n```yaml\n---\nname: Skill Name\ndescription: This skill should be used when the user asks to \"specific phrase 1\", \"specific phrase 2\", \"specific phrase 3\". Include exact phrases users would say that should trigger this skill. Be concrete and specific.\nversion: 0.1.0\n---\n```\n\n**Good description examples:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", \"implement prompt-based hooks\", or mentions hook events (PreToolUse, PostToolUse, Stop).\n```\n\n**Bad description examples:**\n```yaml\ndescription: Use this skill when working with hooks.  # Wrong person, vague\ndescription: Load when user needs hook help.  # Not third person\ndescription: Provides hook guidance.  # No trigger phrases\n```\n\nTo complete SKILL.md body, answer the following questions:\n\n1. What is the purpose of the skill, in a few sentences?\n2. When should the skill be used? (Include this in frontmatter description with specific triggers)\n3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.\n\n**Keep SKILL.md lean:** Target 1,500-2,000 words for the body. Move detailed content to references/:\n- Detailed patterns → `references/patterns.md`\n- Advanced techniques → `references/advanced.md`\n- Migration guides → `references/migration.md`\n- API references → `references/api-reference.md`\n\n**Reference resources in SKILL.md:**\n```markdown\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and techniques, consult:\n- **`references/patterns.md`** - Common patterns\n- **`references/advanced.md`** - Advanced use cases\n\n### Example Files\n\nWorking examples in `examples/`:\n- **`example-script.sh`** - Working example\n```\n\n### Step 5: Validate and Test\n\n**For plugin skills, validation is different from generic skills:**\n\n1. **Check structure**: Skill directory in `plugin-name/skills/skill-name/`\n2. **Validate SKILL.md**: Has frontmatter with name and description\n3. **Check trigger phrases**: Description includes specific user queries\n4. **Verify writing style**: Body uses imperative/infinitive form, not second person\n5. **Test progressive disclosure**: SKILL.md is lean (~1,500-2,000 words), detailed content in references/\n6. **Check references**: All referenced files exist\n7. **Validate examples**: Examples are complete and correct\n8. **Test scripts**: Scripts are executable and work correctly\n\n**Use the skill-reviewer agent:**\n```\nAsk: \"Review my skill and check if it follows best practices\"\n```\n\nThe skill-reviewer agent will check description quality, content organization, and progressive disclosure.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n\n**Common improvements:**\n- Strengthen trigger phrases in description\n- Move long sections from SKILL.md to references/\n- Add missing examples or scripts\n- Clarify ambiguous instructions\n- Add edge case handling\n\n## Plugin-Specific Considerations\n\n### Skill Location in Plugins\n\nPlugin skills live in the plugin's `skills/` directory:\n\n```\nmy-plugin/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n├── agents/\n└── skills/\n    └── my-skill/\n        ├── SKILL.md\n        ├── references/\n        ├── examples/\n        └── scripts/\n```\n\n### Auto-Discovery\n\nClaude Code automatically discovers skills:\n- Scans `skills/` directory\n- Finds subdirectories containing `SKILL.md`\n- Loads skill metadata (name + description) always\n- Loads SKILL.md body when skill triggers\n- Loads references/examples when needed\n\n### No Packaging Needed\n\nPlugin skills are distributed as part of the plugin, not as separate ZIP files. Users get skills when they install the plugin.\n\n### Testing in Plugins\n\nTest skills by installing plugin locally:\n\n```bash\n# Test with --plugin-dir\ncc --plugin-dir /path/to/plugin\n\n# Ask questions that should trigger the skill\n# Verify skill loads correctly\n```\n\n## Examples from Plugin-Dev\n\nStudy the skills in this plugin as examples of best practices:\n\n**hook-development skill:**\n- Excellent trigger phrases: \"create a hook\", \"add a PreToolUse hook\", etc.\n- Lean SKILL.md (1,651 words)\n- 3 references/ files for detailed content\n- 3 examples/ of working hooks\n- 3 scripts/ utilities\n\n**agent-development skill:**\n- Strong triggers: \"create an agent\", \"agent frontmatter\", etc.\n- Focused SKILL.md (1,438 words)\n- References include the AI generation prompt from Claude Code\n- Complete agent examples\n\n**plugin-settings skill:**\n- Specific triggers: \"plugin settings\", \".local.md files\", \"YAML frontmatter\"\n- References show real implementations (multi-agent-swarm, ralph-wiggum)\n- Working parsing scripts\n\nEach demonstrates progressive disclosure and strong triggering.\n\n## Progressive Disclosure in Practice\n\n### What Goes in SKILL.md\n\n**Include (always loaded when skill triggers):**\n- Core concepts and overview\n- Essential procedures and workflows\n- Quick reference tables\n- Pointers to references/examples/scripts\n- Most common use cases\n\n**Keep under 3,000 words, ideally 1,500-2,000 words**\n\n### What Goes in references/\n\n**Move to references/ (loaded as needed):**\n- Detailed patterns and advanced techniques\n- Comprehensive API documentation\n- Migration guides\n- Edge cases and troubleshooting\n- Extensive examples and walkthroughs\n\n**Each reference file can be large (2,000-5,000+ words)**\n\n### What Goes in examples/\n\n**Working code examples:**\n- Complete, runnable scripts\n- Configuration files\n- Template files\n- Real-world usage examples\n\n**Users can copy and adapt these directly**\n\n### What Goes in scripts/\n\n**Utility scripts:**\n- Validation tools\n- Testing helpers\n- Parsing utilities\n- Automation scripts\n\n**Should be executable and documented**\n\n## Writing Style Requirements\n\n### Imperative/Infinitive Form\n\nWrite using verb-first instructions, not second person:\n\n**Correct (imperative):**\n```\nTo create a hook, define the event type.\nConfigure the MCP server with authentication.\nValidate settings before use.\n```\n\n**Incorrect (second person):**\n```\nYou should create a hook by defining the event type.\nYou need to configure the MCP server.\nYou must validate settings before use.\n```\n\n### Third-Person in Description\n\nThe frontmatter description must use third person:\n\n**Correct:**\n```yaml\ndescription: This skill should be used when the user asks to \"create X\", \"configure Y\"...\n```\n\n**Incorrect:**\n```yaml\ndescription: Use this skill when you want to create X...\ndescription: Load this skill when user asks...\n```\n\n### Objective, Instructional Language\n\nFocus on what to do, not who should do it:\n\n**Correct:**\n```\nParse the frontmatter using sed.\nExtract fields with grep.\nValidate values before use.\n```\n\n**Incorrect:**\n```\nYou can parse the frontmatter...\nClaude should extract fields...\nThe user might validate values...\n```\n\n## Validation Checklist\n\nBefore finalizing a skill:\n\n**Structure:**\n- [ ] SKILL.md file exists with valid YAML frontmatter\n- [ ] Frontmatter has `name` and `description` fields\n- [ ] Markdown body is present and substantial\n- [ ] Referenced files actually exist\n\n**Description Quality:**\n- [ ] Uses third person (\"This skill should be used when...\")\n- [ ] Includes specific trigger phrases users would say\n- [ ] Lists concrete scenarios (\"create X\", \"configure Y\")\n- [ ] Not vague or generic\n\n**Content Quality:**\n- [ ] SKILL.md body uses imperative/infinitive form\n- [ ] Body is focused and lean (1,500-2,000 words ideal, <5k max)\n- [ ] Detailed content moved to references/\n- [ ] Examples are complete and working\n- [ ] Scripts are executable and documented\n\n**Progressive Disclosure:**\n- [ ] Core concepts in SKILL.md\n- [ ] Detailed docs in references/\n- [ ] Working code in examples/\n- [ ] Utilities in scripts/\n- [ ] SKILL.md references these resources\n\n**Testing:**\n- [ ] Skill triggers on expected user queries\n- [ ] Content is helpful for intended tasks\n- [ ] No duplicated information across files\n- [ ] References load when needed\n\n## Common Mistakes to Avoid\n\n### Mistake 1: Weak Trigger Description\n\n❌ **Bad:**\n```yaml\ndescription: Provides guidance for working with hooks.\n```\n\n**Why bad:** Vague, no specific trigger phrases, not third person\n\n✅ **Good:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", or mentions hook events. Provides comprehensive hooks API guidance.\n```\n\n**Why good:** Third person, specific phrases, concrete scenarios\n\n### Mistake 2: Too Much in SKILL.md\n\n❌ **Bad:**\n```\nskill-name/\n└── SKILL.md  (8,000 words - everything in one file)\n```\n\n**Why bad:** Bloats context when skill loads, detailed content always loaded\n\n✅ **Good:**\n```\nskill-name/\n├── SKILL.md  (1,800 words - core essentials)\n└── references/\n    ├── patterns.md (2,500 words)\n    └── advanced.md (3,700 words)\n```\n\n**Why good:** Progressive disclosure, detailed content loaded only when needed\n\n### Mistake 3: Second Person Writing\n\n❌ **Bad:**\n```markdown\nYou should start by reading the configuration file.\nYou need to validate the input.\nYou can use the grep tool to search.\n```\n\n**Why bad:** Second person, not imperative form\n\n✅ **Good:**\n```markdown\nStart by reading the configuration file.\nValidate the input before processing.\nUse the grep tool to search for patterns.\n```\n\n**Why good:** Imperative form, direct instructions\n\n### Mistake 4: Missing Resource References\n\n❌ **Bad:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n[No mention of references/ or examples/]\n```\n\n**Why bad:** Claude doesn't know references exist\n\n✅ **Good:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n## Additional Resources\n\n### Reference Files\n- **`references/patterns.md`** - Detailed patterns\n- **`references/advanced.md`** - Advanced techniques\n\n### Examples\n- **`examples/script.sh`** - Working example\n```\n\n**Why good:** Claude knows where to find additional information\n\n## Quick Reference\n\n### Minimal Skill\n\n```\nskill-name/\n└── SKILL.md\n```\n\nGood for: Simple knowledge, no complex resources needed\n\n### Standard Skill (Recommended)\n\n```\nskill-name/\n├── SKILL.md\n├── references/\n│   └── detailed-guide.md\n└── examples/\n    └── working-example.sh\n```\n\nGood for: Most plugin skills with detailed documentation\n\n### Complete Skill\n\n```\nskill-name/\n├── SKILL.md\n├── references/\n│   ├── patterns.md\n│   └── advanced.md\n├── examples/\n│   ├── example1.sh\n│   └── example2.json\n└── scripts/\n    └── validate.sh\n```\n\nGood for: Complex domains with validation utilities\n\n## Best Practices Summary\n\n✅ **DO:**\n- Use third-person in description (\"This skill should be used when...\")\n- Include specific trigger phrases (\"create X\", \"configure Y\")\n- Keep SKILL.md lean (1,500-2,000 words)\n- Use progressive disclosure (move details to references/)\n- Write in imperative/infinitive form\n- Reference supporting files clearly\n- Provide working examples\n- Create utility scripts for common operations\n- Study plugin-dev's skills as templates\n\n❌ **DON'T:**\n- Use second person anywhere\n- Have vague trigger conditions\n- Put everything in SKILL.md (>3,000 words without references/)\n- Write in second person (\"You should...\")\n- Leave resources unreferenced\n- Include broken or incomplete examples\n- Skip validation\n\n## Additional Resources\n\n### Study These Skills\n\nPlugin-dev's skills demonstrate best practices:\n- `../hook-development/` - Progressive disclosure, utilities\n- `../agent-development/` - AI-assisted creation, references\n- `../mcp-integration/` - Comprehensive references\n- `../plugin-settings/` - Real-world examples\n- `../command-development/` - Clear critical concepts\n- `../plugin-structure/` - Good organization\n\n### Reference Files\n\nFor complete skill-creator methodology:\n- **`references/skill-creator-original.md`** - Full original skill-creator content\n\n## Implementation Workflow\n\nTo create a skill for your plugin:\n\n1. **Understand use cases**: Identify concrete examples of skill usage\n2. **Plan resources**: Determine what scripts/references/examples needed\n3. **Create structure**: `mkdir -p skills/skill-name/{references,examples,scripts}`\n4. **Write SKILL.md**:\n   - Frontmatter with third-person description and trigger phrases\n   - Lean body (1,500-2,000 words) in imperative form\n   - Reference supporting files\n5. **Add resources**: Create references/, examples/, scripts/ as needed\n6. **Validate**: Check description, writing style, organization\n7. **Test**: Verify skill loads on expected triggers\n8. **Iterate**: Improve based on usage\n\nFocus on strong trigger descriptions, progressive disclosure, and imperative writing style for effective skills that load when needed and provide targeted guidance.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Skill Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** Skill Development\n\n**SKILL.md Content:**\n```markdown\n---\nname: Skill Development\ndescription: This skill should be used when the user wants to \"create a skill\", \"add a skill to plugin\", \"write a new skill\", \"improve skill description\", \"organize skill content\", or needs guidance on skill structure, progressive disclosure, or skill development best practices for Claude Code plugins.\nversion: 0.1.0\n---\n\n# Skill Development for Claude Code Plugins\n\nThis skill provides guidance for creating effective skills for Claude Code plugins.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasks—they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n├── SKILL.md (required)\n│   ├── YAML frontmatter metadata (required)\n│   │   ├── name: (required)\n│   │   └── description: (required)\n│   └── Markdown instructions (required)\n└── Bundled Resources (optional)\n    ├── scripts/          - Executable code (Python/Bash/etc.)\n    ├── references/       - Documentation intended to be loaded into context as needed\n    └── assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\n**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. \"This skill should be used when...\" instead of \"Use this skill when...\").\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited*)\n\n*Unlimited because scripts can be executed without reading into context window.\n\n## Skill Creation Process\n\nTo create a skill, follow the \"Skill Creation Process\" in order, skipping steps only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\n**For Claude Code plugins:** When building a hooks skill, the analysis shows:\n1. Developers repeatedly need to validate hooks.json and test hook scripts\n2. `scripts/validate-hook-schema.sh` and `scripts/test-hook.sh` utilities would be helpful\n3. `references/patterns.md` for detailed hook patterns to avoid bloating SKILL.md\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Create Skill Structure\n\nFor Claude Code plugins, create the skill directory structure:\n\n```bash\nmkdir -p plugin-name/skills/skill-name/{references,examples,scripts}\ntouch plugin-name/skills/skill-name/SKILL.md\n```\n\n**Note:** Unlike the generic skill-creator which uses `init_skill.py`, plugin skills are created directly in the plugin's `skills/` directory with a simpler manual structure.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-created or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAlso, delete any example files and directories not needed for the skill. Create only the directories you actually need (references/, examples/, scripts/).\n\n#### Update SKILL.md\n\n**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., \"To accomplish X, do Y\" rather than \"You should do X\" or \"If you need to do X\"). This maintains consistency and clarity for AI consumption.\n\n**Description (Frontmatter):** Use third-person format with specific trigger phrases:\n\n```yaml\n---\nname: Skill Name\ndescription: This skill should be used when the user asks to \"specific phrase 1\", \"specific phrase 2\", \"specific phrase 3\". Include exact phrases users would say that should trigger this skill. Be concrete and specific.\nversion: 0.1.0\n---\n```\n\n**Good description examples:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", \"implement prompt-based hooks\", or mentions hook events (PreToolUse, PostToolUse, Stop).\n```\n\n**Bad description examples:**\n```yaml\ndescription: Use this skill when working with hooks.  # Wrong person, vague\ndescription: Load when user needs hook help.  # Not third person\ndescription: Provides hook guidance.  # No trigger phrases\n```\n\nTo complete SKILL.md body, answer the following questions:\n\n1. What is the purpose of the skill, in a few sentences?\n2. When should the skill be used? (Include this in frontmatter description with specific triggers)\n3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.\n\n**Keep SKILL.md lean:** Target 1,500-2,000 words for the body. Move detailed content to references/:\n- Detailed patterns → `references/patterns.md`\n- Advanced techniques → `references/advanced.md`\n- Migration guides → `references/migration.md`\n- API references → `references/api-reference.md`\n\n**Reference resources in SKILL.md:**\n```markdown\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and techniques, consult:\n- **`references/patterns.md`** - Common patterns\n- **`references/advanced.md`** - Advanced use cases\n\n### Example Files\n\nWorking examples in `examples/`:\n- **`example-script.sh`** - Working example\n```\n\n### Step 5: Validate and Test\n\n**For plugin skills, validation is different from generic skills:**\n\n1. **Check structure**: Skill directory in `plugin-name/skills/skill-name/`\n2. **Validate SKILL.md**: Has frontmatter with name and description\n3. **Check trigger phrases**: Description includes specific user queries\n4. **Verify writing style**: Body uses imperative/infinitive form, not second person\n5. **Test progressive disclosure**: SKILL.md is lean (~1,500-2,000 words), detailed content in references/\n6. **Check references**: All referenced files exist\n7. **Validate examples**: Examples are complete and correct\n8. **Test scripts**: Scripts are executable and work correctly\n\n**Use the skill-reviewer agent:**\n```\nAsk: \"Review my skill and check if it follows best practices\"\n```\n\nThe skill-reviewer agent will check description quality, content organization, and progressive disclosure.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n\n**Common improvements:**\n- Strengthen trigger phrases in description\n- Move long sections from SKILL.md to references/\n- Add missing examples or scripts\n- Clarify ambiguous instructions\n- Add edge case handling\n\n## Plugin-Specific Considerations\n\n### Skill Location in Plugins\n\nPlugin skills live in the plugin's `skills/` directory:\n\n```\nmy-plugin/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n├── agents/\n└── skills/\n    └── my-skill/\n        ├── SKILL.md\n        ├── references/\n        ├── examples/\n        └── scripts/\n```\n\n### Auto-Discovery\n\nClaude Code automatically discovers skills:\n- Scans `skills/` directory\n- Finds subdirectories containing `SKILL.md`\n- Loads skill metadata (name + description) always\n- Loads SKILL.md body when skill triggers\n- Loads references/examples when needed\n\n### No Packaging Needed\n\nPlugin skills are distributed as part of the plugin, not as separate ZIP files. Users get skills when they install the plugin.\n\n### Testing in Plugins\n\nTest skills by installing plugin locally:\n\n```bash\n# Test with --plugin-dir\ncc --plugin-dir /path/to/plugin\n\n# Ask questions that should trigger the skill\n# Verify skill loads correctly\n```\n\n## Examples from Plugin-Dev\n\nStudy the skills in this plugin as examples of best practices:\n\n**hook-development skill:**\n- Excellent trigger phrases: \"create a hook\", \"add a PreToolUse hook\", etc.\n- Lean SKILL.md (1,651 words)\n- 3 references/ files for detailed content\n- 3 examples/ of working hooks\n- 3 scripts/ utilities\n\n**agent-development skill:**\n- Strong triggers: \"create an agent\", \"agent frontmatter\", etc.\n- Focused SKILL.md (1,438 words)\n- References include the AI generation prompt from Claude Code\n- Complete agent examples\n\n**plugin-settings skill:**\n- Specific triggers: \"plugin settings\", \".local.md files\", \"YAML frontmatter\"\n- References show real implementations (multi-agent-swarm, ralph-wiggum)\n- Working parsing scripts\n\nEach demonstrates progressive disclosure and strong triggering.\n\n## Progressive Disclosure in Practice\n\n### What Goes in SKILL.md\n\n**Include (always loaded when skill triggers):**\n- Core concepts and overview\n- Essential procedures and workflows\n- Quick reference tables\n- Pointers to references/examples/scripts\n- Most common use cases\n\n**Keep under 3,000 words, ideally 1,500-2,000 words**\n\n### What Goes in references/\n\n**Move to references/ (loaded as needed):**\n- Detailed patterns and advanced techniques\n- Comprehensive API documentation\n- Migration guides\n- Edge cases and troubleshooting\n- Extensive examples and walkthroughs\n\n**Each reference file can be large (2,000-5,000+ words)**\n\n### What Goes in examples/\n\n**Working code examples:**\n- Complete, runnable scripts\n- Configuration files\n- Template files\n- Real-world usage examples\n\n**Users can copy and adapt these directly**\n\n### What Goes in scripts/\n\n**Utility scripts:**\n- Validation tools\n- Testing helpers\n- Parsing utilities\n- Automation scripts\n\n**Should be executable and documented**\n\n## Writing Style Requirements\n\n### Imperative/Infinitive Form\n\nWrite using verb-first instructions, not second person:\n\n**Correct (imperative):**\n```\nTo create a hook, define the event type.\nConfigure the MCP server with authentication.\nValidate settings before use.\n```\n\n**Incorrect (second person):**\n```\nYou should create a hook by defining the event type.\nYou need to configure the MCP server.\nYou must validate settings before use.\n```\n\n### Third-Person in Description\n\nThe frontmatter description must use third person:\n\n**Correct:**\n```yaml\ndescription: This skill should be used when the user asks to \"create X\", \"configure Y\"...\n```\n\n**Incorrect:**\n```yaml\ndescription: Use this skill when you want to create X...\ndescription: Load this skill when user asks...\n```\n\n### Objective, Instructional Language\n\nFocus on what to do, not who should do it:\n\n**Correct:**\n```\nParse the frontmatter using sed.\nExtract fields with grep.\nValidate values before use.\n```\n\n**Incorrect:**\n```\nYou can parse the frontmatter...\nClaude should extract fields...\nThe user might validate values...\n```\n\n## Validation Checklist\n\nBefore finalizing a skill:\n\n**Structure:**\n- [ ] SKILL.md file exists with valid YAML frontmatter\n- [ ] Frontmatter has `name` and `description` fields\n- [ ] Markdown body is present and substantial\n- [ ] Referenced files actually exist\n\n**Description Quality:**\n- [ ] Uses third person (\"This skill should be used when...\")\n- [ ] Includes specific trigger phrases users would say\n- [ ] Lists concrete scenarios (\"create X\", \"configure Y\")\n- [ ] Not vague or generic\n\n**Content Quality:**\n- [ ] SKILL.md body uses imperative/infinitive form\n- [ ] Body is focused and lean (1,500-2,000 words ideal, <5k max)\n- [ ] Detailed content moved to references/\n- [ ] Examples are complete and working\n- [ ] Scripts are executable and documented\n\n**Progressive Disclosure:**\n- [ ] Core concepts in SKILL.md\n- [ ] Detailed docs in references/\n- [ ] Working code in examples/\n- [ ] Utilities in scripts/\n- [ ] SKILL.md references these resources\n\n**Testing:**\n- [ ] Skill triggers on expected user queries\n- [ ] Content is helpful for intended tasks\n- [ ] No duplicated information across files\n- [ ] References load when needed\n\n## Common Mistakes to Avoid\n\n### Mistake 1: Weak Trigger Description\n\n❌ **Bad:**\n```yaml\ndescription: Provides guidance for working with hooks.\n```\n\n**Why bad:** Vague, no specific trigger phrases, not third person\n\n✅ **Good:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", or mentions hook events. Provides comprehensive hooks API guidance.\n```\n\n**Why good:** Third person, specific phrases, concrete scenarios\n\n### Mistake 2: Too Much in SKILL.md\n\n❌ **Bad:**\n```\nskill-name/\n└── SKILL.md  (8,000 words - everything in one file)\n```\n\n**Why bad:** Bloats context when skill loads, detailed content always loaded\n\n✅ **Good:**\n```\nskill-name/\n├── SKILL.md  (1,800 words - core essentials)\n└── references/\n    ├── patterns.md (2,500 words)\n    └── advanced.md (3,700 words)\n```\n\n**Why good:** Progressive disclosure, detailed content loaded only when needed\n\n### Mistake 3: Second Person Writing\n\n❌ **Bad:**\n```markdown\nYou should start by reading the configuration file.\nYou need to validate the input.\nYou can use the grep tool to search.\n```\n\n**Why bad:** Second person, not imperative form\n\n✅ **Good:**\n```markdown\nStart by reading the configuration file.\nValidate the input before processing.\nUse the grep tool to search for patterns.\n```\n\n**Why good:** Imperative form, direct instructions\n\n### Mistake 4: Missing Resource References\n\n❌ **Bad:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n[No mention of references/ or examples/]\n```\n\n**Why bad:** Claude doesn't know references exist\n\n✅ **Good:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n## Additional Resources\n\n### Reference Files\n- **`references/patterns.md`** - Detailed patterns\n- **`references/advanced.md`** - Advanced techniques\n\n### Examples\n- **`examples/script.sh`** - Working example\n```\n\n**Why good:** Claude knows where to find additional information\n\n## Quick Reference\n\n### Minimal Skill\n\n```\nskill-name/\n└── SKILL.md\n```\n\nGood for: Simple knowledge, no complex resources needed\n\n### Standard Skill (Recommended)\n\n```\nskill-name/\n├── SKILL.md\n├── references/\n│   └── detailed-guide.md\n└── examples/\n    └── working-example.sh\n```\n\nGood for: Most plugin skills with detailed documentation\n\n### Complete Skill\n\n```\nskill-name/\n├── SKILL.md\n├── references/\n│   ├── patterns.md\n│   └── advanced.md\n├── examples/\n│   ├── example1.sh\n│   └── example2.json\n└── scripts/\n    └── validate.sh\n```\n\nGood for: Complex domains with validation utilities\n\n## Best Practices Summary\n\n✅ **DO:**\n- Use third-person in description (\"This skill should be used when...\")\n- Include specific trigger phrases (\"create X\", \"configure Y\")\n- Keep SKILL.md lean (1,500-2,000 words)\n- Use progressive disclosure (move details to references/)\n- Write in imperative/infinitive form\n- Reference supporting files clearly\n- Provide working examples\n- Create utility scripts for common operations\n- Study plugin-dev's skills as templates\n\n❌ **DON'T:**\n- Use second person anywhere\n- Have vague trigger conditions\n- Put everything in SKILL.md (>3,000 words without references/)\n- Write in second person (\"You should...\")\n- Leave resources unreferenced\n- Include broken or incomplete examples\n- Skip validation\n\n## Additional Resources\n\n### Study These Skills\n\nPlugin-dev's skills demonstrate best practices:\n- `../hook-development/` - Progressive disclosure, utilities\n- `../agent-development/` - AI-assisted creation, references\n- `../mcp-integration/` - Comprehensive references\n- `../plugin-settings/` - Real-world examples\n- `../command-development/` - Clear critical concepts\n- `../plugin-structure/` - Good organization\n\n### Reference Files\n\nFor complete skill-creator methodology:\n- **`references/skill-creator-original.md`** - Full original skill-creator content\n\n## Implementation Workflow\n\nTo create a skill for your plugin:\n\n1. **Understand use cases**: Identify concrete examples of skill usage\n2. **Plan resources**: Determine what scripts/references/examples needed\n3. **Create structure**: `mkdir -p skills/skill-name/{references,examples,scripts}`\n4. **Write SKILL.md**:\n   - Frontmatter with third-person description and trigger phrases\n   - Lean body (1,500-2,000 words) in imperative form\n   - Reference supporting files\n5. **Add resources**: Create references/, examples/, scripts/ as needed\n6. **Validate**: Check description, writing style, organization\n7. **Test**: Verify skill loads on expected triggers\n8. **Iterate**: Improve based on usage\n\nFocus on strong trigger descriptions, progressive disclosure, and imperative writing style for effective skills that load when needed and provide targeted guidance.\n\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - Skill Development\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "Skill Development",
          "skill_dir": "skill-development",
          "skill_file_tree": "SKILL.md\nreferences/skill-creator-original.md",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-webapp-testing",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 22,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** webapp-testing\n\n**SKILL.md Content:**\n```markdown\n---\nname: webapp-testing\ndescription: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task → Is it static HTML?\n    ├─ Yes → Read HTML file directly to identify selectors\n    │         ├─ Success → Write Playwright script using selectors\n    │         └─ Fails/Incomplete → Treat as dynamic (below)\n    │\n    └─ No (dynamic webapp) → Is the server already running?\n        ├─ No → Run: python scripts/with_server.py --help\n        │        Then use the helper + write simplified Playwright script\n        │\n        └─ Yes → Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n❌ **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n✅ **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - webapp-testing\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** webapp-testing\n\n**SKILL.md Content:**\n```markdown\n---\nname: webapp-testing\ndescription: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task → Is it static HTML?\n    ├─ Yes → Read HTML file directly to identify selectors\n    │         ├─ Success → Write Playwright script using selectors\n    │         └─ Fails/Incomplete → Treat as dynamic (below)\n    │\n    └─ No (dynamic webapp) → Is the server already running?\n        ├─ No → Run: python scripts/with_server.py --help\n        │        Then use the helper + write simplified Playwright script\n        │\n        └─ Yes → Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n❌ **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n✅ **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - webapp-testing\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "webapp-testing",
          "skill_dir": "webapp-testing",
          "skill_file_tree": "LICENSE.txt\nSKILL.md\nexamples/console_logging.py\nexamples/element_discovery.py\nexamples/static_html_automation.py\nscripts/with_server.py",
          "additional_file_names": []
        }
      },
      "results": []
    },
    {
      "id": "skill-test-xlsx",
      "dataset": "dataset_all",
      "source": "skill-test-writing/data/dataset_all.jsonl",
      "line": 23,
      "input_turns": [
        {
          "role": "input",
          "content": "You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** xlsx\n\n**SKILL.md Content:**\n```markdown\n---\nname: xlsx\ndescription: \"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n### ❌ WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n### ✅ CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - xlsx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```\n"
        }
      ],
      "ground_truth_turns": [],
      "input_text": "input: You are an evaluation designer. You have a skill document that teaches an LLM domain-specific knowledge. Your job is to write ONE test case that distinguishes a knowledgeable practitioner from a novice in this domain.\n\n## The Skill\n\n**Skill Name:** xlsx\n\n**SKILL.md Content:**\n```markdown\n---\nname: xlsx\ndescription: \"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n### ❌ WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n### ✅ CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections\n```\n\n\n\n## What Makes a Great Test Case\n\nA great test case captures a **moment where domain expertise changes the outcome** -- a decision point, a subtle pitfall, or a design tradeoff that practitioners learn from experience. The test should feel like a real question someone would ask a senior colleague, not a quiz about documentation.\n\n### The Transferability Principle\n\nAsk yourself: \"Would a domain expert who has NEVER read this specific skill document still get this right?\" If yes, you are testing genuine domain knowledge. If no, you are testing document memorization.\n\n**Transferable knowledge** (test this):\n- Architectural patterns and tradeoffs (e.g., \"formulas vs hardcoded values in spreadsheets\")\n- Common pitfalls and debugging patterns (e.g., \"race condition when server isn't ready\")\n- When to use which tool/approach for a given problem (e.g., \"library X for tables, library Y for merging\")\n- Destructive operations and their consequences (e.g., \"opening with data_only=True then saving destroys formulas\")\n\n**Skill-document trivia** (do NOT test this):\n- Exact script names, file paths, or CLI flags specific to this skill's tooling\n- Magic numbers or specific configuration values mentioned in the skill\n- The exact workflow steps described in the skill document\n- Which fields are required in the skill's configuration format\n\n### Prompt Style: Scenario-Based\n\nWrite the prompt as a realistic situation -- a practitioner encountering a problem, making a decision, or asking for guidance. Frame it as something someone would type into a chat with a knowledgeable colleague.\n\n**Strong prompt patterns:**\n- \"I'm doing X and getting error Y. What's going wrong?\" (debugging scenario)\n- \"I need to accomplish X. Should I use approach A or approach B?\" (design decision)\n- \"I'm about to do X. Are there any risks I should know about?\" (pitfall awareness)\n- \"My X works in development but fails in production. Why?\" (environment difference)\n\n**Weak prompt patterns to avoid:**\n- \"What is the correct workflow for X?\" (asks to recite steps from docs)\n- \"What fields/parameters are required for X?\" (asks to list facts)\n- \"How often should I do X?\" (asks for a magic number)\n- \"What does tool X do?\" (asks for a description, not application)\n\n## Grader Design: Grade Concepts, Not Strings\n\nThe grader evaluates whether the response demonstrates **understanding of the underlying concept**, not whether it mentions specific terms.\n\n### Grader Anti-Patterns (avoid these):\n- Requiring a specific library name when alternatives exist (e.g., only accepting \"openpyxl\" when \"xlsxwriter\" also works)\n- Requiring a specific numeric value (e.g., \"must say 30 minutes\") when the concept matters more\n- Checking for exact phrases or keywords rather than conceptual understanding\n- Requiring mention of skill-specific script names or tools\n- Only accepting one valid approach when multiple exist\n\n### Grader Best Practices:\n- Grade on whether the response identifies the **core concept or pitfall**\n- Accept multiple valid solutions to the same problem\n- Use graduated scoring (0, 0.5, 1.0) based on depth of understanding\n- Focus the 1.0 criteria on explaining the \"why\" not just the \"what\"\n\n## Output Format\n\nOutput your test.yaml content directly in a YAML code block:\n\n```yaml\nname: descriptive-kebab-case-name\nskills:\n  - xlsx\nprompt: |\n  A scenario-based prompt that a practitioner would realistically encounter.\n  Should test transferable domain knowledge, not skill-document trivia.\ntimeout: 120\ngrader:\n  kind: letta_judge\n  prompt: |\n    Evaluate whether the response demonstrates understanding of [core concept].\n\n    The key insight is: [what a knowledgeable practitioner would know]\n\n    Score 1.0 if: Explains [concept] AND why it matters. Accepts any valid approach.\n    Score 0.5 if: Identifies the issue but explanation is incomplete or misses the \"why\".\n    Score 0.0 if: Does not address [concept] or gives incorrect guidance.\n  extractor: last_assistant\n```",
      "ground_truth_text": "",
      "meta": {
        "rubric_vars": {
          "skill_name": "xlsx",
          "skill_dir": "xlsx",
          "skill_file_tree": "LICENSE.txt\nSKILL.md\nrecalc.py",
          "additional_file_names": []
        }
      },
      "results": []
    }
  ],
  "models": [
    {
      "model_name": "claude-haiku-4-5-20251001",
      "avg_score": 0.8271,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "claude-opus-4-5-20251101",
      "avg_score": 0.8269,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "claude-sonnet-4-5-20250929",
      "avg_score": 0.8269,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "gemini-3-flash-preview",
      "avg_score": 0.8115,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "claude-opus-4-1-20250805",
      "avg_score": 0.8038,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "gpt-5.2-2025-12-11",
      "avg_score": 0.8026,
      "total": 39,
      "attempted": 39
    },
    {
      "model_name": "gemini-3-pro-preview",
      "avg_score": 0.7385,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "gpt-5.1-codex",
      "avg_score": 0.7385,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "minimax/minimax-m2",
      "avg_score": 0.6846,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "gpt-5.1-codex-mini",
      "avg_score": 0.6808,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "z-ai/glm-4.6",
      "avg_score": 0.65,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
      "avg_score": 0.4577,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
      "avg_score": 0.3462,
      "total": 13,
      "attempted": 13
    },
    {
      "model_name": "deepseek-chat",
      "avg_score": 0.0,
      "total": 13,
      "attempted": 0
    },
    {
      "model_name": "deepseek-reasoner",
      "avg_score": 0.0,
      "total": 13,
      "attempted": 0
    },
    {
      "model_name": "deepseek/deepseek-chat-v3.1",
      "avg_score": 0.0,
      "total": 13,
      "attempted": 0
    },
    {
      "model_name": "mistralai/mistral-large-2512",
      "avg_score": 0.0,
      "total": 13,
      "attempted": 0
    }
  ],
  "runs": [
    {
      "name": "cloud-v3",
      "path": "skill-test-writing/results/cloud-v3"
    }
  ],
  "errors": [],
  "rubric": {
    "path": "skill-test-writing/judge_prompt.md",
    "content": "## Task\n\nEvaluate a test case for the **{skill_name}** skill. A good test measures whether skill knowledge actually helps; a bad test is trivia or has a broken grader.\n\n## Skill Files\n\n```\n{skill_file_tree}\n```\n\n## Model Output\n\n{model_output}\n\n## Scoring (1-10 each)\n\n### Non-Obviousness\n\nDoes this test transferable domain knowledge, not skill-specific trivia?\n\n| Score | Criteria |\n|-------|----------|\n| 8-10 | Tests a genuine gotcha/pattern that transfers across contexts |\n| 5-7 | Tests useful knowledge; specific details are incidental |\n| 3-4 | Could be general but overly focused on specific strings/names |\n| 1-2 | Pure trivia: exact script names, magic numbers, memorization |\n\n**Score 1-3 if:** Test requires knowing arbitrary identifiers (e.g., \"thumbnail.py\", \"cais/mmlu\", \"rebuild every 30 minutes\") rather than underlying concepts.\n\n### Clarity\n\nIs the grader robust? Does it accept all valid solutions?\n\n| Score | Criteria |\n|-------|----------|\n| 8-10 | Tests actual capability, accepts multiple valid approaches |\n| 5-7 | Mostly robust, minor edge cases |\n| 3-4 | Significant brittleness or missing requirements |\n| 1-2 | Fundamentally broken grader |\n\n**Score 1-3 if ANY apply:**\n\n- Regex/string matching that rejects valid solutions\n- Only accepts one approach when alternatives exist (e.g., only openpyxl, not xlsxwriter)\n- Requires files/servers/setup not provided\n- Asks to \"run\" code that would fail\n\n### Realism\n\nWould a practitioner actually do this task?\n\n| Score | Criteria |\n|-------|----------|\n| 8-10 | Common real-world task |\n| 5-7 | Realistic scenario |\n| 3-4 | Plausible but contrived |\n| 1-2 | Artificial/synthetic task |\n\n**Score 1-3 if:** Task exists only to test the skill, not because anyone would actually need to do it (e.g., \"list all frontmatter fields\" vs \"create a command for X\").\n\n## Output\n\nJSON only. Keep rationale to 1-2 sentences.\n\n```json\n{{\n  \"rationale\": \"<1-2 sentences>\",\n  \"non_obviousness\": <1-10>,\n  \"clarity\": <1-10>,\n  \"realism\": <1-10>\n}}\n```\n"
  }
}
