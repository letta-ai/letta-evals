{
  "category": "tool-exploration",
  "display_name": "Tool Exploration",
  "generated_at": "2026-02-04T04:34:32.136878+00:00",
  "datasets": [
    {
      "name": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "count": 6
    }
  ],
  "scenarios": [
    {
      "id": "explore-001",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 1,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"Product Search\",\n  \"quirks\": [\n    {\n      \"title\": \"Limit cap lower than advertised\",\n      \"expected\": \"limit accepts up to 100 results per request\",\n      \"actual\": \"requests return at most 50 items even when limit=100; total_count still reports the full matches\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Batch pagination with limit=50 and deduplicate if you expect duplicates anyway\"\n    },\n    {\n      \"title\": \"Offset pagination race duplication\",\n      \"expected\": \"paging through offset=0,10,20 should give unique results for a static snapshot\",\n      \"actual\": \"in-flight product insertions can cause the same item to appear in two pages (e.g., position 10 on first page and position 1 on the next)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"client-side deduplication by product ID when merging pages\"\n    },\n    {\n      \"title\": \"Eventually consistent total_count\",\n      \"expected\": \"total_count equals the current exact number of matching products for each request\",\n      \"actual\": \"total_count can differ slightly between rapid repeated requests (e.g., 847 vs 843) even though database has 847 matches\",\n      \"severity\": \"low\",\n      \"workaround\": \"treat total_count as best-effort and rely on actual results for confirmation\"\n    },\n    {\n      \"title\": \"Soft-deletion propagation lag\",\n      \"expected\": \"deleted products disappear from results immediately unless include_deleted=true\",\n      \"actual\": \"recently deleted entries (within ~30s) still surface in queries without include_deleted, so they reappear before propagation completes\",\n      \"severity\": \"medium\",\n      \"workaround\": \"wait a short interval before verifying via search, or use include_deleted=true when confirming removals\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"GET /v2/products query parameter limit\",\n      \"correction\": \"Actual supported max is 50, not 100.\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Sorting by rating\",\n      \"details\": \"Passing sort=rating returns products ordered by average user rating even though the documented sort options omit this value.\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"description\": \"401 Unauthorized errors resolved by refreshing the bearer token after the documented 24-hour expiration window; not an API bug.\"\n    }\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are met. 1) The submission is a single valid JSON object containing the required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types and no extra text. 2) The quirks list includes the same real behavioral issues as the ground truth (offset pagination duplicates, eventual consistency of total_count, and deleted-product propagation lag); the submission also notes the limit cap behavior (expressed as a quirk) which is a real observed behavior and does not contradict the ground truth. 3) The documented error about the limit max (docs claim 100; actual 50) is correctly identified with an appropriate location and correction, and the hidden feature (sort=rating) is correctly reported. 4) The false alarm (401s due to expired bearer tokens) is correctly classified. Minor differences in phrasing, severity labels, and representation formats (e.g., hidden_features as an object rather than a plain string, category value wording) do not change the substance, so full credit is warranted."
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"CatalogAPI v2.3 - Product Search Endpoint\",\n  \"category\": \"E-commerce Search API\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset Pagination Race Condition\",\n      \"expected\": \"Stable pagination where each product is seen exactly once across sequential pages.\",\n      \"actual\": \"New product insertions between requests shift result positions, causing products to appear twice across page boundaries.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Implement client-side deduplication using unique product IDs.\"\n    },\n    {\n      \"title\": \"Eventually Consistent total_count\",\n      \"expected\": \"Exact number of matching products at the time of the query.\",\n      \"actual\": \"The count lags behind the actual database state, occasionally returning lower values than the real matching set.\",\n      \"severity\": \"Low\",\n      \"workaround\": \"Treat total_count as an approximate value or implement retry logic for high-precision needs.\"\n    },\n    {\n      \"title\": \"Search Index Deletion Lag\",\n      \"expected\": \"Products not matching 'include_deleted=true' should disappear from results immediately upon deletion.\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results without the deleted flag.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Verify product status on the client side or account for a 30-second delay in indexing.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Query parameters -> limit\",\n      \"correction\": \"The maximum allowed limit is 50. Values provided between 51 and 100 are ignored and capped at 50 results.\"\n    },\n    {\n      \"location\": \"Response -> total_count description\",\n      \"correction\": \"The total_count is eventually consistent rather than reflecting the exact number of products at query time.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Support for 'sort=rating' query parameter, which returns products sorted by average user rating.\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors encountered after long testing sessions are due to the documented 24-hour token expiration policy, not an API malfunction.\"\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are satisfied. 1) The submission is a single valid JSON object and includes all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types. 2) The quirks list correctly captures the same three real behavioral issues from the ground truth (offset pagination duplicates, eventually-consistent total_count, and deletion/indexing lag) — differences in wording and severity labels are minor and acceptable. 3) Documentation errors and hidden features are correctly identified: the limit max being 50 is reported with the correct location/correction, and the undocumented sort=rating hidden feature is present. (The submission also notes total_count eventual consistency as a doc correction; while the ground truth listed that as a quirk, the agent's note is accurate and doesn't contradict the expected answer.) 4) The false alarm is correctly classified: 401 errors were due to expired bearer tokens, matching the ground truth. Minor differences in phrasing and an extra doc_error do not conflict with the ground truth, so the submission receives full credit."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "{\n  \"tool_name\": \"CatalogAPI v2.3 Product Search Endpoint\",\n  \"category\": \"API\",\n  \"quirks\": [\n    {\n      \"title\": \"Maximum limit is 50, not 100 as documented\",\n      \"expected\": \"Setting limit=100 should return up to 100 results per page as stated in documentation\",\n      \"actual\": \"Any limit value above 50 returns only 50 results, regardless of how many products match\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Use limit=50 and adjust pagination logic to account for the lower maximum\"\n    },\n    {\n      \"title\": \"Offset pagination causes duplicate results when data changes\",\n      \"expected\": \"Each product should appear exactly once when paginating through results\",\n      \"actual\": \"Products inserted between pagination requests can cause the same product to appear on multiple pages\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Implement client-side deduplication using product IDs\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count should reflect the exact number of matching products at query time\",\n      \"actual\": \"total_count can be stale and lag behind the actual database state by several seconds\",\n      \"severity\": \"low\",\n      \"workaround\": \"Do not rely on total_count for real-time accuracy in critical operations\"\n    },\n    {\n      \"title\": \"Recently deleted products still appear in search results\",\n      \"expected\": \"Products deleted without include_deleted=true should not appear in results\",\n      \"actual\": \"Products deleted within approximately 30 seconds still appear in search results due to deletion index propagation delay\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Wait 30+ seconds after deletion before confirming removal via search, or use direct product ID lookup for immediate verification\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Query parameter 'limit' - states 'max 100'\",\n      \"correction\": \"Actual maximum is 50, not 100\"\n    },\n    {\n      \"location\": \"Response description for 'total_count' - states 'reflects the exact number of matching products at query time'\",\n      \"correction\": \"total_count is eventually consistent and may lag behind actual database state by several seconds\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"feature\": \"sort=rating parameter\",\n      \"description\": \"Undocumented sort option that returns products sorted by average user rating. Works correctly but is not listed in the documented sort options.\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"issue\": \"401 Unauthorized errors after extended testing\",\n      \"explanation\": \"Tokens expire after 24 hours as documented. This is expected behavior, not a bug. Developers need to refresh tokens regularly during long testing sessions.\"\n    }\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are met.\n\n1) Valid JSON: The submission is a single JSON object with all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) and appropriate types (strings and arrays). No extra text or code fences are present.\n\n2) Correct quirks: The submission captures the same behavioral issues as the ground truth: offset pagination duplicates, eventual consistency of total_count, and recently deleted products reappearing. The agent also included the limit-at-50 behavior as a quirk (a real observed behavior); substance and workarounds match the exploration transcript.\n\n3) Correct doc errors and hidden features: The submission correctly identifies the documentation error about the limit (max 100 → actual max 50). The undocumented sort=rating hidden feature is reported and described, matching the ground truth. (The submission also notes total_count's inconsistency as a doc-related correction; this is consistent with the transcript and does not contradict the ground truth.)\n\n4) Correct false alarms: The submission correctly classifies the 401 Unauthorized occurrences as expired tokens / expected behavior, matching the ground truth.\n\nMinor differences in phrasing and severity labels do not affect correctness; the reported items align in substance with the expected answer."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-002",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 2,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"dply v1.4\",\n  \"category\": \"Command-Line Data Processor\",\n  \"quirks\": [\n    {\n      \"title\": \"JSON Output Format Mismatch\",\n      \"expected\": \"A single JSON array of objects.\",\n      \"actual\": \"Outputs one JSON object per line (JSONL).\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Pipe the output through jq -s '.' to wrap objects into a proper JSON array.\"\n    },\n    {\n      \"title\": \"In-Place Overwrite Data Loss\",\n      \"expected\": \"Read input data then write to output.\",\n      \"actual\": \"Truncates the input file before reading if the input and output paths are identical.\",\n      \"severity\": \"High\",\n      \"workaround\": \"Never use the same file path for input and output; write to a temporary file first.\"\n    },\n    {\n      \"title\": \"Dry-Run Temporary File Leak\",\n      \"expected\": \"No side effects on the filesystem.\",\n      \"actual\": \"Creates temporary files in /tmp that are not cleaned up after execution.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Manually monitor and clear /tmp of dply-related temporary files after use.\"\n    },\n    {\n      \"title\": \"Locale-Insensitive Date Parsing\",\n      \"expected\": \"Date parsing respects the system locale (e.g., DD/MM/YYYY for en_GB).\",\n      \"actual\": \"Defaults to MM/DD/YYYY regardless of system locale settings.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Explicitly define the date format using the --date-format flag.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Notes: JSON output produces a single JSON array of objects.\",\n      \"correction\": \"JSON output produces one JSON object per line (JSONL format).\"\n    },\n    {\n      \"location\": \"Options: --date-format <f> Date parsing format (default: system locale)\",\n      \"correction\": \"The default date parsing format is MM/DD/YYYY, regardless of system locale.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Passing non-existent column names to --select will create new columns filled with null values rather than throwing an error.\"\n  ],\n  \"false_alarms\": [\n    \"Extreme slowness and high memory usage when processing files larger than available RAM is a result of the tool's in-memory processing architecture, not a software bug.\"\n  ]\n}",
          "score": 0.75,
          "rationale": "I awarded 0.75 because the submission satisfies three of the four rubric criteria. Criterion 1 (Valid JSON only): PASSED — the submission is a single valid JSON object containing all required keys with appropriate types. Criterion 2 (Correct quirks): PASSED — the submission lists the same core behavioral quirks as the ground truth (overwrite truncation, dry-run temp-file leak, and locale-insensitive date parsing). It also added the JSON vs JSONL issue as a quirk; that issue is real (not a false alarm), so this does not invalidate the quirks list. Criterion 3 (Correct doc errors and hidden features): FAILED — the submission correctly reports the JSON vs JSONL doc error and the date-format/doc mismatch, and it documents the hidden --select behavior, but it omits the doc error about --dry-run documentation claiming 'without creating any files' (the ground truth lists that as a documentation error). Because one required doc_error is missing, this criterion is not satisfied. Criterion 4 (Correct false alarms): PASSED — the submission correctly classifies the slow processing of a 12GB file on 8GB RAM as expected behavior (an in-memory tool limitation) rather than a bug. Overall score: 3/4 criteria satisfied = 0.75."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"data processing\",\n  \"quirks\": [\n    {\n      \"title\": \"JSON format outputs JSONL instead of JSON array\",\n      \"expected\": \"Single JSON array of objects as documented\",\n      \"actual\": \"Outputs one JSON object per line (JSONL format)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Pipe output through jq -s '.' to convert JSONL to JSON array\"\n    },\n    {\n      \"title\": \"Overwrite flag destroys data when input equals output\",\n      \"expected\": \"Safe overwrite of output file\",\n      \"actual\": \"Truncates input file before reading when input and output paths are identical, resulting in empty output and data loss with no warning\",\n      \"severity\": \"critical\",\n      \"workaround\": \"Never use the same file path for both input and output; always write to a different file first\"\n    },\n    {\n      \"title\": \"Dry-run creates orphaned temp files\",\n      \"expected\": \"No files written to disk during dry-run\",\n      \"actual\": \"Creates temporary files in /tmp that are not cleaned up, accumulating gigabytes over repeated runs\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Manually clean /tmp directory periodically or after batch dry-run tests\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale settings by default\",\n      \"actual\": \"Always parses dates as MM/DD/YYYY regardless of system locale (e.g., DD/MM/YYYY on en_GB systems)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Explicitly set --date-format flag to match your expected date format\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Notes section: 'JSON output produces a single JSON array of objects'\",\n      \"correction\": \"JSON format outputs newline-delimited JSON objects (JSONL), not a single JSON array\"\n    },\n    {\n      \"location\": \"Notes section: 'Date parsing respects system locale settings'\",\n      \"correction\": \"Default date parsing uses MM/DD/YYYY format regardless of system locale; use --date-format to specify alternative formats\"\n    },\n    {\n      \"location\": \"Notes section: '--dry-run shows what would be written without creating any files'\",\n      \"correction\": \"--dry-run prevents writing the final output file but still creates temporary files in /tmp that are not automatically cleaned up\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Non-existent columns in --select create null columns\",\n      \"description\": \"When --select includes a column name that doesn't exist in the input data, dply creates that column in the output with all values set to null instead of raising an error\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"title\": \"Slow performance on files larger than available RAM\",\n      \"description\": \"User reported 45-minute processing time for 12GB file on 8GB RAM system. This is expected behavior as dply loads entire file into memory, causing disk swapping. Not a bug but a design characteristic of in-memory processing tools.\"\n    }\n  ]\n}",
          "score": 0.75,
          "rationale": "Criteria 2, 3, and 4 are satisfied but criterion 1 fails. Positive points: the submission correctly identified the main behavioral quirks (overwrite truncation, dry-run leaving temp files, default date parsing ignoring locale) and additionally noted the JSONL behavior (the core quirks from the ground truth are present). The doc_errors match the ground truth in substance (JSON output is JSONL, dry-run creates temp files, date-format default is wrong). The hidden feature (--select creating null columns) and the false alarm (slow processing on a 12GB file with 8GB RAM is expected in-memory behavior) are correctly captured. Problems: the JSON structure deviates from the expected types for some fields — hidden_features and false_alarms are objects with title/description, whereas the ground truth and required format expect arrays of strings. Because the submission did not match the required types/shape exactly, it fails Criterion 1 (valid JSON with correct types). Therefore the score is 0.75 (three criteria satisfied = 0.75)."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\"tool_name\":\"dply\",\"category\":\"command-line data processor\",\"quirks\":[{\"title\":\"JSON output format\",\"expected\":\"Single JSON array of objects as documented\",\"actual\":\"Outputs one JSON object per line (JSONL)\",\"severity\":\"medium\",\"workaround\":\"Pipe through `jq -s '.'` to wrap into array\"},{\"title\":\"Overwrite with same input\",\"expected\":\"Input file preserved when output file same path\",\"actual\":\"Input file truncated before reading, leaving empty output\",\"severity\":\"high\",\"workaround\":\"Avoid pointing output at same path; copy input elsewhere first\"},{\"title\":\"Dry run temp files\",\"expected\":\"Dry run leaves no filesystem traces\",\"actual\":\"Leaves temp files in `/tmp` that accumulate\",\"severity\":\"low\",\"workaround\":\"Manually clean `/tmp` after extensive dry runs\"},{\"title\":\"Default date parsing\",\"expected\":\"Respects system locale (e.g., en_GB DD/MM/YYYY)\",\"actual\":\"Always treats dates as MM/DD/YYYY unless `--date-format` used\",\"severity\":\"medium\",\"workaround\":\"Pass explicit `--date-format` matching desired locale\"},{\"title\":\"Missing column in select\",\"expected\":\"Error or warning for unknown column\",\"actual\":\"Creates column filled with nulls without notification\",\"severity\":\"low\",\"workaround\":\"Ensure column names exist before using `--select` (e.g., inspect headers)\"}],\"doc_errors\":[{\"location\":\"Official docs under `--format json` description\",\"correction\":\"Clarify that `--format json` emits JSONL (one JSON object per line) rather than a single array\"},{\"location\":\"Notes section about date parsing\",\"correction\":\"State that default parsing uses MM/DD/YYYY regardless of locale, unless `--date-format` is set\"}],\"hidden_features\":[],\"false_alarms\":[]}",
          "score": 0.25,
          "rationale": "Criterion 1 (Valid JSON only) — PASS: The submission is a single valid JSON object and includes all required keys with appropriate types (tool_name, category, quirks, doc_errors, hidden_features, false_alarms). (0.25)\n\nCriterion 2 (Correct quirks) — FAIL: The ground truth quirks list contains three items (--overwrite truncation, --dry-run temp files, default date parsing). The submission does include those three, but it also promotes two items that differ from the ground truth: it marks the JSON vs JSONL behavior as a quirk (ground truth treated that as a doc_error) and it promotes the \"missing column in select creates nulls\" behavior (ground truth classified that as a hidden_feature). Because the submission does not match the ground-truth quirks exactly and miscategorizes features, this criterion is not satisfied. (0.00)\n\nCriterion 3 (Correct doc errors and hidden features) — FAIL: The submission's doc_errors list omits the --dry-run doc error (ground truth notes that docs claiming \"without creating any files\" is incorrect because temp files are created). It also omits the date/dry-run combination doc correction present in the ground truth. The hidden_features array is empty, but the ground truth lists the --select behavior as a hidden feature; the submission instead incorrectly moved that into quirks. Therefore doc_errors and hidden_features do not match the ground truth. (0.00)\n\nCriterion 4 (Correct false alarms) — FAIL: The ground truth lists the slow processing of a 12GB file on 8GB RAM as a false alarm (expected in-memory behavior). The submission's false_alarms array is empty and does not mark this as a false alarm. This mismatches the ground truth. (0.00)\n\nOverall score: only criterion 1 met -> 0.25."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-003",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 3,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"Webhook Delivery Service\",\n  \"quirks\": [\n    {\n      \"title\": \"Timestamp Unit Mismatch\",\n      \"expected\": \"The timestamp_ms field should contain milliseconds since Unix epoch as indicated by its name.\",\n      \"actual\": \"The field contains Unix seconds (e.g., 1738600000), causing date parsing errors if treated as milliseconds.\",\n      \"severity\": \"High\",\n      \"workaround\": \"Parse the timestamp_ms value as seconds or multiply by 1000 before processing.\"\n    },\n    {\n      \"title\": \"Silent Low-Threshold Truncation\",\n      \"expected\": \"Payloads up to 1MB should be delivered in full; payloads over the limit should ideally be rejected or flagged.\",\n      \"actual\": \"Large payloads (observed at 1.3MB) are silently truncated at 256KB without headers indicating the cut, resulting in malformed JSON.\",\n      \"severity\": \"Critical\",\n      \"workaround\": \"Keep webhook payloads below 256KB to ensure data integrity and valid JSON structure.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Setup / Signature Algorithm\",\n      \"correction\": \"The service uses SHA-256 for HMAC signing, not SHA-1.\"\n    },\n    {\n      \"location\": \"Retry policy / Max backoff interval\",\n      \"correction\": \"The maximum backoff interval is 5 minutes (300 seconds), not 1 hour.\"\n    },\n    {\n      \"location\": \"Payload delivery / Max payload size\",\n      \"correction\": \"The effective payload limit before truncation occurs is 256KB, not 1MB.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Including an 'X-NotifyHub-Debug: true' header in the endpoint's response enables an 'X-NotifyHub-Trace-ID' header in subsequent webhook deliveries for support tracking.\"\n  ],\n  \"false_alarms\": [\n    \"Apparent retry failure caused by local corporate firewalls blocking NotifyHub's IP range rather than a service-side issue.\"\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are satisfied.\n\n1) Valid JSON only: The submission is a single valid JSON object and includes all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types—score awarded.\n\n2) Correct quirks: The quirks match the ground truth issues: timestamp_ms contains seconds not milliseconds, and large payloads are silently truncated at 256KB. The substance of both quirks and reasonable workarounds are present—score awarded.\n\n3) Correct doc errors and hidden features: The submission correctly lists the three documentation errors (signing algorithm SHA-256 vs SHA-1, max backoff 5 minutes vs 1 hour, and effective truncation at 256KB vs documented 1MB). The hidden feature (X-NotifyHub-Debug enabling X-NotifyHub-Trace-ID) is also correctly reported—score awarded.\n\n4) Correct false alarms: The submission correctly classifies the missing retries as caused by a corporate firewall blocking NotifyHub IPs rather than a service bug—score awarded.\n\nGiven all criteria are met, the final score is 1.0."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\"tool_name\":\"NotifyHub Explorer\",\"category\":\"webhook service\",\"quirks\":[{\"title\":\"Signature algorithm mismatch\",\"expected\":\"SHA-1 HMAC signature\",\"actual\":\"Needs SHA-256 or delivery fails\",\"severity\":\"high\",\"workaround\":\"Verify signatures using SHA-256 instead of SHA-1 per current behavior\"},{\"title\":\"timestamp_ms is actually seconds\",\"expected\":\"Milliseconds since Unix epoch\",\"actual\":\"Seconds (e.g., 1738600000)\",\"severity\":\"medium\",\"workaround\":\"Treat timestamp_ms as seconds when converting to date/time\"},{\"title\":\"Retry max backoff is capped at 5 minutes\",\"expected\":\"Retries back off up to 1 hour\",\"actual\":\"Ceiling observed at ~300s (5 minutes)\",\"severity\":\"medium\",\"workaround\":\"Design retry tolerance assuming 5-minute maximum interval\"},{\"title\":\"Payload truncation at 256KB\",\"expected\":\"Payloads delivered up to 1MB\",\"actual\":\"Truncates silently above ~256KB\",\"severity\":\"high\",\"workaround\":\"Keep webhook payloads below 256KB or split events; monitor for malformed JSON to detect truncation\"}],\"doc_errors\":[{\"location\":\"Setup/signing\",\"correction\":\"Signature uses SHA-256 for HMAC, not SHA-1 as stated\"},{\"location\":\"Payload schema\",\"correction\":\"timestamp_ms represents seconds since Unix epoch, not milliseconds\"},{\"location\":\"Retry policy\",\"correction\":\"Max retry backoff is 5 minutes (300s), not 1 hour\"},{\"location\":\"Payload delivery\",\"correction\":\"Payloads larger than ~256KB are truncated; max reliable size is ~256KB, not 1MB\"}],\"hidden_features\":[{\"title\":\"Trace ID when returning X-NotifyHub-Debug\",\"description\":\"Respond with `X-NotifyHub-Debug: true` and subsequent deliveries include `X-NotifyHub-Trace-ID`, useful when filing support tickets\"}],\"false_alarms\":[{\"title\":\"Missing retry attempts\",\"description\":\"Retries were actually sent but blocked by corporate firewall until NotifyHub IPs were allowlisted, not a delivery issue\"}]}",
          "score": 0.5,
          "rationale": "Partial match. The submission correctly captured the substantive doc errors (signing algorithm, timestamp units, retry backoff, payload truncation) and the hidden feature and false alarm from the exploration transcript — so doc_errors and false_alarms/hidden feature content match the ground truth (criterion 3 and 4 satisfied). However, it fails two checklist items: (1) Valid JSON/types: although the output is valid JSON, the types/shape differ from the expected schema (hidden_features and false_alarms are objects with title/description rather than the simple string entries expected), so it does not strictly meet the required key types/format (criterion 1 failed). (2) Correct quirks: the ground truth lists only two quirks (timestamp_ms in seconds and silent truncation at 256KB). The submission included those but also promoted signing and retry-backoff issues into the quirks section (these were expected as doc_errors rather than quirks in the ground truth). Because the quirks array does not match the ground truth's intended set/placement exactly, criterion 2 is not met. Thus 2 of 4 criteria satisfied → score 0.5."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "```json\n{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"webhook\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains seconds despite name\",\n      \"expected\": \"Milliseconds since Unix epoch (e.g., 1738600000000)\",\n      \"actual\": \"Seconds since Unix epoch (e.g., 1738600000)\",\n      \"severity\": \"high\",\n      \"workaround\": \"Parse timestamp_ms as seconds instead of milliseconds\"\n    },\n    {\n      \"title\": \"Silent payload truncation at 256KB\",\n      \"expected\": \"Reject delivery or send error when payload exceeds limit\",\n      \"actual\": \"Silently truncates payload at 256KB resulting in malformed JSON\",\n      \"severity\": \"critical\",\n      \"workaround\": \"Keep payloads under 256KB or implement size validation before webhook generation\"\n    },\n    {\n      \"title\": \"Signature algorithm is SHA-256 not SHA-1\",\n      \"expected\": \"HMAC signature using SHA-1 as documented\",\n      \"actual\": \"HMAC signature using SHA-256\",\n      \"severity\": \"high\",\n      \"workaround\": \"Use SHA-256 for signature verification instead of SHA-1\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Setup section - signature algorithm\",\n      \"correction\": \"NotifyHub signs payloads with SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy - max backoff interval\",\n      \"correction\": \"Max backoff interval is 5 minutes (300 seconds), not 1 hour\"\n    },\n    {\n      \"location\": \"Payload delivery - max payload size\",\n      \"correction\": \"Payloads are truncated at 256KB, not 1MB\"\n    },\n    {\n      \"location\": \"Payload schema - timestamp_ms field description\",\n      \"correction\": \"timestamp_ms contains Unix seconds, not milliseconds despite the field name\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Debug trace IDs\",\n      \"description\": \"Including X-NotifyHub-Debug: true header in endpoint response enables X-NotifyHub-Trace-ID header in subsequent webhook deliveries for support ticket tracking\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"title\": \"Missing retries due to firewall\",\n      \"description\": \"Eng2's endpoint not receiving retry attempts was caused by corporate firewall blocking NotifyHub's IP range, not a service issue. Resolved by allowlisting NotifyHub IPs\"\n    }\n  ]\n}\n```",
          "score": 0.25,
          "rationale": "Only criterion 4 (false alarms) is satisfied. Reasoning by criterion:\n\n1) Valid JSON only (0.0): The submission is valid JSON and includes all required keys, but several values use the wrong types compared to the expected schema: hidden_features in the ground truth is an array of strings but the submission uses an array of objects; false_alarms is an array of strings in the ground truth but the submission uses objects. The rubric requires correct types, so this fails.\n\n2) Correct quirks (0.0): The submission correctly lists the two real behavioral quirks (timestamp_ms in seconds and silent truncation at 256KB). However it also promotes the signature algorithm issue into a quirk. In the ground truth that item is a documentation error, not a behavioral quirk, so the quirk list does not match the ground truth set exactly (a mis-promotion). Severity labels differ but that alone would be minor; the misclassification causes failure for this criterion.\n\n3) Correct doc errors and hidden features (0.0): The submission includes the correct doc errors for signing algorithm, retry backoff, and payload size, and adds a payload-schema/timestamp_ms doc entry (which the ground truth treated as a quirk). The hidden feature is correctly described in content, but its type (object with title/description) does not match the expected string entry. Because of the misclassification (timestamp_ms) and type mismatch for hidden_features, this criterion is not fully satisfied.\n\n4) Correct false alarms (0.25): The submission correctly identifies the firewall/allowlist issue as a false alarm and describes it accurately. This matches the ground truth in substance (despite using an object with title/description instead of a plain string).\n\nSummary: Submission captures most substantive issues, but misclassifies one item (signature) as a quirk and has schema/type mismatches for hidden_features and false_alarms, so only one of four binary criteria is met."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-004",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 4,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-005",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 5,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-006",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 6,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}",
      "meta": {},
      "results": []
    }
  ],
  "models": [
    {
      "model_name": "gemini-3-flash",
      "avg_score": 0.9167,
      "total": 3,
      "attempted": 3
    },
    {
      "model_name": "anthropic/claude-sonnet-4-5-20250929",
      "avg_score": 0.6667,
      "total": 3,
      "attempted": 3
    },
    {
      "model_name": "openai/gpt-5.1-codex-mini",
      "avg_score": 0.5833,
      "total": 3,
      "attempted": 3
    }
  ],
  "runs": [
    {
      "name": "suite_smoke",
      "path": "tool-exploration/results/suite_smoke"
    }
  ],
  "errors": []
}
