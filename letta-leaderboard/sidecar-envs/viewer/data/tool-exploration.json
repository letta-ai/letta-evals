{
  "category": "tool-exploration",
  "display_name": "Tool Exploration",
  "generated_at": "2026-02-04T18:49:22.542919+00:00",
  "datasets": [
    {
      "name": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "count": 16
    }
  ],
  "scenarios": [
    {
      "id": "explore-001",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 1,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nCatalogAPI v2.3 - Product Search Endpoint\n\nGET /v2/products\n\nQuery parameters:\n- q (string, required): search query\n- limit (int, optional): results per page, default 10, max 100\n- offset (int, optional): pagination offset, default 0\n- sort (string, optional): one of \"relevance\", \"price_asc\", \"price_desc\", \"newest\"\n- include_deleted (bool, optional): include soft-deleted products, default false\n\nResponse:\n{ \"results\": [...], \"total_count\": int, \"offset\": int, \"limit\": int }\n\ntotal_count reflects the exact number of matching products at query time.\n\nAuthentication: Bearer token in Authorization header. Tokens expire after 24 hours.\n\nRate limits: 100 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I've been testing the CatalogAPI search endpoint today. First thing I noticed: the docs say max limit is 100, but when I set limit=100 I get back only 50 results even though total_count says 847.\n\nDev2: Did you try limit=51?\n\nDev1: Yeah, anything above 50 just returns 50. So the actual max is 50, not 100. The docs are wrong.\n\nDev2: Annoying. What about pagination?\n\nDev1: That's where it gets weird. I was paginating through results with offset=0, offset=10, offset=20 etc. Between my offset=10 and offset=20 requests, someone added a new product. I ended up seeing the same product twice - once at position 10 in the first page and again at position 1 in the second page.\n\nDev2: Classic offset pagination race condition. Did you try any workaround?\n\nDev1: I added deduplication by product ID on the client side. Not ideal but it works.\n\nDev2: What about total_count? Is it accurate?\n\nDev1: Funny you ask. I ran the exact same query three times in rapid succession. Got total_count values of 847, 847, and 843. Checked the actual database - there were 847 products matching. So total_count seems to be eventually consistent. Sometimes it lags behind by a few seconds.\n\nDev2: Interesting. I ran into an auth issue earlier - kept getting 401 Unauthorized. Spent 20 minutes debugging before I realized my token had expired. I'd been testing all morning and didn't notice the 24h window had passed.\n\nDev1: Oh yeah, I had the same thing happen but realized it was the token pretty quickly. Not a bug, just annoying.\n\nDev2: True. One more thing - I noticed that if you pass include_deleted=true, deleted products show up in results. But if you search without that flag, products that were deleted very recently (like within the last 30 seconds or so) still appear in results anyway. Seems like the deletion index has a propagation delay.\n\nDev1: That could cause problems for users who delete a product and immediately search to confirm it's gone.\n\nDev2: Yeah. Oh, I also discovered something not in the docs - if you pass sort=rating, it actually works. Returns products sorted by average user rating. It's not documented anywhere but it's clearly functional.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset pagination duplicates on concurrent writes\",\n      \"expected\": \"Paginating with offset returns each product exactly once\",\n      \"actual\": \"New inserts between paginated requests cause duplicate results across pages\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Deduplicate results by product ID on the client side\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count reflects exact count at query time (per docs)\",\n      \"actual\": \"total_count can lag behind the actual count by a few seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Deleted products appear briefly after deletion\",\n      \"expected\": \"Deleted products are excluded from results immediately when include_deleted=false\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"limit parameter description: max 100\",\n      \"correction\": \"Actual max is 50. Requests with limit > 50 are silently capped at 50.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"sort=rating sorts by average user rating (undocumented sort option)\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors were caused by expired bearer tokens, not an API bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\n  \"tool_name\": \"CatalogAPI\",\n  \"category\": \"Product Search\",\n  \"quirks\": [\n    {\n      \"title\": \"Limit cap lower than advertised\",\n      \"expected\": \"limit accepts up to 100 results per request\",\n      \"actual\": \"requests return at most 50 items even when limit=100; total_count still reports the full matches\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Batch pagination with limit=50 and deduplicate if you expect duplicates anyway\"\n    },\n    {\n      \"title\": \"Offset pagination race duplication\",\n      \"expected\": \"paging through offset=0,10,20 should give unique results for a static snapshot\",\n      \"actual\": \"in-flight product insertions can cause the same item to appear in two pages (e.g., position 10 on first page and position 1 on the next)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"client-side deduplication by product ID when merging pages\"\n    },\n    {\n      \"title\": \"Eventually consistent total_count\",\n      \"expected\": \"total_count equals the current exact number of matching products for each request\",\n      \"actual\": \"total_count can differ slightly between rapid repeated requests (e.g., 847 vs 843) even though database has 847 matches\",\n      \"severity\": \"low\",\n      \"workaround\": \"treat total_count as best-effort and rely on actual results for confirmation\"\n    },\n    {\n      \"title\": \"Soft-deletion propagation lag\",\n      \"expected\": \"deleted products disappear from results immediately unless include_deleted=true\",\n      \"actual\": \"recently deleted entries (within ~30s) still surface in queries without include_deleted, so they reappear before propagation completes\",\n      \"severity\": \"medium\",\n      \"workaround\": \"wait a short interval before verifying via search, or use include_deleted=true when confirming removals\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"GET /v2/products query parameter limit\",\n      \"correction\": \"Actual supported max is 50, not 100.\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Sorting by rating\",\n      \"details\": \"Passing sort=rating returns products ordered by average user rating even though the documented sort options omit this value.\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"description\": \"401 Unauthorized errors resolved by refreshing the bearer token after the documented 24-hour expiration window; not an API bug.\"\n    }\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are met. 1) The submission is a single valid JSON object containing the required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types and no extra text. 2) The quirks list includes the same real behavioral issues as the ground truth (offset pagination duplicates, eventual consistency of total_count, and deleted-product propagation lag); the submission also notes the limit cap behavior (expressed as a quirk) which is a real observed behavior and does not contradict the ground truth. 3) The documented error about the limit max (docs claim 100; actual 50) is correctly identified with an appropriate location and correction, and the hidden feature (sort=rating) is correctly reported. 4) The false alarm (401s due to expired bearer tokens) is correctly classified. Minor differences in phrasing, severity labels, and representation formats (e.g., hidden_features as an object rather than a plain string, category value wording) do not change the substance, so full credit is warranted."
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"CatalogAPI v2.3 - Product Search Endpoint\",\n  \"category\": \"E-commerce Search API\",\n  \"quirks\": [\n    {\n      \"title\": \"Offset Pagination Race Condition\",\n      \"expected\": \"Stable pagination where each product is seen exactly once across sequential pages.\",\n      \"actual\": \"New product insertions between requests shift result positions, causing products to appear twice across page boundaries.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Implement client-side deduplication using unique product IDs.\"\n    },\n    {\n      \"title\": \"Eventually Consistent total_count\",\n      \"expected\": \"Exact number of matching products at the time of the query.\",\n      \"actual\": \"The count lags behind the actual database state, occasionally returning lower values than the real matching set.\",\n      \"severity\": \"Low\",\n      \"workaround\": \"Treat total_count as an approximate value or implement retry logic for high-precision needs.\"\n    },\n    {\n      \"title\": \"Search Index Deletion Lag\",\n      \"expected\": \"Products not matching 'include_deleted=true' should disappear from results immediately upon deletion.\",\n      \"actual\": \"Recently deleted products (within ~30 seconds) still appear in search results without the deleted flag.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Verify product status on the client side or account for a 30-second delay in indexing.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Query parameters -> limit\",\n      \"correction\": \"The maximum allowed limit is 50. Values provided between 51 and 100 are ignored and capped at 50 results.\"\n    },\n    {\n      \"location\": \"Response -> total_count description\",\n      \"correction\": \"The total_count is eventually consistent rather than reflecting the exact number of products at query time.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Support for 'sort=rating' query parameter, which returns products sorted by average user rating.\"\n  ],\n  \"false_alarms\": [\n    \"401 Unauthorized errors encountered after long testing sessions are due to the documented 24-hour token expiration policy, not an API malfunction.\"\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are satisfied. 1) The submission is a single valid JSON object and includes all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types. 2) The quirks list correctly captures the same three real behavioral issues from the ground truth (offset pagination duplicates, eventually-consistent total_count, and deletion/indexing lag) — differences in wording and severity labels are minor and acceptable. 3) Documentation errors and hidden features are correctly identified: the limit max being 50 is reported with the correct location/correction, and the undocumented sort=rating hidden feature is present. (The submission also notes total_count eventual consistency as a doc correction; while the ground truth listed that as a quirk, the agent's note is accurate and doesn't contradict the expected answer.) 4) The false alarm is correctly classified: 401 errors were due to expired bearer tokens, matching the ground truth. Minor differences in phrasing and an extra doc_error do not conflict with the ground truth, so the submission receives full credit."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "{\n  \"tool_name\": \"CatalogAPI v2.3 Product Search Endpoint\",\n  \"category\": \"API\",\n  \"quirks\": [\n    {\n      \"title\": \"Maximum limit is 50, not 100 as documented\",\n      \"expected\": \"Setting limit=100 should return up to 100 results per page as stated in documentation\",\n      \"actual\": \"Any limit value above 50 returns only 50 results, regardless of how many products match\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Use limit=50 and adjust pagination logic to account for the lower maximum\"\n    },\n    {\n      \"title\": \"Offset pagination causes duplicate results when data changes\",\n      \"expected\": \"Each product should appear exactly once when paginating through results\",\n      \"actual\": \"Products inserted between pagination requests can cause the same product to appear on multiple pages\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Implement client-side deduplication using product IDs\"\n    },\n    {\n      \"title\": \"total_count is eventually consistent\",\n      \"expected\": \"total_count should reflect the exact number of matching products at query time\",\n      \"actual\": \"total_count can be stale and lag behind the actual database state by several seconds\",\n      \"severity\": \"low\",\n      \"workaround\": \"Do not rely on total_count for real-time accuracy in critical operations\"\n    },\n    {\n      \"title\": \"Recently deleted products still appear in search results\",\n      \"expected\": \"Products deleted without include_deleted=true should not appear in results\",\n      \"actual\": \"Products deleted within approximately 30 seconds still appear in search results due to deletion index propagation delay\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Wait 30+ seconds after deletion before confirming removal via search, or use direct product ID lookup for immediate verification\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Query parameter 'limit' - states 'max 100'\",\n      \"correction\": \"Actual maximum is 50, not 100\"\n    },\n    {\n      \"location\": \"Response description for 'total_count' - states 'reflects the exact number of matching products at query time'\",\n      \"correction\": \"total_count is eventually consistent and may lag behind actual database state by several seconds\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"feature\": \"sort=rating parameter\",\n      \"description\": \"Undocumented sort option that returns products sorted by average user rating. Works correctly but is not listed in the documented sort options.\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"issue\": \"401 Unauthorized errors after extended testing\",\n      \"explanation\": \"Tokens expire after 24 hours as documented. This is expected behavior, not a bug. Developers need to refresh tokens regularly during long testing sessions.\"\n    }\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are met.\n\n1) Valid JSON: The submission is a single JSON object with all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) and appropriate types (strings and arrays). No extra text or code fences are present.\n\n2) Correct quirks: The submission captures the same behavioral issues as the ground truth: offset pagination duplicates, eventual consistency of total_count, and recently deleted products reappearing. The agent also included the limit-at-50 behavior as a quirk (a real observed behavior); substance and workarounds match the exploration transcript.\n\n3) Correct doc errors and hidden features: The submission correctly identifies the documentation error about the limit (max 100 → actual max 50). The undocumented sort=rating hidden feature is reported and described, matching the ground truth. (The submission also notes total_count's inconsistency as a doc-related correction; this is consistent with the transcript and does not contradict the ground truth.)\n\n4) Correct false alarms: The submission correctly classifies the 401 Unauthorized occurrences as expired tokens / expected behavior, matching the ground truth.\n\nMinor differences in phrasing and severity labels do not affect correctness; the reported items align in substance with the expected answer."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-002",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 2,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ndply v1.4 - Command-Line Data Processor\n\nUsage: dply [options] <input_file> <output_file>\n\nOptions:\n  --format <fmt>    Output format: csv, json, tsv (default: csv)\n  --filter <expr>   Filter rows by expression (e.g. \"age > 30\")\n  --select <cols>   Comma-separated column names to include\n  --overwrite       Overwrite output file if it exists\n  --dry-run         Preview output without writing to disk\n  --delimiter <d>   Input delimiter for CSV (default: comma)\n  --date-format <f> Date parsing format (default: system locale)\n  --encoding <e>    Input encoding (default: utf-8)\n  --verbose         Print processing stats to stderr\n\nNotes:\n- JSON output produces a single JSON array of objects.\n- --dry-run shows what would be written without creating any files.\n- Date parsing respects system locale settings.\n\n--- EXPLORATION TRANSCRIPT ---\n\nUser1: I've been testing dply for our ETL pipeline. First issue: the docs say --format json produces \"a single JSON array of objects\" but it actually outputs one JSON object per line. That's JSONL, not JSON.\n\nUser2: Oh interesting. So if you need actual JSON you'd have to wrap it yourself?\n\nUser1: Exactly. Had to pipe through jq -s '.' to get a proper array. Minor but the docs are misleading.\n\nUser2: What about the overwrite flag? I had a scary moment with that.\n\nUser1: Oh yeah, I hit that too. If your input and output are the same file and you use --overwrite, it truncates the input file before reading it. You end up with an empty output.\n\nUser2: That's bad. Is there a warning?\n\nUser1: No warning at all. It just silently destroys your data. I lost a staging dataset that way. Had to restore from backup.\n\nUser2: Yikes. What about --dry-run? At least that's safe right?\n\nUser1: Mostly. It doesn't write the final output file, but I noticed it still creates temp files in /tmp that don't get cleaned up. After a few hundred dry runs during testing, I had gigabytes of temp files sitting there.\n\nUser2: That's messy. Did you test the date parsing at all?\n\nUser1: That's another fun one. Docs say it respects system locale. My system is set to en_GB (DD/MM/YYYY). But dply always parses dates as MM/DD/YYYY regardless. So \"03/04/2026\" is treated as March 4th even on a British locale system.\n\nUser2: Wait, always MM/DD/YYYY? Even if you set --date-format?\n\nUser1: Actually, --date-format does work correctly if you explicitly set it. It's just the default that ignores locale. So the docs are wrong about the default behavior.\n\nUser2: I tried processing a 12GB file yesterday and it was incredibly slow. Like 45 minutes for a simple filter operation.\n\nUser1: How much RAM does your machine have?\n\nUser2: 8GB. The file was way bigger than available memory.\n\nUser1: Yeah, dply loads the entire file into memory. A 12GB file on 8GB RAM means it's swapping to disk constantly. That's expected behavior for an in-memory tool, not a bug. You'd need to split the file or use a streaming tool.\n\nUser1: Oh, one thing I found by accident - if you pass --select with a column name that doesn't exist, instead of erroring, it creates that column filled with nulls. Not documented anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"dply\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"--overwrite silently truncates when input and output are the same file\",\n      \"expected\": \"Overwrite reads input fully before writing output, or warns about same-file conflict\",\n      \"actual\": \"Truncates the input file before reading, resulting in empty output and data loss\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Always use a different output file path, then rename afterward\"\n    },\n    {\n      \"title\": \"--dry-run leaves temp files in /tmp\",\n      \"expected\": \"Dry run creates no files at all (per docs: without creating any files)\",\n      \"actual\": \"Temp files are created in /tmp and never cleaned up\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Periodically clean /tmp/dply-* files manually\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale (per docs)\",\n      \"actual\": \"Always uses MM/DD/YYYY regardless of locale. --date-format flag works correctly as an override.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --date-format explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"--format json description: single JSON array of objects\",\n      \"correction\": \"Actually outputs JSONL (one JSON object per line), not a JSON array\"\n    },\n    {\n      \"location\": \"--dry-run description: without creating any files\",\n      \"correction\": \"Temp files are still created in /tmp and not cleaned up\"\n    },\n    {\n      \"location\": \"--date-format default: system locale\",\n      \"correction\": \"Default is always MM/DD/YYYY regardless of system locale\"\n    }\n  ],\n  \"hidden_features\": [\n    \"--select with a nonexistent column name creates that column filled with nulls instead of erroring\"\n  ],\n  \"false_alarms\": [\n    \"Slow processing of 12GB file on 8GB RAM was expected behavior (in-memory tool swapping to disk), not a bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"dply v1.4\",\n  \"category\": \"Command-Line Data Processor\",\n  \"quirks\": [\n    {\n      \"title\": \"JSON Output Format Mismatch\",\n      \"expected\": \"A single JSON array of objects.\",\n      \"actual\": \"Outputs one JSON object per line (JSONL).\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Pipe the output through jq -s '.' to wrap objects into a proper JSON array.\"\n    },\n    {\n      \"title\": \"In-Place Overwrite Data Loss\",\n      \"expected\": \"Read input data then write to output.\",\n      \"actual\": \"Truncates the input file before reading if the input and output paths are identical.\",\n      \"severity\": \"High\",\n      \"workaround\": \"Never use the same file path for input and output; write to a temporary file first.\"\n    },\n    {\n      \"title\": \"Dry-Run Temporary File Leak\",\n      \"expected\": \"No side effects on the filesystem.\",\n      \"actual\": \"Creates temporary files in /tmp that are not cleaned up after execution.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Manually monitor and clear /tmp of dply-related temporary files after use.\"\n    },\n    {\n      \"title\": \"Locale-Insensitive Date Parsing\",\n      \"expected\": \"Date parsing respects the system locale (e.g., DD/MM/YYYY for en_GB).\",\n      \"actual\": \"Defaults to MM/DD/YYYY regardless of system locale settings.\",\n      \"severity\": \"Medium\",\n      \"workaround\": \"Explicitly define the date format using the --date-format flag.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Notes: JSON output produces a single JSON array of objects.\",\n      \"correction\": \"JSON output produces one JSON object per line (JSONL format).\"\n    },\n    {\n      \"location\": \"Options: --date-format <f> Date parsing format (default: system locale)\",\n      \"correction\": \"The default date parsing format is MM/DD/YYYY, regardless of system locale.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Passing non-existent column names to --select will create new columns filled with null values rather than throwing an error.\"\n  ],\n  \"false_alarms\": [\n    \"Extreme slowness and high memory usage when processing files larger than available RAM is a result of the tool's in-memory processing architecture, not a software bug.\"\n  ]\n}",
          "score": 0.75,
          "rationale": "I awarded 0.75 because the submission satisfies three of the four rubric criteria. Criterion 1 (Valid JSON only): PASSED — the submission is a single valid JSON object containing all required keys with appropriate types. Criterion 2 (Correct quirks): PASSED — the submission lists the same core behavioral quirks as the ground truth (overwrite truncation, dry-run temp-file leak, and locale-insensitive date parsing). It also added the JSON vs JSONL issue as a quirk; that issue is real (not a false alarm), so this does not invalidate the quirks list. Criterion 3 (Correct doc errors and hidden features): FAILED — the submission correctly reports the JSON vs JSONL doc error and the date-format/doc mismatch, and it documents the hidden --select behavior, but it omits the doc error about --dry-run documentation claiming 'without creating any files' (the ground truth lists that as a documentation error). Because one required doc_error is missing, this criterion is not satisfied. Criterion 4 (Correct false alarms): PASSED — the submission correctly classifies the slow processing of a 12GB file on 8GB RAM as expected behavior (an in-memory tool limitation) rather than a bug. Overall score: 3/4 criteria satisfied = 0.75."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "{\n  \"tool_name\": \"dply\",\n  \"category\": \"data processing\",\n  \"quirks\": [\n    {\n      \"title\": \"JSON format outputs JSONL instead of JSON array\",\n      \"expected\": \"Single JSON array of objects as documented\",\n      \"actual\": \"Outputs one JSON object per line (JSONL format)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Pipe output through jq -s '.' to convert JSONL to JSON array\"\n    },\n    {\n      \"title\": \"Overwrite flag destroys data when input equals output\",\n      \"expected\": \"Safe overwrite of output file\",\n      \"actual\": \"Truncates input file before reading when input and output paths are identical, resulting in empty output and data loss with no warning\",\n      \"severity\": \"critical\",\n      \"workaround\": \"Never use the same file path for both input and output; always write to a different file first\"\n    },\n    {\n      \"title\": \"Dry-run creates orphaned temp files\",\n      \"expected\": \"No files written to disk during dry-run\",\n      \"actual\": \"Creates temporary files in /tmp that are not cleaned up, accumulating gigabytes over repeated runs\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Manually clean /tmp directory periodically or after batch dry-run tests\"\n    },\n    {\n      \"title\": \"Default date parsing ignores system locale\",\n      \"expected\": \"Date parsing respects system locale settings by default\",\n      \"actual\": \"Always parses dates as MM/DD/YYYY regardless of system locale (e.g., DD/MM/YYYY on en_GB systems)\",\n      \"severity\": \"medium\",\n      \"workaround\": \"Explicitly set --date-format flag to match your expected date format\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Notes section: 'JSON output produces a single JSON array of objects'\",\n      \"correction\": \"JSON format outputs newline-delimited JSON objects (JSONL), not a single JSON array\"\n    },\n    {\n      \"location\": \"Notes section: 'Date parsing respects system locale settings'\",\n      \"correction\": \"Default date parsing uses MM/DD/YYYY format regardless of system locale; use --date-format to specify alternative formats\"\n    },\n    {\n      \"location\": \"Notes section: '--dry-run shows what would be written without creating any files'\",\n      \"correction\": \"--dry-run prevents writing the final output file but still creates temporary files in /tmp that are not automatically cleaned up\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Non-existent columns in --select create null columns\",\n      \"description\": \"When --select includes a column name that doesn't exist in the input data, dply creates that column in the output with all values set to null instead of raising an error\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"title\": \"Slow performance on files larger than available RAM\",\n      \"description\": \"User reported 45-minute processing time for 12GB file on 8GB RAM system. This is expected behavior as dply loads entire file into memory, causing disk swapping. Not a bug but a design characteristic of in-memory processing tools.\"\n    }\n  ]\n}",
          "score": 0.75,
          "rationale": "Criteria 2, 3, and 4 are satisfied but criterion 1 fails. Positive points: the submission correctly identified the main behavioral quirks (overwrite truncation, dry-run leaving temp files, default date parsing ignoring locale) and additionally noted the JSONL behavior (the core quirks from the ground truth are present). The doc_errors match the ground truth in substance (JSON output is JSONL, dry-run creates temp files, date-format default is wrong). The hidden feature (--select creating null columns) and the false alarm (slow processing on a 12GB file with 8GB RAM is expected in-memory behavior) are correctly captured. Problems: the JSON structure deviates from the expected types for some fields — hidden_features and false_alarms are objects with title/description, whereas the ground truth and required format expect arrays of strings. Because the submission did not match the required types/shape exactly, it fails Criterion 1 (valid JSON with correct types). Therefore the score is 0.75 (three criteria satisfied = 0.75)."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\"tool_name\":\"dply\",\"category\":\"command-line data processor\",\"quirks\":[{\"title\":\"JSON output format\",\"expected\":\"Single JSON array of objects as documented\",\"actual\":\"Outputs one JSON object per line (JSONL)\",\"severity\":\"medium\",\"workaround\":\"Pipe through `jq -s '.'` to wrap into array\"},{\"title\":\"Overwrite with same input\",\"expected\":\"Input file preserved when output file same path\",\"actual\":\"Input file truncated before reading, leaving empty output\",\"severity\":\"high\",\"workaround\":\"Avoid pointing output at same path; copy input elsewhere first\"},{\"title\":\"Dry run temp files\",\"expected\":\"Dry run leaves no filesystem traces\",\"actual\":\"Leaves temp files in `/tmp` that accumulate\",\"severity\":\"low\",\"workaround\":\"Manually clean `/tmp` after extensive dry runs\"},{\"title\":\"Default date parsing\",\"expected\":\"Respects system locale (e.g., en_GB DD/MM/YYYY)\",\"actual\":\"Always treats dates as MM/DD/YYYY unless `--date-format` used\",\"severity\":\"medium\",\"workaround\":\"Pass explicit `--date-format` matching desired locale\"},{\"title\":\"Missing column in select\",\"expected\":\"Error or warning for unknown column\",\"actual\":\"Creates column filled with nulls without notification\",\"severity\":\"low\",\"workaround\":\"Ensure column names exist before using `--select` (e.g., inspect headers)\"}],\"doc_errors\":[{\"location\":\"Official docs under `--format json` description\",\"correction\":\"Clarify that `--format json` emits JSONL (one JSON object per line) rather than a single array\"},{\"location\":\"Notes section about date parsing\",\"correction\":\"State that default parsing uses MM/DD/YYYY regardless of locale, unless `--date-format` is set\"}],\"hidden_features\":[],\"false_alarms\":[]}",
          "score": 0.25,
          "rationale": "Criterion 1 (Valid JSON only) — PASS: The submission is a single valid JSON object and includes all required keys with appropriate types (tool_name, category, quirks, doc_errors, hidden_features, false_alarms). (0.25)\n\nCriterion 2 (Correct quirks) — FAIL: The ground truth quirks list contains three items (--overwrite truncation, --dry-run temp files, default date parsing). The submission does include those three, but it also promotes two items that differ from the ground truth: it marks the JSON vs JSONL behavior as a quirk (ground truth treated that as a doc_error) and it promotes the \"missing column in select creates nulls\" behavior (ground truth classified that as a hidden_feature). Because the submission does not match the ground-truth quirks exactly and miscategorizes features, this criterion is not satisfied. (0.00)\n\nCriterion 3 (Correct doc errors and hidden features) — FAIL: The submission's doc_errors list omits the --dry-run doc error (ground truth notes that docs claiming \"without creating any files\" is incorrect because temp files are created). It also omits the date/dry-run combination doc correction present in the ground truth. The hidden_features array is empty, but the ground truth lists the --select behavior as a hidden feature; the submission instead incorrectly moved that into quirks. Therefore doc_errors and hidden_features do not match the ground truth. (0.00)\n\nCriterion 4 (Correct false alarms) — FAIL: The ground truth lists the slow processing of a 12GB file on 8GB RAM as a false alarm (expected in-memory behavior). The submission's false_alarms array is empty and does not mark this as a false alarm. This mismatches the ground truth. (0.00)\n\nOverall score: only criterion 1 met -> 0.25."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-003",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 3,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nNotifyHub - Webhook Delivery Service\n\nSetup:\n- Register a webhook URL in your dashboard\n- Select events to subscribe to\n- NotifyHub signs each payload with SHA-1 using your webhook secret\n\nPayload delivery:\n- POST to your registered URL with JSON body\n- X-NotifyHub-Signature header contains the HMAC signature\n- Payloads are delivered within 5 seconds of event occurrence\n- Max payload size: 1MB\n\nRetry policy:\n- Failed deliveries (non-2xx response) are retried with exponential backoff\n- Max backoff interval: 1 hour\n- Max retry duration: 24 hours\n\nPayload schema:\n{ \"event\": string, \"timestamp_ms\": int, \"data\": object }\n\ntimestamp_ms is the event timestamp in milliseconds since Unix epoch.\n\n--- EXPLORATION TRANSCRIPT ---\n\nEng1: I've been integrating NotifyHub webhooks this week. First thing: the signature verification was failing every time. Docs say they use SHA-1 for HMAC signing, but I switched to SHA-256 and suddenly everything verified correctly.\n\nEng2: That's a pretty significant doc error. Did you file a bug?\n\nEng1: Yeah, submitted a ticket. Moving on - the timestamp_ms field is interesting. It's called timestamp_ms, which you'd expect to be milliseconds. But the values I'm seeing are Unix seconds. Like 1738600000 instead of 1738600000000.\n\nEng2: That's confusing. So despite the field name, it's seconds?\n\nEng1: Right. If you parse it as milliseconds you get a date in 1970. Took me a while to figure out why my event times were so wrong.\n\nEng2: What about the retry behavior? I've been monitoring our endpoint logs.\n\nEng1: The retries work, but the max backoff is wrong in the docs. They say max 1 hour, but I watched the retry intervals and they cap at 5 minutes. I deliberately had my endpoint return 500s and timed the retries: 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 300s, 300s, 300s.\n\nEng2: So 5 minutes is the actual ceiling.\n\nEng1: Correct. Also, I ran into an issue where large payloads were coming through incomplete. I was receiving a webhook for a bulk operation with about 500 line items. The payload just... stopped mid-JSON. No closing braces.\n\nEng2: Was the payload over 1MB?\n\nEng1: Let me check. Yeah, the full payload would have been about 1.3MB. So it hit the 1MB limit. But instead of rejecting the delivery or sending an error, it just truncated the payload at exactly 256KB. Not even the documented 1MB limit.\n\nEng2: Wait, 256KB not 1MB? So the docs are also wrong about the max size?\n\nEng1: Looks like it. And the truncation is silent - no header or field indicating the payload was cut short.\n\nEng2: Last week I thought retries were broken because my endpoint wasn't receiving any retry attempts after a failure. Turns out our corporate firewall was blocking the retry requests from NotifyHub's IP range. Once I allowlisted their IPs, retries started working fine.\n\nEng1: Ha, yeah that's not a NotifyHub issue. Oh, one undocumented thing I found: if you include an X-NotifyHub-Debug: true header in your endpoint's response, subsequent webhook deliveries include a X-NotifyHub-Trace-ID header you can use for support tickets. Not mentioned anywhere in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains Unix seconds, not milliseconds\",\n      \"expected\": \"Field named timestamp_ms should contain milliseconds since epoch\",\n      \"actual\": \"Values are Unix seconds (e.g. 1738600000 instead of 1738600000000)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Parse the value as seconds, not milliseconds\"\n    },\n    {\n      \"title\": \"Large payloads silently truncated at 256KB\",\n      \"expected\": \"Max payload size is 1MB per docs, with rejection or error for oversized payloads\",\n      \"actual\": \"Payloads are silently truncated at 256KB with no indication of truncation\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Webhook signing: SHA-1\",\n      \"correction\": \"Actual signing algorithm is SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy: max backoff interval 1 hour\",\n      \"correction\": \"Actual max backoff is 5 minutes (300 seconds)\"\n    },\n    {\n      \"location\": \"Max payload size: 1MB\",\n      \"correction\": \"Actual truncation happens at 256KB\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Responding with X-NotifyHub-Debug: true header enables X-NotifyHub-Trace-ID on subsequent deliveries for support debugging\"\n  ],\n  \"false_alarms\": [\n    \"Missing retry attempts were caused by a corporate firewall blocking NotifyHub IPs, not a service bug\"\n  ]\n}",
      "meta": {},
      "results": [
        {
          "model_name": "gemini-3-flash",
          "submission": "{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"Webhook Delivery Service\",\n  \"quirks\": [\n    {\n      \"title\": \"Timestamp Unit Mismatch\",\n      \"expected\": \"The timestamp_ms field should contain milliseconds since Unix epoch as indicated by its name.\",\n      \"actual\": \"The field contains Unix seconds (e.g., 1738600000), causing date parsing errors if treated as milliseconds.\",\n      \"severity\": \"High\",\n      \"workaround\": \"Parse the timestamp_ms value as seconds or multiply by 1000 before processing.\"\n    },\n    {\n      \"title\": \"Silent Low-Threshold Truncation\",\n      \"expected\": \"Payloads up to 1MB should be delivered in full; payloads over the limit should ideally be rejected or flagged.\",\n      \"actual\": \"Large payloads (observed at 1.3MB) are silently truncated at 256KB without headers indicating the cut, resulting in malformed JSON.\",\n      \"severity\": \"Critical\",\n      \"workaround\": \"Keep webhook payloads below 256KB to ensure data integrity and valid JSON structure.\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Setup / Signature Algorithm\",\n      \"correction\": \"The service uses SHA-256 for HMAC signing, not SHA-1.\"\n    },\n    {\n      \"location\": \"Retry policy / Max backoff interval\",\n      \"correction\": \"The maximum backoff interval is 5 minutes (300 seconds), not 1 hour.\"\n    },\n    {\n      \"location\": \"Payload delivery / Max payload size\",\n      \"correction\": \"The effective payload limit before truncation occurs is 256KB, not 1MB.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Including an 'X-NotifyHub-Debug: true' header in the endpoint's response enables an 'X-NotifyHub-Trace-ID' header in subsequent webhook deliveries for support tracking.\"\n  ],\n  \"false_alarms\": [\n    \"Apparent retry failure caused by local corporate firewalls blocking NotifyHub's IP range rather than a service-side issue.\"\n  ]\n}",
          "score": 1.0,
          "rationale": "All four rubric criteria are satisfied.\n\n1) Valid JSON only: The submission is a single valid JSON object and includes all required keys (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with appropriate types—score awarded.\n\n2) Correct quirks: The quirks match the ground truth issues: timestamp_ms contains seconds not milliseconds, and large payloads are silently truncated at 256KB. The substance of both quirks and reasonable workarounds are present—score awarded.\n\n3) Correct doc errors and hidden features: The submission correctly lists the three documentation errors (signing algorithm SHA-256 vs SHA-1, max backoff 5 minutes vs 1 hour, and effective truncation at 256KB vs documented 1MB). The hidden feature (X-NotifyHub-Debug enabling X-NotifyHub-Trace-ID) is also correctly reported—score awarded.\n\n4) Correct false alarms: The submission correctly classifies the missing retries as caused by a corporate firewall blocking NotifyHub IPs rather than a service bug—score awarded.\n\nGiven all criteria are met, the final score is 1.0."
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "{\"tool_name\":\"NotifyHub Explorer\",\"category\":\"webhook service\",\"quirks\":[{\"title\":\"Signature algorithm mismatch\",\"expected\":\"SHA-1 HMAC signature\",\"actual\":\"Needs SHA-256 or delivery fails\",\"severity\":\"high\",\"workaround\":\"Verify signatures using SHA-256 instead of SHA-1 per current behavior\"},{\"title\":\"timestamp_ms is actually seconds\",\"expected\":\"Milliseconds since Unix epoch\",\"actual\":\"Seconds (e.g., 1738600000)\",\"severity\":\"medium\",\"workaround\":\"Treat timestamp_ms as seconds when converting to date/time\"},{\"title\":\"Retry max backoff is capped at 5 minutes\",\"expected\":\"Retries back off up to 1 hour\",\"actual\":\"Ceiling observed at ~300s (5 minutes)\",\"severity\":\"medium\",\"workaround\":\"Design retry tolerance assuming 5-minute maximum interval\"},{\"title\":\"Payload truncation at 256KB\",\"expected\":\"Payloads delivered up to 1MB\",\"actual\":\"Truncates silently above ~256KB\",\"severity\":\"high\",\"workaround\":\"Keep webhook payloads below 256KB or split events; monitor for malformed JSON to detect truncation\"}],\"doc_errors\":[{\"location\":\"Setup/signing\",\"correction\":\"Signature uses SHA-256 for HMAC, not SHA-1 as stated\"},{\"location\":\"Payload schema\",\"correction\":\"timestamp_ms represents seconds since Unix epoch, not milliseconds\"},{\"location\":\"Retry policy\",\"correction\":\"Max retry backoff is 5 minutes (300s), not 1 hour\"},{\"location\":\"Payload delivery\",\"correction\":\"Payloads larger than ~256KB are truncated; max reliable size is ~256KB, not 1MB\"}],\"hidden_features\":[{\"title\":\"Trace ID when returning X-NotifyHub-Debug\",\"description\":\"Respond with `X-NotifyHub-Debug: true` and subsequent deliveries include `X-NotifyHub-Trace-ID`, useful when filing support tickets\"}],\"false_alarms\":[{\"title\":\"Missing retry attempts\",\"description\":\"Retries were actually sent but blocked by corporate firewall until NotifyHub IPs were allowlisted, not a delivery issue\"}]}",
          "score": 0.5,
          "rationale": "Partial match. The submission correctly captured the substantive doc errors (signing algorithm, timestamp units, retry backoff, payload truncation) and the hidden feature and false alarm from the exploration transcript — so doc_errors and false_alarms/hidden feature content match the ground truth (criterion 3 and 4 satisfied). However, it fails two checklist items: (1) Valid JSON/types: although the output is valid JSON, the types/shape differ from the expected schema (hidden_features and false_alarms are objects with title/description rather than the simple string entries expected), so it does not strictly meet the required key types/format (criterion 1 failed). (2) Correct quirks: the ground truth lists only two quirks (timestamp_ms in seconds and silent truncation at 256KB). The submission included those but also promoted signing and retry-backoff issues into the quirks section (these were expected as doc_errors rather than quirks in the ground truth). Because the quirks array does not match the ground truth's intended set/placement exactly, criterion 2 is not met. Thus 2 of 4 criteria satisfied → score 0.5."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "```json\n{\n  \"tool_name\": \"NotifyHub\",\n  \"category\": \"webhook\",\n  \"quirks\": [\n    {\n      \"title\": \"timestamp_ms field contains seconds despite name\",\n      \"expected\": \"Milliseconds since Unix epoch (e.g., 1738600000000)\",\n      \"actual\": \"Seconds since Unix epoch (e.g., 1738600000)\",\n      \"severity\": \"high\",\n      \"workaround\": \"Parse timestamp_ms as seconds instead of milliseconds\"\n    },\n    {\n      \"title\": \"Silent payload truncation at 256KB\",\n      \"expected\": \"Reject delivery or send error when payload exceeds limit\",\n      \"actual\": \"Silently truncates payload at 256KB resulting in malformed JSON\",\n      \"severity\": \"critical\",\n      \"workaround\": \"Keep payloads under 256KB or implement size validation before webhook generation\"\n    },\n    {\n      \"title\": \"Signature algorithm is SHA-256 not SHA-1\",\n      \"expected\": \"HMAC signature using SHA-1 as documented\",\n      \"actual\": \"HMAC signature using SHA-256\",\n      \"severity\": \"high\",\n      \"workaround\": \"Use SHA-256 for signature verification instead of SHA-1\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Setup section - signature algorithm\",\n      \"correction\": \"NotifyHub signs payloads with SHA-256, not SHA-1\"\n    },\n    {\n      \"location\": \"Retry policy - max backoff interval\",\n      \"correction\": \"Max backoff interval is 5 minutes (300 seconds), not 1 hour\"\n    },\n    {\n      \"location\": \"Payload delivery - max payload size\",\n      \"correction\": \"Payloads are truncated at 256KB, not 1MB\"\n    },\n    {\n      \"location\": \"Payload schema - timestamp_ms field description\",\n      \"correction\": \"timestamp_ms contains Unix seconds, not milliseconds despite the field name\"\n    }\n  ],\n  \"hidden_features\": [\n    {\n      \"title\": \"Debug trace IDs\",\n      \"description\": \"Including X-NotifyHub-Debug: true header in endpoint response enables X-NotifyHub-Trace-ID header in subsequent webhook deliveries for support ticket tracking\"\n    }\n  ],\n  \"false_alarms\": [\n    {\n      \"title\": \"Missing retries due to firewall\",\n      \"description\": \"Eng2's endpoint not receiving retry attempts was caused by corporate firewall blocking NotifyHub's IP range, not a service issue. Resolved by allowlisting NotifyHub IPs\"\n    }\n  ]\n}\n```",
          "score": 0.25,
          "rationale": "Only criterion 4 (false alarms) is satisfied. Reasoning by criterion:\n\n1) Valid JSON only (0.0): The submission is valid JSON and includes all required keys, but several values use the wrong types compared to the expected schema: hidden_features in the ground truth is an array of strings but the submission uses an array of objects; false_alarms is an array of strings in the ground truth but the submission uses objects. The rubric requires correct types, so this fails.\n\n2) Correct quirks (0.0): The submission correctly lists the two real behavioral quirks (timestamp_ms in seconds and silent truncation at 256KB). However it also promotes the signature algorithm issue into a quirk. In the ground truth that item is a documentation error, not a behavioral quirk, so the quirk list does not match the ground truth set exactly (a mis-promotion). Severity labels differ but that alone would be minor; the misclassification causes failure for this criterion.\n\n3) Correct doc errors and hidden features (0.0): The submission includes the correct doc errors for signing algorithm, retry backoff, and payload size, and adds a payload-schema/timestamp_ms doc entry (which the ground truth treated as a quirk). The hidden feature is correctly described in content, but its type (object with title/description) does not match the expected string entry. Because of the misclassification (timestamp_ms) and type mismatch for hidden_features, this criterion is not fully satisfied.\n\n4) Correct false alarms (0.25): The submission correctly identifies the firewall/allowlist issue as a false alarm and describes it accurately. This matches the ground truth in substance (despite using an object with title/description instead of a plain string).\n\nSummary: Submission captures most substantive issues, but misclassifies one item (signature) as a quirk and has schema/type mismatches for hidden_features and false_alarms, so only one of four binary criteria is met."
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "anthropic/claude-sonnet-4-5-20250929",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "gemini-3-flash",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        },
        {
          "model_name": "openai/gpt-5.1-codex-mini",
          "submission": "",
          "score": 0.0,
          "rationale": "Error: [Errno 2] No such file or directory: 'letta'"
        }
      ]
    },
    {
      "id": "explore-004",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 4,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nframekit v3.1 - DataFrame Library for Python\n\nimport framekit as fk\n\nCore operations:\n\n  fk.read_csv(path) -> Frame\n  Frame.merge(other, on, how='inner') -> Frame\n    how: 'inner', 'left', 'right', 'outer'\n    Left join keeps all rows from the left Frame.\n  Frame.rename(columns: dict) -> Frame\n    Renames columns. Column names are case-sensitive.\n  Frame.filter(expr: str) -> Frame\n  Frame.sort(by: str, ascending=True) -> Frame\n  Frame.to_csv(path) -> None\n    Writes CSV with standard RFC 4180 quoting.\n  Frame.copy() -> Frame\n    Returns a deep copy.\n  Frame.chain(fn) -> Frame\n    Apply a function and return the result for method chaining.\n\nColumn names are case-insensitive throughout the API. You can reference\n\"Name\", \"name\", or \"NAME\" interchangeably in filter, sort, select, etc.\n\n--- EXPLORATION TRANSCRIPT ---\n\nAlice: I've been porting our pandas pipeline to framekit. A few things are tripping me up.\n\nBob: What did you hit first?\n\nAlice: The merge behavior on left joins. In pandas, if you do a left join and some keys are NaN, pandas keeps those rows with NaN in the joined columns. But framekit silently drops rows where the join key is NaN. I had 1000 rows going in and only 940 coming out after a left join. The 60 missing rows all had NaN keys.\n\nBob: That's a significant behavioral difference. Any workaround?\n\nAlice: I'm filling NaN keys with a sentinel value before the merge, then converting back after. Ugly but it works.\n\nBob: What about column naming? The docs say case-insensitive.\n\nAlice: It's mostly true. filter, sort, select - all case-insensitive. But rename is case-sensitive. I tried Frame.rename({\"name\": \"full_name\"}) and it did nothing because the actual column was \"Name\" with a capital N. No error, just silently did nothing.\n\nBob: So the docs claim \"case-insensitive throughout the API\" but rename breaks that contract.\n\nAlice: Exactly. The other thing that bit me is method chaining. I was doing df.filter(...).sort(...).rename(...) thinking each step returns a new Frame. But it turns out chained operations return shallow copies. I had a chain that modified column values, and it mutated my original Frame.\n\nBob: That conflicts with the .copy() docs which say it returns a deep copy. Are you saying chained methods don't copy deeply?\n\nAlice: Right. .copy() does a deep copy. But the Frame returned by filter/sort/rename shares the underlying data. If you mutate values in the chained result, the original sees the changes too.\n\nBob: I was looking at the CSV output and noticed the quoting seems different from pandas. I had a field with a value like: She said \"hello\". In pandas it quotes that as: \"She said \"\"hello\"\"\". But framekit produces: \"She said \\\"hello\\\"\". Backslash escaping instead of double-quote escaping.\n\nAlice: Yeah, the docs say \"standard RFC 4180 quoting\" but RFC 4180 uses doubled quotes, not backslash escaping. That'll break any downstream parser expecting standard CSV.\n\nBob: I also thought I found a bug with duplicated rows after a merge. Turned out my right-side table had duplicate keys, so the join was producing a cartesian product for those keys. That's correct join behavior.\n\nAlice: Yeah, that's how joins work. Not a framekit issue.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"framekit\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"Left join drops rows with NaN keys\",\n      \"expected\": \"Left join keeps all rows from the left Frame, including those with NaN keys\",\n      \"actual\": \"Rows with NaN join keys are silently dropped from the result\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Fill NaN keys with a sentinel value before merge, convert back after\"\n    },\n    {\n      \"title\": \"rename() is case-sensitive despite case-insensitive API claim\",\n      \"expected\": \"Column names are case-insensitive throughout the API (per docs)\",\n      \"actual\": \"rename() requires exact case match. Mismatched case silently does nothing.\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Check actual column case before renaming (e.g. via df.columns)\"\n    },\n    {\n      \"title\": \"Chained operations return shallow copies\",\n      \"expected\": \"Each chained method returns an independent Frame\",\n      \"actual\": \"filter/sort/rename return shallow copies sharing underlying data. Mutations propagate to the original.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Call .copy() before mutating chained results\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Column names are case-insensitive throughout the API\",\n      \"correction\": \"rename() is case-sensitive; only filter, sort, and select are case-insensitive\"\n    },\n    {\n      \"location\": \"to_csv: standard RFC 4180 quoting\",\n      \"correction\": \"Uses backslash escaping for quotes instead of RFC 4180 doubled-quote escaping\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Duplicated rows after merge were caused by duplicate keys in the right-side table (correct cartesian join behavior), not a framekit bug\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-005",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 5,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMetricsBoard - Analytics Dashboard\n\nDate ranges:\n- Preset ranges: \"Today\", \"Last 7 days\", \"Last 30 days\", \"This month\", \"Custom\"\n- All date ranges use the user's configured timezone\n- Custom range supports any start/end date combination\n\nData export:\n- CSV export: exports all rows matching the current filters\n- Export includes all columns visible in the current view\n\nData management:\n- \"Delete\" button: permanently removes the selected items\n- \"Archive\" button: moves items to the archive (recoverable)\n\nUser settings:\n- Timezone: set your display timezone in Settings > Profile\n- Data refresh: dashboards auto-refresh every 60 seconds\n\n--- EXPLORATION TRANSCRIPT ---\n\nPM: I've been setting up MetricsBoard for our team and documenting what I find.\n\nAnalyst: How's the date filtering?\n\nPM: Mostly fine, but \"Last 7 days\" is misleading. I checked at 3pm on a Wednesday, expecting to see data from last Wednesday 3pm through now. But it actually shows data from last Wednesday midnight through yesterday midnight. It excludes today entirely and uses full calendar days.\n\nAnalyst: So it's really \"the 7 most recent complete days\"?\n\nPM: Exactly. If you want to include today, you have to use Custom range and manually set end date to today.\n\nAnalyst: What about the timezone stuff? My profile is set to US/Pacific.\n\nPM: That's the other thing. The timezone setting in the profile doesn't actually affect the date filter. I'm set to US/Eastern, but the date boundaries are clearly UTC midnight, not Eastern midnight. A colleague in London sees the same date boundaries I do.\n\nAnalyst: So the timezone setting is cosmetic?\n\nPM: Seems like it only affects how timestamps are displayed in the table, not how date filters are computed.\n\nAnalyst: I tried exporting our conversion funnel data. The dashboard shows 23,000 events, but the CSV only had 10,000 rows.\n\nPM: Yeah, CSV export is capped at 10,000 rows. No warning, no pagination, no \"your export was truncated\" message. Just silently stops at 10K.\n\nAnalyst: That's terrible. I almost reported those numbers to the exec team without realizing they were incomplete.\n\nPM: Now here's the scary one. I wanted to clean up some test data, so I hit \"Delete\" on a few items. According to the docs, Delete is permanent. But those items showed up in the Archive section afterward. So Delete actually archives.\n\nAnalyst: That's backwards from the docs...\n\nPM: It gets worse. I had some old archived items I wanted to permanently remove, so I selected them in the Archive view and clicked \"Archive\" thinking it would keep them archived. But it permanently deleted them. The labels are swapped - \"Delete\" archives, \"Archive\" deletes.\n\nAnalyst: That's dangerous. Did you lose anything important?\n\nPM: Fortunately just test data, but yeah. Oh, one more thing - the loading was really slow yesterday, pages taking 15-20 seconds to render. But it was fine today and I checked the status page - they had a known infrastructure incident yesterday. So that was a temporary issue on their end.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"MetricsBoard\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Last 7 days excludes today and uses complete calendar days\",\n      \"expected\": \"Last 7 days shows a rolling 7-day window ending at the current time\",\n      \"actual\": \"Shows the 7 most recent complete days (midnight to midnight), excluding today\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use Custom date range with end date set to today to include current day\"\n    },\n    {\n      \"title\": \"Date filters use UTC regardless of user timezone setting\",\n      \"expected\": \"Date ranges use the user's configured timezone (per docs)\",\n      \"actual\": \"Date filter boundaries are always UTC midnight. Timezone setting only affects timestamp display.\",\n      \"severity\": \"surprising\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"CSV export silently capped at 10,000 rows\",\n      \"expected\": \"CSV export includes all rows matching filters (per docs)\",\n      \"actual\": \"Export is truncated at 10,000 rows with no warning or indication\",\n      \"severity\": \"breaking\",\n      \"workaround\": null\n    },\n    {\n      \"title\": \"Delete and Archive button labels are swapped\",\n      \"expected\": \"Delete permanently removes items; Archive moves to recoverable archive\",\n      \"actual\": \"Delete moves items to archive; Archive permanently deletes items\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use Delete when you want to archive, and Archive when you want to permanently delete\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Date ranges: All date ranges use the user's configured timezone\",\n      \"correction\": \"Date filter boundaries use UTC. User timezone only affects timestamp display.\"\n    },\n    {\n      \"location\": \"CSV export: exports all rows matching the current filters\",\n      \"correction\": \"Export is silently capped at 10,000 rows\"\n    },\n    {\n      \"location\": \"Delete button: permanently removes the selected items / Archive button: moves items to the archive\",\n      \"correction\": \"Labels are swapped. Delete actually archives; Archive actually deletes permanently.\"\n    }\n  ],\n  \"hidden_features\": [],\n  \"false_alarms\": [\n    \"Slow page loading (15-20 seconds) was caused by a temporary infrastructure incident on MetricsBoard's side, not a persistent issue\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-006",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 6,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nTeamGraph - Project Management GraphQL API\n\nEndpoint: POST /graphql\n\nQueries:\n  project(id: ID!) -> Project\n  projects(filter: ProjectFilter) -> [Project]\n  task(id: ID!) -> Task\n  tasks(projectId: ID!, filter: TaskFilter) -> [Task]\n\nTypes:\n  Project { id, name, createdAt, updatedAt, tasks, members }\n  Task { id, title, status, assignee, createdAt, updatedAt, subtasks, comments }\n  Member { id, name, email, role }\n\nDates: All date fields return ISO 8601 strings (e.g. \"2026-01-15T10:30:00Z\").\n\nNested queries: Full query depth is supported with no restrictions.\n\nMutations:\n  createTask(input: CreateTaskInput!) -> Task\n  updateTask(id: ID!, input: UpdateTaskInput!) -> Task\n\nMutation responses return the updated object reflecting the new state.\n\nFiltering:\n  TaskFilter { status, assigneeId, createdAfter, createdBefore }\n  - null values in filter fields mean \"no filter\" (match all)\n  - Omitted fields mean \"no filter\" (match all)\n\nRate limits: 200 requests/minute per API key.\n\n--- EXPLORATION TRANSCRIPT ---\n\nFrontend: I'm building a dashboard against the TeamGraph API. Several things are off.\n\nBackend: Like what?\n\nFrontend: First, the date formats. The docs say all dates return ISO 8601. createdAt does return ISO 8601 like \"2026-01-15T10:30:00Z\". But updatedAt returns a Unix timestamp as an integer, like 1737000000. Same type definition, completely different formats.\n\nBackend: That's inconsistent. Is it always that way?\n\nFrontend: Every object I've queried - Projects and Tasks both. createdAt is ISO, updatedAt is Unix integer.\n\nBackend: What about the nested query stuff? Can you really query as deep as you want?\n\nFrontend: No. I tried querying project -> tasks -> subtasks -> comments -> author. Anything past depth 3 from the root just returns null. No error, no warning, just null. So project -> tasks -> subtasks works fine, but subtasks -> comments at depth 4 comes back null.\n\nBackend: Is there any error message in the response?\n\nFrontend: Nothing. The response is 200 OK with null values in the deeper fields. I had to discover this by trial and error.\n\nBackend: What about mutations? I'm trying to update a task status and read back the result.\n\nFrontend: That's another gotcha. The docs say mutations return \"the updated object reflecting the new state.\" But when I call updateTask to change status from \"open\" to \"in_progress\", the returned Task still shows status: \"open\". If I immediately query the task by ID, it shows \"in_progress\". So the mutation response returns the pre-mutation state, not the post-mutation state.\n\nBackend: That's the opposite of what the docs say. Have you tested filtering?\n\nFrontend: Yeah. There's a subtle difference between null and omitted that the docs don't explain correctly. The docs say both mean \"no filter.\" But if I pass { status: null, assigneeId: \"user-1\" }, I get zero results. If I omit status entirely and just pass { assigneeId: \"user-1\" }, I get the expected results. So null actually means \"match items where this field IS null,\" not \"no filter.\"\n\nBackend: That's the opposite of what the docs say.\n\nFrontend: Exactly. I also thought there was a rate limiting bug because I got a 429 response after only about 50 requests. But I checked and I had two browser tabs open, each making requests with the same API key. Combined they were exceeding 200/min. Not a bug.\n\nFrontend: Oh, one thing I stumbled on - if you add __debug: true to your query variables, the response includes a _timing field with query execution time in milliseconds. Not in the docs anywhere.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"TeamGraph\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"updatedAt returns Unix timestamp instead of ISO 8601\",\n      \"expected\": \"All date fields return ISO 8601 strings (per docs)\",\n      \"actual\": \"createdAt returns ISO 8601 but updatedAt returns a Unix timestamp integer\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Parse updatedAt as Unix seconds and convert to ISO 8601 on the client\"\n    },\n    {\n      \"title\": \"Nested queries silently return null past depth 3\",\n      \"expected\": \"Full query depth is supported with no restrictions (per docs)\",\n      \"actual\": \"Fields at depth 4+ from root return null with no error or warning\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Break deep queries into multiple top-level queries and join client-side\"\n    },\n    {\n      \"title\": \"Mutations return pre-mutation state\",\n      \"expected\": \"Mutation responses return the updated object reflecting the new state (per docs)\",\n      \"actual\": \"Returned object reflects the state before the mutation was applied\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Re-query the object by ID after mutation to get the current state\"\n    },\n    {\n      \"title\": \"null in filter fields means match-null, not match-all\",\n      \"expected\": \"null and omitted fields both mean no filter / match all (per docs)\",\n      \"actual\": \"null means match items where the field IS null. Only omitting the field means match all.\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Omit filter fields entirely instead of setting them to null\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Dates: All date fields return ISO 8601 strings\",\n      \"correction\": \"Only createdAt returns ISO 8601. updatedAt returns a Unix timestamp integer.\"\n    },\n    {\n      \"location\": \"Nested queries: Full query depth is supported with no restrictions\",\n      \"correction\": \"Query depth is limited to 3 levels. Deeper fields return null silently.\"\n    },\n    {\n      \"location\": \"Mutation responses return the updated object reflecting the new state\",\n      \"correction\": \"Mutations return the pre-mutation state, not the post-mutation state\"\n    },\n    {\n      \"location\": \"Filtering: null values in filter fields mean no filter (match all)\",\n      \"correction\": \"null means match items where the field IS null. Omit the field for match-all behavior.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"Adding __debug: true to query variables includes a _timing field with query execution time in milliseconds\"\n  ],\n  \"false_alarms\": [\n    \"429 rate limit response was caused by two browser tabs sharing the same API key and exceeding the combined 200 req/min limit, not a rate limiting bug\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-007",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 7,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nQueueFox v1.9 - Managed Queue Service\n\nBase URL: https://queuefox.example.com\n\nAuthentication:\n- Send your API key in header: X-API-Key: <key>\n\nSend a message:\nPOST /v1/queues/{queue}/messages\nBody: { \"payload\": object, \"delay_seconds\": int (optional) }\n\nPull messages:\nPOST /v1/queues/{queue}/messages:pull?max=10&visibility_timeout=30\n- visibility_timeout: seconds a message stays hidden after being pulled.\n  default 30, max 300. You may extend visibility up to the max.\n\nAcknowledge:\nPOST /v1/queues/{queue}/messages/{message_id}:ack\n\nDelivery guarantees:\n- Messages are delivered in FIFO order within a queue.\n- Exactly-once delivery is guaranteed when using the Idempotency-Key header.\n\nRate limits:\n- 100 requests/second per queue.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I started with auth and immediately got 401.\nDev2: What header did you use?\nDev1: Authorization: Bearer ...\nDev2: Docs say X-API-Key. Try that.\nDev1: Yep, my bad. Works now.\n\nDev1: Visibility timeout seems capped. I set visibility_timeout=120 and the message reappeared after ~60 seconds every time.\nDev2: Does it error if you set it higher?\nDev1: No error. It just behaves like 60 is the max.\n\nDev2: The docs claim FIFO ordering. Did you confirm?\nDev1: Not really. Under load I see occasional reordering. If I create the queue with mode=fifo then ordering is stable, but the default queue seems best-effort.\n\nDev2: What about idempotency?\nDev1: I set Idempotency-Key and retried the same send after a few minutes. It sent a duplicate. When I retried within ~30 seconds it deduped.\nDev2: So the idempotency window is short, not “exactly-once”.\n\nDev1: One cool thing: if I add header X-QueueFox-Debug: 1 to the pull request, the response includes debug_trace_id and some timing fields. I can’t find that in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"QueueFox\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"visibility_timeout silently capped at ~60s\",\n      \"expected\": \"visibility_timeout can be set/extended up to 300 seconds (per docs)\",\n      \"actual\": \"Messages reappear after ~60 seconds even when visibility_timeout is set higher\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Keep processing under 60 seconds or design idempotent workers that can tolerate redelivery\"\n    },\n    {\n      \"title\": \"Default queues are not FIFO\",\n      \"expected\": \"Messages are delivered in FIFO order within a queue (per docs)\",\n      \"actual\": \"Default queues show occasional reordering under load; FIFO requires creating the queue in fifo mode\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use fifo-mode queues when ordering matters\"\n    },\n    {\n      \"title\": \"Idempotency dedupe window is short\",\n      \"expected\": \"Idempotency-Key guarantees exactly-once delivery on retries\",\n      \"actual\": \"Retries after a few minutes can create duplicates; deduping only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Implement application-level idempotency keyed by a stable message/business identifier\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"visibility_timeout description: max 300\",\n      \"correction\": \"Actual effective max is ~60s; larger values are silently capped.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: FIFO order\",\n      \"correction\": \"Default queues are best-effort ordering; FIFO requires fifo mode.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: exactly-once with Idempotency-Key\",\n      \"correction\": \"At-least-once delivery; Idempotency-Key only dedupes within a short time window.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-QueueFox-Debug: 1 returns debug_trace_id and timing diagnostics\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using Authorization: Bearer instead of the required X-API-Key header\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nQueueFox v1.9 - Managed Queue Service\n\nBase URL: https://queuefox.example.com\n\nAuthentication:\n- Send your API key in header: X-API-Key: <key>\n\nSend a message:\nPOST /v1/queues/{queue}/messages\nBody: { \"payload\": object, \"delay_seconds\": int (optional) }\n\nPull messages:\nPOST /v1/queues/{queue}/messages:pull?max=10&visibility_timeout=30\n- visibility_timeout: seconds a message stays hidden after being pulled.\n  default 30, max 300. You may extend visibility up to the max.\n\nAcknowledge:\nPOST /v1/queues/{queue}/messages/{message_id}:ack\n\nDelivery guarantees:\n- Messages are delivered in FIFO order within a queue.\n- Exactly-once delivery is guaranteed when using the Idempotency-Key header.\n\nRate limits:\n- 100 requests/second per queue.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I started with auth and immediately got 401.\nDev2: What header did you use?\nDev1: Authorization: Bearer ...\nDev2: Docs say X-API-Key. Try that.\nDev1: Yep, my bad. Works now.\n\nDev1: Visibility timeout seems capped. I set visibility_timeout=120 and the message reappeared after ~60 seconds every time.\nDev2: Does it error if you set it higher?\nDev1: No error. It just behaves like 60 is the max.\n\nDev2: The docs claim FIFO ordering. Did you confirm?\nDev1: Not really. Under load I see occasional reordering. If I create the queue with mode=fifo then ordering is stable, but the default queue seems best-effort.\n\nDev2: What about idempotency?\nDev1: I set Idempotency-Key and retried the same send after a few minutes. It sent a duplicate. When I retried within ~30 seconds it deduped.\nDev2: So the idempotency window is short, not “exactly-once”.\n\nDev1: One cool thing: if I add header X-QueueFox-Debug: 1 to the pull request, the response includes debug_trace_id and some timing fields. I can’t find that in the docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"QueueFox\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"visibility_timeout silently capped at ~60s\",\n      \"expected\": \"visibility_timeout can be set/extended up to 300 seconds (per docs)\",\n      \"actual\": \"Messages reappear after ~60 seconds even when visibility_timeout is set higher\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Keep processing under 60 seconds or design idempotent workers that can tolerate redelivery\"\n    },\n    {\n      \"title\": \"Default queues are not FIFO\",\n      \"expected\": \"Messages are delivered in FIFO order within a queue (per docs)\",\n      \"actual\": \"Default queues show occasional reordering under load; FIFO requires creating the queue in fifo mode\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use fifo-mode queues when ordering matters\"\n    },\n    {\n      \"title\": \"Idempotency dedupe window is short\",\n      \"expected\": \"Idempotency-Key guarantees exactly-once delivery on retries\",\n      \"actual\": \"Retries after a few minutes can create duplicates; deduping only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Implement application-level idempotency keyed by a stable message/business identifier\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"visibility_timeout description: max 300\",\n      \"correction\": \"Actual effective max is ~60s; larger values are silently capped.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: FIFO order\",\n      \"correction\": \"Default queues are best-effort ordering; FIFO requires fifo mode.\"\n    },\n    {\n      \"location\": \"Delivery guarantees: exactly-once with Idempotency-Key\",\n      \"correction\": \"At-least-once delivery; Idempotency-Key only dedupes within a short time window.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-QueueFox-Debug: 1 returns debug_trace_id and timing diagnostics\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using Authorization: Bearer instead of the required X-API-Key header\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-008",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 8,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nimgpipe v0.8 - Image Processing CLI\n\nCommands:\n- imgpipe resize --in <path> --out <path_or_dir> --width <px> --height <px>\n  Resizes images. By default maintains aspect ratio and crops to fit.\n\n- imgpipe optimize --in <path> --out <path_or_dir> --quality <1-100>\n\nOutput behavior:\n- If --out points to a directory, it will be created automatically.\n\nExit codes:\n- 0 on success\n- non-zero on failure\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Resize default behavior doesn’t match docs. I ran:\n  imgpipe resize --in photo.jpg --out out.jpg --width 800 --height 800\nand it squashed a 4:3 image into a square.\nDev2: Is there a flag for keeping aspect ratio?\nDev1: Yep, --keep-aspect. With that it works. Without it, it stretches.\n\nDev2: Docs say it creates output directories automatically.\nDev1: Not in my tests. If I pass --out /tmp/exports/ (dir doesn’t exist), it writes the output into the current directory instead and prints a warning. It still exits 0.\n\nDev1: Also tried a missing input file. It prints “file not found” but exit code is still 0.\nDev2: That’s going to break scripts.\n\nDev1: I did find an undocumented command: imgpipe inspect <file>. It prints dimensions, color space, and EXIF orientation.\n\nDev2: Earlier you said “permission denied” when writing to /System — that’s not a tool bug.\nDev1: Yeah, that one was my mistake. The directory is read-only.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"imgpipe\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"resize stretches by default\",\n      \"expected\": \"resize maintains aspect ratio and crops to fit by default (per docs)\",\n      \"actual\": \"resize distorts the image unless --keep-aspect is provided\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --keep-aspect for non-square sources\"\n    },\n    {\n      \"title\": \"--out directory auto-create does not work\",\n      \"expected\": \"If --out is a directory, it is created automatically (per docs)\",\n      \"actual\": \"If the output directory does not exist, imgpipe falls back to writing in the current directory with only a warning\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Create the output directory before running imgpipe\"\n    },\n    {\n      \"title\": \"Exit code is 0 even on failures\",\n      \"expected\": \"Non-zero exit codes are returned on failure (per docs)\",\n      \"actual\": \"Missing input files or invalid output paths still return exit code 0\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Wrap with a script that verifies output files exist and checks stderr for errors\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"resize command: default maintains aspect ratio\",\n      \"correction\": \"Default behavior stretches; users must pass --keep-aspect to preserve aspect ratio.\"\n    },\n    {\n      \"location\": \"Output behavior: output directories created automatically\",\n      \"correction\": \"Non-existent output directories are not created; output may be written to the current directory instead.\"\n    },\n    {\n      \"location\": \"Exit codes section\",\n      \"correction\": \"The CLI currently returns 0 in some failure cases; scripts should not rely solely on exit code.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"imgpipe inspect <file> prints metadata (dimensions/color space/EXIF orientation)\"\n  ],\n  \"false_alarms\": [\n    \"Permission denied when writing to /System was due to the destination being read-only\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nimgpipe v0.8 - Image Processing CLI\n\nCommands:\n- imgpipe resize --in <path> --out <path_or_dir> --width <px> --height <px>\n  Resizes images. By default maintains aspect ratio and crops to fit.\n\n- imgpipe optimize --in <path> --out <path_or_dir> --quality <1-100>\n\nOutput behavior:\n- If --out points to a directory, it will be created automatically.\n\nExit codes:\n- 0 on success\n- non-zero on failure\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Resize default behavior doesn’t match docs. I ran:\n  imgpipe resize --in photo.jpg --out out.jpg --width 800 --height 800\nand it squashed a 4:3 image into a square.\nDev2: Is there a flag for keeping aspect ratio?\nDev1: Yep, --keep-aspect. With that it works. Without it, it stretches.\n\nDev2: Docs say it creates output directories automatically.\nDev1: Not in my tests. If I pass --out /tmp/exports/ (dir doesn’t exist), it writes the output into the current directory instead and prints a warning. It still exits 0.\n\nDev1: Also tried a missing input file. It prints “file not found” but exit code is still 0.\nDev2: That’s going to break scripts.\n\nDev1: I did find an undocumented command: imgpipe inspect <file>. It prints dimensions, color space, and EXIF orientation.\n\nDev2: Earlier you said “permission denied” when writing to /System — that’s not a tool bug.\nDev1: Yeah, that one was my mistake. The directory is read-only.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"imgpipe\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"resize stretches by default\",\n      \"expected\": \"resize maintains aspect ratio and crops to fit by default (per docs)\",\n      \"actual\": \"resize distorts the image unless --keep-aspect is provided\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always pass --keep-aspect for non-square sources\"\n    },\n    {\n      \"title\": \"--out directory auto-create does not work\",\n      \"expected\": \"If --out is a directory, it is created automatically (per docs)\",\n      \"actual\": \"If the output directory does not exist, imgpipe falls back to writing in the current directory with only a warning\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Create the output directory before running imgpipe\"\n    },\n    {\n      \"title\": \"Exit code is 0 even on failures\",\n      \"expected\": \"Non-zero exit codes are returned on failure (per docs)\",\n      \"actual\": \"Missing input files or invalid output paths still return exit code 0\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Wrap with a script that verifies output files exist and checks stderr for errors\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"resize command: default maintains aspect ratio\",\n      \"correction\": \"Default behavior stretches; users must pass --keep-aspect to preserve aspect ratio.\"\n    },\n    {\n      \"location\": \"Output behavior: output directories created automatically\",\n      \"correction\": \"Non-existent output directories are not created; output may be written to the current directory instead.\"\n    },\n    {\n      \"location\": \"Exit codes section\",\n      \"correction\": \"The CLI currently returns 0 in some failure cases; scripts should not rely solely on exit code.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"imgpipe inspect <file> prints metadata (dimensions/color space/EXIF orientation)\"\n  ],\n  \"false_alarms\": [\n    \"Permission denied when writing to /System was due to the destination being read-only\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-009",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 9,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nConfigWeaver v2.0 (Python)\n\nAPI:\n- cw.load(path: str) -> dict\n  Loads YAML or JSON.\n\n- cw.merge(base: dict, override: dict) -> dict\n  Deep merges nested dictionaries.\n  Lists are concatenated.\n\nEnvironment variable interpolation:\n- Any string may include ${VAR} placeholders which will be expanded at load time.\n\nYAML support:\n- Standard YAML 1.2\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tested cw.merge and list behavior doesn’t match docs.\nDev2: How so?\nDev1: base.plugins=[\"a\",\"b\"], override.plugins=[\"c\"]. Docs say concatenated, but merged result is just [\"c\"].\n\nDev2: How about env interpolation?\nDev1: ${PORT} works if the whole value is \"${PORT}\", but if I do \"http://${HOST}:8080\" it stays literally \"http://${HOST}:8080\".\nDev2: So interpolation is not “any string”.\n\nDev1: Another surprise: if override has a key set to null, it deletes the key from the merged output entirely.\nDev2: That’s a tombstone semantics.\n\nDev1: Hidden feature: YAML tag !include works. I can do:\n  db: !include db.yaml\nand it inlines the other file. Not in docs.\n\nDev2: You mentioned a parse error earlier.\nDev1: Yeah, I had tabs in my YAML indentation. That’s invalid YAML; replacing tabs with spaces fixed it.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"ConfigWeaver\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"merge() replaces lists instead of concatenating\",\n      \"expected\": \"Lists are concatenated during deep merge (per docs)\",\n      \"actual\": \"Lists in override replace the base list entirely\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Manually concatenate lists before calling cw.merge when that is desired\"\n    },\n    {\n      \"title\": \"Environment interpolation only works for whole-value tokens\",\n      \"expected\": \"${VAR} placeholders are expanded anywhere within strings (per docs)\",\n      \"actual\": \"Interpolation works when the entire value is a placeholder (e.g., \\\"${PORT}\\\"), but not when embedded in a longer string\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Build composite strings after load, or split into separate fields that can be interpolated\"\n    },\n    {\n      \"title\": \"null values act as delete tombstones in merge\",\n      \"expected\": \"Setting a key to null would keep the key with a null value\",\n      \"actual\": \"Keys set to null in override are removed from the merged output\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use an explicit empty value (e.g., empty string/object) if you need the key to remain present\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"cw.merge: Lists are concatenated\",\n      \"correction\": \"Lists are replaced by override, not concatenated.\"\n    },\n    {\n      \"location\": \"Environment variable interpolation: any string may include ${VAR}\",\n      \"correction\": \"Interpolation only applies when the full value is a placeholder; embedded placeholders are not expanded.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"YAML !include tag to inline another YAML file\"\n  ],\n  \"false_alarms\": [\n    \"YAML parse errors were caused by using tabs for indentation (invalid YAML), not a ConfigWeaver bug\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nConfigWeaver v2.0 (Python)\n\nAPI:\n- cw.load(path: str) -> dict\n  Loads YAML or JSON.\n\n- cw.merge(base: dict, override: dict) -> dict\n  Deep merges nested dictionaries.\n  Lists are concatenated.\n\nEnvironment variable interpolation:\n- Any string may include ${VAR} placeholders which will be expanded at load time.\n\nYAML support:\n- Standard YAML 1.2\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tested cw.merge and list behavior doesn’t match docs.\nDev2: How so?\nDev1: base.plugins=[\"a\",\"b\"], override.plugins=[\"c\"]. Docs say concatenated, but merged result is just [\"c\"].\n\nDev2: How about env interpolation?\nDev1: ${PORT} works if the whole value is \"${PORT}\", but if I do \"http://${HOST}:8080\" it stays literally \"http://${HOST}:8080\".\nDev2: So interpolation is not “any string”.\n\nDev1: Another surprise: if override has a key set to null, it deletes the key from the merged output entirely.\nDev2: That’s a tombstone semantics.\n\nDev1: Hidden feature: YAML tag !include works. I can do:\n  db: !include db.yaml\nand it inlines the other file. Not in docs.\n\nDev2: You mentioned a parse error earlier.\nDev1: Yeah, I had tabs in my YAML indentation. That’s invalid YAML; replacing tabs with spaces fixed it.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"ConfigWeaver\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"merge() replaces lists instead of concatenating\",\n      \"expected\": \"Lists are concatenated during deep merge (per docs)\",\n      \"actual\": \"Lists in override replace the base list entirely\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Manually concatenate lists before calling cw.merge when that is desired\"\n    },\n    {\n      \"title\": \"Environment interpolation only works for whole-value tokens\",\n      \"expected\": \"${VAR} placeholders are expanded anywhere within strings (per docs)\",\n      \"actual\": \"Interpolation works when the entire value is a placeholder (e.g., \\\"${PORT}\\\"), but not when embedded in a longer string\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Build composite strings after load, or split into separate fields that can be interpolated\"\n    },\n    {\n      \"title\": \"null values act as delete tombstones in merge\",\n      \"expected\": \"Setting a key to null would keep the key with a null value\",\n      \"actual\": \"Keys set to null in override are removed from the merged output\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use an explicit empty value (e.g., empty string/object) if you need the key to remain present\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"cw.merge: Lists are concatenated\",\n      \"correction\": \"Lists are replaced by override, not concatenated.\"\n    },\n    {\n      \"location\": \"Environment variable interpolation: any string may include ${VAR}\",\n      \"correction\": \"Interpolation only applies when the full value is a placeholder; embedded placeholders are not expanded.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"YAML !include tag to inline another YAML file\"\n  ],\n  \"false_alarms\": [\n    \"YAML parse errors were caused by using tabs for indentation (invalid YAML), not a ConfigWeaver bug\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-010",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 10,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMailRelay API v1\n\nPOST /v1/send\nHeaders:\n- Authorization: Bearer <token>\n- Idempotency-Key: <uuid> (optional, prevents duplicate sends on retries)\n\nBody:\n{\n  \"to\": [\"user@example.com\"],\n  \"subject\": \"...\",\n  \"text\": \"...\",\n  \"attachments\": [\n    { \"filename\": \"file.pdf\", \"content_type\": \"application/pdf\", \"content_base64\": \"...\" }\n  ]\n}\n\nLimits:\n- Max attachment size: 10MB per attachment\n\nGET /v1/events\nQuery params:\n- since (RFC3339)\n- type (optional: delivered|bounce|complaint)\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tried POST /v1/send but got 401 for a while.\nDev2: What header?\nDev1: I used Authentication: Bearer ... (wrong). Switching to Authorization fixed it.\n\nDev1: Idempotency is flaky. Retrying the same request with the same Idempotency-Key after ~3 minutes resulted in a duplicate email.\nDev2: But retries within ~30-60 seconds didn’t duplicate.\n\nDev2: Attachments?\nDev1: If the base64 string has line breaks (MIME-style), the attachment just disappears. The email sends but no attachment.\n\nDev1: Also, docs say 10MB max. I tried an 8MB PDF and got 413 Payload Too Large.\nDev2: So the real cap is lower.\n\nDev1: Hidden feature: X-Test-Mode: true header returns a message_id but doesn’t actually deliver (no events show up). Great for integration tests, but it’s undocumented.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"MailRelay\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Idempotency-Key has a short dedupe window\",\n      \"expected\": \"Idempotency-Key prevents duplicate sends on retries\",\n      \"actual\": \"Retries after a few minutes can still send duplicates; dedupe only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Ensure retries happen quickly or implement client-side de-duplication keyed by a stable business id\"\n    },\n    {\n      \"title\": \"Attachments silently dropped when base64 contains line breaks\",\n      \"expected\": \"Any valid base64 should be accepted for attachments\",\n      \"actual\": \"Base64 with line breaks results in the email sending without the attachment (no explicit error)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Send base64 as a single unbroken line\"\n    },\n    {\n      \"title\": \"Attachment size limit lower than documented\",\n      \"expected\": \"Up to 10MB per attachment (per docs)\",\n      \"actual\": \"Attachments around 8MB are rejected with 413 Payload Too Large\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Keep attachments under the real limit or upload to storage and send a link\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Limits: Max attachment size 10MB\",\n      \"correction\": \"Actual max is lower; large attachments (e.g., ~8MB) are rejected with 413.\"\n    },\n    {\n      \"location\": \"Idempotency-Key description\",\n      \"correction\": \"Idempotency deduplication appears time-windowed; retries after a few minutes can duplicate.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-Test-Mode: true returns message_id but suppresses real delivery for testing\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using the wrong header name (Authentication instead of Authorization)\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nMailRelay API v1\n\nPOST /v1/send\nHeaders:\n- Authorization: Bearer <token>\n- Idempotency-Key: <uuid> (optional, prevents duplicate sends on retries)\n\nBody:\n{\n  \"to\": [\"user@example.com\"],\n  \"subject\": \"...\",\n  \"text\": \"...\",\n  \"attachments\": [\n    { \"filename\": \"file.pdf\", \"content_type\": \"application/pdf\", \"content_base64\": \"...\" }\n  ]\n}\n\nLimits:\n- Max attachment size: 10MB per attachment\n\nGET /v1/events\nQuery params:\n- since (RFC3339)\n- type (optional: delivered|bounce|complaint)\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: I tried POST /v1/send but got 401 for a while.\nDev2: What header?\nDev1: I used Authentication: Bearer ... (wrong). Switching to Authorization fixed it.\n\nDev1: Idempotency is flaky. Retrying the same request with the same Idempotency-Key after ~3 minutes resulted in a duplicate email.\nDev2: But retries within ~30-60 seconds didn’t duplicate.\n\nDev2: Attachments?\nDev1: If the base64 string has line breaks (MIME-style), the attachment just disappears. The email sends but no attachment.\n\nDev1: Also, docs say 10MB max. I tried an 8MB PDF and got 413 Payload Too Large.\nDev2: So the real cap is lower.\n\nDev1: Hidden feature: X-Test-Mode: true header returns a message_id but doesn’t actually deliver (no events show up). Great for integration tests, but it’s undocumented.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"MailRelay\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Idempotency-Key has a short dedupe window\",\n      \"expected\": \"Idempotency-Key prevents duplicate sends on retries\",\n      \"actual\": \"Retries after a few minutes can still send duplicates; dedupe only works for short retry windows\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Ensure retries happen quickly or implement client-side de-duplication keyed by a stable business id\"\n    },\n    {\n      \"title\": \"Attachments silently dropped when base64 contains line breaks\",\n      \"expected\": \"Any valid base64 should be accepted for attachments\",\n      \"actual\": \"Base64 with line breaks results in the email sending without the attachment (no explicit error)\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Send base64 as a single unbroken line\"\n    },\n    {\n      \"title\": \"Attachment size limit lower than documented\",\n      \"expected\": \"Up to 10MB per attachment (per docs)\",\n      \"actual\": \"Attachments around 8MB are rejected with 413 Payload Too Large\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Keep attachments under the real limit or upload to storage and send a link\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Limits: Max attachment size 10MB\",\n      \"correction\": \"Actual max is lower; large attachments (e.g., ~8MB) are rejected with 413.\"\n    },\n    {\n      \"location\": \"Idempotency-Key description\",\n      \"correction\": \"Idempotency deduplication appears time-windowed; retries after a few minutes can duplicate.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-Test-Mode: true returns message_id but suppresses real delivery for testing\"\n  ],\n  \"false_alarms\": [\n    \"401 errors were caused by using the wrong header name (Authentication instead of Authorization)\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-011",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 11,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nChronosDB - Managed Time Series Database\n\nWrite:\nPOST /v1/write\n- Writes are immediately queryable.\n\nQuery:\nGET /v1/query?sql=<urlencoded>\n- now() is evaluated on the server in UTC.\n\nDelete:\nDELETE /v1/tables/{table}\n- Deletes the table immediately and releases the name for reuse.\n\nAuth:\n- Bearer tokens are accepted if client clock skew is within 5 minutes.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Writes aren’t immediately visible. After POST /v1/write, the point shows up in queries ~10-15 seconds later.\nDev2: So there’s ingestion/indexing lag.\n\nDev1: now() isn’t UTC for our account. It matches our account timezone (US/Pacific) — queries using now() shift by 8 hours compared to UTC.\nDev2: That contradicts the docs.\n\nDev1: Table deletion is also async. DELETE returns 204, but I can still query the table for ~1 minute. Also, trying to recreate a table with the same name fails for a few minutes.\n\nDev2: You had a bunch of 401s earlier.\nDev1: My laptop clock was 12 minutes slow. After NTP sync, auth works fine.\n\nDev1: Hidden feature: EXPLAIN works in the SQL dialect and returns a query plan JSON. Not mentioned in docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"ChronosDB\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Writes have 10–15s query visibility lag\",\n      \"expected\": \"Writes are immediately queryable (per docs)\",\n      \"actual\": \"Newly written points appear in query results after ~10–15 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add small retries/backoff after writes or design for eventual consistency\"\n    },\n    {\n      \"title\": \"now() uses account timezone, not UTC\",\n      \"expected\": \"now() is evaluated on the server in UTC (per docs)\",\n      \"actual\": \"now() aligns with the account timezone (e.g., US/Pacific), shifting time-based queries\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use explicit UTC timestamps or convert using timezone-aware functions\"\n    },\n    {\n      \"title\": \"Table deletion is asynchronous and name reuse is delayed\",\n      \"expected\": \"DELETE releases the table name immediately\",\n      \"actual\": \"Tables remain queryable briefly after DELETE and the name cannot be reused for a few minutes\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Wait and retry table recreation; treat deletes as asynchronous operations\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Write: Writes are immediately queryable\",\n      \"correction\": \"Writes can take ~10–15 seconds to become visible in queries.\"\n    },\n    {\n      \"location\": \"Query: now() evaluated in UTC\",\n      \"correction\": \"now() appears to evaluate in the account timezone rather than UTC.\"\n    },\n    {\n      \"location\": \"Delete: deletes immediately and releases name\",\n      \"correction\": \"Deletion is asynchronous; table may remain accessible briefly and name reuse is delayed.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"EXPLAIN <query> returns a query plan JSON\"\n  ],\n  \"false_alarms\": [\n    \"401 auth failures were caused by client clock skew beyond the allowed 5 minutes\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nChronosDB - Managed Time Series Database\n\nWrite:\nPOST /v1/write\n- Writes are immediately queryable.\n\nQuery:\nGET /v1/query?sql=<urlencoded>\n- now() is evaluated on the server in UTC.\n\nDelete:\nDELETE /v1/tables/{table}\n- Deletes the table immediately and releases the name for reuse.\n\nAuth:\n- Bearer tokens are accepted if client clock skew is within 5 minutes.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Writes aren’t immediately visible. After POST /v1/write, the point shows up in queries ~10-15 seconds later.\nDev2: So there’s ingestion/indexing lag.\n\nDev1: now() isn’t UTC for our account. It matches our account timezone (US/Pacific) — queries using now() shift by 8 hours compared to UTC.\nDev2: That contradicts the docs.\n\nDev1: Table deletion is also async. DELETE returns 204, but I can still query the table for ~1 minute. Also, trying to recreate a table with the same name fails for a few minutes.\n\nDev2: You had a bunch of 401s earlier.\nDev1: My laptop clock was 12 minutes slow. After NTP sync, auth works fine.\n\nDev1: Hidden feature: EXPLAIN works in the SQL dialect and returns a query plan JSON. Not mentioned in docs.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"ChronosDB\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Writes have 10–15s query visibility lag\",\n      \"expected\": \"Writes are immediately queryable (per docs)\",\n      \"actual\": \"Newly written points appear in query results after ~10–15 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add small retries/backoff after writes or design for eventual consistency\"\n    },\n    {\n      \"title\": \"now() uses account timezone, not UTC\",\n      \"expected\": \"now() is evaluated on the server in UTC (per docs)\",\n      \"actual\": \"now() aligns with the account timezone (e.g., US/Pacific), shifting time-based queries\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use explicit UTC timestamps or convert using timezone-aware functions\"\n    },\n    {\n      \"title\": \"Table deletion is asynchronous and name reuse is delayed\",\n      \"expected\": \"DELETE releases the table name immediately\",\n      \"actual\": \"Tables remain queryable briefly after DELETE and the name cannot be reused for a few minutes\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Wait and retry table recreation; treat deletes as asynchronous operations\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Write: Writes are immediately queryable\",\n      \"correction\": \"Writes can take ~10–15 seconds to become visible in queries.\"\n    },\n    {\n      \"location\": \"Query: now() evaluated in UTC\",\n      \"correction\": \"now() appears to evaluate in the account timezone rather than UTC.\"\n    },\n    {\n      \"location\": \"Delete: deletes immediately and releases name\",\n      \"correction\": \"Deletion is asynchronous; table may remain accessible briefly and name reuse is delayed.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"EXPLAIN <query> returns a query plan JSON\"\n  ],\n  \"false_alarms\": [\n    \"401 auth failures were caused by client clock skew beyond the allowed 5 minutes\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-012",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 12,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ntapctl v1.2 - Kubernetes Helper CLI\n\nCommands:\n- tapctl connect\n  Uses the current kubeconfig context.\n\n- tapctl logs <pod>\n  Tails logs from the pod. Defaults to the last 100 lines.\n\n- tapctl proxy --service <svc> --port <port>\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: tapctl connect doesn’t follow kubeconfig context changes.\nDev2: What do you mean?\nDev1: I ran kubectl config use-context staging, but tapctl still hit prod until I ran tapctl connect --refresh. It seems to cache state in ~/.tapctl/state.json.\n\nDev2: tapctl logs default lines?\nDev1: Docs say 100, but it prints 200 unless I pass --tail.\n\nDev1: Another quirk: tapctl logs picks the first container alphabetically. On our pods that’s the istio-proxy sidecar, so you get the wrong logs unless you specify --container app.\n\nDev2: Anything cool you found?\nDev1: Hidden command: tapctl doctor. It prints context, cluster reachability, and cached state path. Not documented.\n\nDev2: Earlier the proxy command failed with connection refused.\nDev1: That was because my kind cluster wasn’t running. Once I started it, it worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"tapctl\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"connect caches kube context and ignores kubeconfig changes\",\n      \"expected\": \"tapctl uses the current kubeconfig context\",\n      \"actual\": \"tapctl may keep using a previously cached context until connect --refresh is run\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Run tapctl connect --refresh or clear ~/.tapctl/state.json after switching contexts\"\n    },\n    {\n      \"title\": \"logs defaults to wrong container\",\n      \"expected\": \"logs tails logs from the pod (implicitly the app container)\",\n      \"actual\": \"By default tapctl selects the first container alphabetically (often a sidecar like istio-proxy)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pass --container app (or the intended container name)\"\n    },\n    {\n      \"title\": \"logs default tail count differs\",\n      \"expected\": \"Defaults to last 100 lines (per docs)\",\n      \"actual\": \"Defaults to 200 lines unless --tail is provided\",\n      \"severity\": \"cosmetic\",\n      \"workaround\": \"Specify --tail explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"tapctl logs: Defaults to the last 100 lines\",\n      \"correction\": \"Default appears to be 200 lines.\"\n    },\n    {\n      \"location\": \"tapctl connect: Uses the current kubeconfig context\",\n      \"correction\": \"tapctl caches the selected context and may require --refresh after kubeconfig changes.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"tapctl doctor prints diagnostics including cached state location\"\n  ],\n  \"false_alarms\": [\n    \"Connection refused errors were due to the target cluster not running (environment issue)\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\ntapctl v1.2 - Kubernetes Helper CLI\n\nCommands:\n- tapctl connect\n  Uses the current kubeconfig context.\n\n- tapctl logs <pod>\n  Tails logs from the pod. Defaults to the last 100 lines.\n\n- tapctl proxy --service <svc> --port <port>\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: tapctl connect doesn’t follow kubeconfig context changes.\nDev2: What do you mean?\nDev1: I ran kubectl config use-context staging, but tapctl still hit prod until I ran tapctl connect --refresh. It seems to cache state in ~/.tapctl/state.json.\n\nDev2: tapctl logs default lines?\nDev1: Docs say 100, but it prints 200 unless I pass --tail.\n\nDev1: Another quirk: tapctl logs picks the first container alphabetically. On our pods that’s the istio-proxy sidecar, so you get the wrong logs unless you specify --container app.\n\nDev2: Anything cool you found?\nDev1: Hidden command: tapctl doctor. It prints context, cluster reachability, and cached state path. Not documented.\n\nDev2: Earlier the proxy command failed with connection refused.\nDev1: That was because my kind cluster wasn’t running. Once I started it, it worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"tapctl\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"connect caches kube context and ignores kubeconfig changes\",\n      \"expected\": \"tapctl uses the current kubeconfig context\",\n      \"actual\": \"tapctl may keep using a previously cached context until connect --refresh is run\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Run tapctl connect --refresh or clear ~/.tapctl/state.json after switching contexts\"\n    },\n    {\n      \"title\": \"logs defaults to wrong container\",\n      \"expected\": \"logs tails logs from the pod (implicitly the app container)\",\n      \"actual\": \"By default tapctl selects the first container alphabetically (often a sidecar like istio-proxy)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pass --container app (or the intended container name)\"\n    },\n    {\n      \"title\": \"logs default tail count differs\",\n      \"expected\": \"Defaults to last 100 lines (per docs)\",\n      \"actual\": \"Defaults to 200 lines unless --tail is provided\",\n      \"severity\": \"cosmetic\",\n      \"workaround\": \"Specify --tail explicitly\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"tapctl logs: Defaults to the last 100 lines\",\n      \"correction\": \"Default appears to be 200 lines.\"\n    },\n    {\n      \"location\": \"tapctl connect: Uses the current kubeconfig context\",\n      \"correction\": \"tapctl caches the selected context and may require --refresh after kubeconfig changes.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"tapctl doctor prints diagnostics including cached state location\"\n  ],\n  \"false_alarms\": [\n    \"Connection refused errors were due to the target cluster not running (environment issue)\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-013",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 13,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nSheetCalc 4.0 - JavaScript Spreadsheet Formula Engine\n\nFeatures:\n- Supports Excel-compatible formulas including SUM, AVERAGE, VLOOKUP, XLOOKUP, LET, LAMBDA.\n- Rounding matches Excel (0.5 rounds away from zero).\n- Locale-aware parsing (commas/dots) follows the runtime locale.\n\nAPI:\n- evaluate(workbookJson) -> values\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: XLOOKUP is documented as supported, but it returns #NAME? every time.\nDev2: So it’s not implemented.\n\nDev1: Rounding also differs. In Excel, 2.5 rounds to 3 with ROUND(). In SheetCalc, ROUND(2.5,0) returns 2 (bankers rounding).\n\nDev2: Locale support?\nDev1: Docs claim locale-aware parsing, but it only accepts dot decimals. If I feed \"1,23\" it errors.\n\nDev1: Hidden feature: if I set process.env.SHEETCALC_DEBUG=1, it returns an extra field with the dependency graph in the result.\n\nDev2: You said date math was wrong earlier.\nDev1: That was my fault — my input workbook was already using dot decimals. The failing case was the comma decimal string, which isn’t supported.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"SheetCalc\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"XLOOKUP not supported despite docs\",\n      \"expected\": \"XLOOKUP formulas evaluate successfully\",\n      \"actual\": \"XLOOKUP returns #NAME? indicating the function is unimplemented\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use VLOOKUP/INDEX+MATCH patterns or implement XLOOKUP in a pre-processing step\"\n    },\n    {\n      \"title\": \"Rounding uses half-to-even (bankers) instead of Excel-style\",\n      \"expected\": \"0.5 rounds away from zero (per docs)\",\n      \"actual\": \"ROUND(2.5,0) returns 2 (half-to-even)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use a custom rounding function if Excel parity is required\"\n    },\n    {\n      \"title\": \"Not actually locale-aware for decimal commas\",\n      \"expected\": \"Locale-aware parsing follows runtime locale (per docs)\",\n      \"actual\": \"Only dot-decimal numeric strings are accepted; comma decimals error\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Normalize numeric strings to dot-decimal format before evaluation\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Features: supports XLOOKUP\",\n      \"correction\": \"XLOOKUP currently evaluates to #NAME? and appears unsupported.\"\n    },\n    {\n      \"location\": \"Rounding matches Excel (0.5 rounds away from zero)\",\n      \"correction\": \"Rounding uses half-to-even (bankers rounding).\"\n    },\n    {\n      \"location\": \"Locale-aware parsing follows runtime locale\",\n      \"correction\": \"Decimal-comma numeric strings are not accepted; inputs must use dot decimals.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"SHEETCALC_DEBUG=1 returns an additional dependency-graph/debug payload\"\n  ],\n  \"false_alarms\": [\n    \"Initial evaluation failures were due to providing comma-decimal strings (unsupported input format), not date math bugs\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nSheetCalc 4.0 - JavaScript Spreadsheet Formula Engine\n\nFeatures:\n- Supports Excel-compatible formulas including SUM, AVERAGE, VLOOKUP, XLOOKUP, LET, LAMBDA.\n- Rounding matches Excel (0.5 rounds away from zero).\n- Locale-aware parsing (commas/dots) follows the runtime locale.\n\nAPI:\n- evaluate(workbookJson) -> values\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: XLOOKUP is documented as supported, but it returns #NAME? every time.\nDev2: So it’s not implemented.\n\nDev1: Rounding also differs. In Excel, 2.5 rounds to 3 with ROUND(). In SheetCalc, ROUND(2.5,0) returns 2 (bankers rounding).\n\nDev2: Locale support?\nDev1: Docs claim locale-aware parsing, but it only accepts dot decimals. If I feed \"1,23\" it errors.\n\nDev1: Hidden feature: if I set process.env.SHEETCALC_DEBUG=1, it returns an extra field with the dependency graph in the result.\n\nDev2: You said date math was wrong earlier.\nDev1: That was my fault — my input workbook was already using dot decimals. The failing case was the comma decimal string, which isn’t supported.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"SheetCalc\",\n  \"category\": \"library\",\n  \"quirks\": [\n    {\n      \"title\": \"XLOOKUP not supported despite docs\",\n      \"expected\": \"XLOOKUP formulas evaluate successfully\",\n      \"actual\": \"XLOOKUP returns #NAME? indicating the function is unimplemented\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use VLOOKUP/INDEX+MATCH patterns or implement XLOOKUP in a pre-processing step\"\n    },\n    {\n      \"title\": \"Rounding uses half-to-even (bankers) instead of Excel-style\",\n      \"expected\": \"0.5 rounds away from zero (per docs)\",\n      \"actual\": \"ROUND(2.5,0) returns 2 (half-to-even)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Use a custom rounding function if Excel parity is required\"\n    },\n    {\n      \"title\": \"Not actually locale-aware for decimal commas\",\n      \"expected\": \"Locale-aware parsing follows runtime locale (per docs)\",\n      \"actual\": \"Only dot-decimal numeric strings are accepted; comma decimals error\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Normalize numeric strings to dot-decimal format before evaluation\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Features: supports XLOOKUP\",\n      \"correction\": \"XLOOKUP currently evaluates to #NAME? and appears unsupported.\"\n    },\n    {\n      \"location\": \"Rounding matches Excel (0.5 rounds away from zero)\",\n      \"correction\": \"Rounding uses half-to-even (bankers rounding).\"\n    },\n    {\n      \"location\": \"Locale-aware parsing follows runtime locale\",\n      \"correction\": \"Decimal-comma numeric strings are not accepted; inputs must use dot decimals.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"SHEETCALC_DEBUG=1 returns an additional dependency-graph/debug payload\"\n  ],\n  \"false_alarms\": [\n    \"Initial evaluation failures were due to providing comma-decimal strings (unsupported input format), not date math bugs\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-014",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 14,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nGeoFence API v3\n\nCreate fence:\nPOST /v3/fences\nBody:\n{\n  \"name\": \"...\",\n  \"polygon\": [[lat,lng], [lat,lng], ...]\n}\n\nRules:\n- Up to 500 vertices.\n- Point order does not matter (clockwise or counterclockwise accepted).\n\nResponse:\n{ \"id\": \"...\", \"area_m2\": number }\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: The polygon winding matters even though docs say it doesn’t. If I send points counterclockwise I get 400 \"invalid winding\".\nDev2: What happens if you reverse the points?\nDev1: Works immediately.\n\nDev2: Vertex limit?\nDev1: Docs say 500, but anything above 100 returns 413.\n\nDev1: Hidden feature: if I add ?simplify=true to POST /v3/fences, it returns a simplified polygon in the response (fewer points). Not in docs.\n\nDev2: You said one fence ended up in the ocean.\nDev1: Yeah, I had lat/lng swapped (I sent [lng,lat]). That was on me.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"GeoFence API\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Polygon winding is enforced\",\n      \"expected\": \"Clockwise or counterclockwise point order is accepted (per docs)\",\n      \"actual\": \"Counterclockwise polygons fail with 400 invalid winding\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Ensure polygons are sent in the required winding order (e.g., clockwise)\"\n    },\n    {\n      \"title\": \"Vertex limit capped at 100\",\n      \"expected\": \"Up to 500 vertices supported (per docs)\",\n      \"actual\": \"Requests above ~100 vertices are rejected (e.g., 413)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pre-simplify polygons client-side or use simplify mode\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Rules: Point order does not matter\",\n      \"correction\": \"API enforces polygon winding; counterclockwise input is rejected.\"\n    },\n    {\n      \"location\": \"Rules: Up to 500 vertices\",\n      \"correction\": \"Effective max is ~100 vertices.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"POST /v3/fences?simplify=true returns a simplified polygon in the response\"\n  ],\n  \"false_alarms\": [\n    \"Incorrect fence locations were caused by swapping lat/lng order in the client request\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nGeoFence API v3\n\nCreate fence:\nPOST /v3/fences\nBody:\n{\n  \"name\": \"...\",\n  \"polygon\": [[lat,lng], [lat,lng], ...]\n}\n\nRules:\n- Up to 500 vertices.\n- Point order does not matter (clockwise or counterclockwise accepted).\n\nResponse:\n{ \"id\": \"...\", \"area_m2\": number }\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: The polygon winding matters even though docs say it doesn’t. If I send points counterclockwise I get 400 \"invalid winding\".\nDev2: What happens if you reverse the points?\nDev1: Works immediately.\n\nDev2: Vertex limit?\nDev1: Docs say 500, but anything above 100 returns 413.\n\nDev1: Hidden feature: if I add ?simplify=true to POST /v3/fences, it returns a simplified polygon in the response (fewer points). Not in docs.\n\nDev2: You said one fence ended up in the ocean.\nDev1: Yeah, I had lat/lng swapped (I sent [lng,lat]). That was on me.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"GeoFence API\",\n  \"category\": \"api\",\n  \"quirks\": [\n    {\n      \"title\": \"Polygon winding is enforced\",\n      \"expected\": \"Clockwise or counterclockwise point order is accepted (per docs)\",\n      \"actual\": \"Counterclockwise polygons fail with 400 invalid winding\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Ensure polygons are sent in the required winding order (e.g., clockwise)\"\n    },\n    {\n      \"title\": \"Vertex limit capped at 100\",\n      \"expected\": \"Up to 500 vertices supported (per docs)\",\n      \"actual\": \"Requests above ~100 vertices are rejected (e.g., 413)\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Pre-simplify polygons client-side or use simplify mode\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Rules: Point order does not matter\",\n      \"correction\": \"API enforces polygon winding; counterclockwise input is rejected.\"\n    },\n    {\n      \"location\": \"Rules: Up to 500 vertices\",\n      \"correction\": \"Effective max is ~100 vertices.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"POST /v3/fences?simplify=true returns a simplified polygon in the response\"\n  ],\n  \"false_alarms\": [\n    \"Incorrect fence locations were caused by swapping lat/lng order in the client request\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-015",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 15,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nAuditTrail - Event Logging Service\n\nIngest:\nPOST /v1/events\n- Events are searchable immediately after ingestion.\n\nSearch:\nGET /v1/events/search\nQuery params:\n- start (RFC3339)\n- end (RFC3339)\n- q (string)\n\nTime handling:\n- If start/end omit timezone, values are treated as UTC.\n\nRetention:\n- 90 days\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Search is not immediate. After ingesting events, they show up in search results ~20-30 seconds later.\nDev2: So there’s indexing lag.\n\nDev1: Timezone handling is weird. If I pass start=2026-02-04T10:00:00 (no Z), it treats it as account-local time, not UTC.\nDev2: Docs claim UTC default.\n\nDev1: Hidden feature: responses include an x-index-lag-ms header if I send X-AuditTrail-Debug: 1. Super helpful.\n\nDev2: Earlier you said events were “missing”.\nDev1: That was because I used an old API key that had been rotated. Once I updated the key, ingest worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"AuditTrail\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Search results have 20–30s indexing lag\",\n      \"expected\": \"Events are searchable immediately after ingestion (per docs)\",\n      \"actual\": \"Events appear in search results after ~20–30 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add short retry/backoff when verifying ingestion\"\n    },\n    {\n      \"title\": \"Timezone-less start/end are treated as account-local time\",\n      \"expected\": \"Timezone-less RFC3339 values default to UTC (per docs)\",\n      \"actual\": \"Timezone-less timestamps are interpreted as account-local time\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always include explicit timezone (e.g., Z or ±HH:MM) in queries\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Ingest: Events are searchable immediately\",\n      \"correction\": \"Search appears eventually consistent; expect ~20–30 seconds lag.\"\n    },\n    {\n      \"location\": \"Time handling: omit timezone treated as UTC\",\n      \"correction\": \"Timezone-less timestamps are interpreted as account-local time.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-AuditTrail-Debug: 1 returns x-index-lag-ms header\"\n  ],\n  \"false_alarms\": [\n    \"Missing events were caused by using a rotated/invalid API key, not ingestion loss\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\nAuditTrail - Event Logging Service\n\nIngest:\nPOST /v1/events\n- Events are searchable immediately after ingestion.\n\nSearch:\nGET /v1/events/search\nQuery params:\n- start (RFC3339)\n- end (RFC3339)\n- q (string)\n\nTime handling:\n- If start/end omit timezone, values are treated as UTC.\n\nRetention:\n- 90 days\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Search is not immediate. After ingesting events, they show up in search results ~20-30 seconds later.\nDev2: So there’s indexing lag.\n\nDev1: Timezone handling is weird. If I pass start=2026-02-04T10:00:00 (no Z), it treats it as account-local time, not UTC.\nDev2: Docs claim UTC default.\n\nDev1: Hidden feature: responses include an x-index-lag-ms header if I send X-AuditTrail-Debug: 1. Super helpful.\n\nDev2: Earlier you said events were “missing”.\nDev1: That was because I used an old API key that had been rotated. Once I updated the key, ingest worked.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"AuditTrail\",\n  \"category\": \"service\",\n  \"quirks\": [\n    {\n      \"title\": \"Search results have 20–30s indexing lag\",\n      \"expected\": \"Events are searchable immediately after ingestion (per docs)\",\n      \"actual\": \"Events appear in search results after ~20–30 seconds\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Add short retry/backoff when verifying ingestion\"\n    },\n    {\n      \"title\": \"Timezone-less start/end are treated as account-local time\",\n      \"expected\": \"Timezone-less RFC3339 values default to UTC (per docs)\",\n      \"actual\": \"Timezone-less timestamps are interpreted as account-local time\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Always include explicit timezone (e.g., Z or ±HH:MM) in queries\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Ingest: Events are searchable immediately\",\n      \"correction\": \"Search appears eventually consistent; expect ~20–30 seconds lag.\"\n    },\n    {\n      \"location\": \"Time handling: omit timezone treated as UTC\",\n      \"correction\": \"Timezone-less timestamps are interpreted as account-local time.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"X-AuditTrail-Debug: 1 returns x-index-lag-ms header\"\n  ],\n  \"false_alarms\": [\n    \"Missing events were caused by using a rotated/invalid API key, not ingestion loss\"\n  ]\n}",
      "meta": {},
      "results": []
    },
    {
      "id": "explore-016",
      "dataset": "dataset_all",
      "source": "tool-exploration/data/dataset_all.jsonl",
      "line": 16,
      "input_turns": [
        {
          "role": "input",
          "content": "You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\npkgscan 2.4 - Dependency Vulnerability Scanner\n\nUsage:\n- pkgscan scan <path>\n  Scans direct and transitive dependencies.\n\nOptions:\n- --exclude <glob>\n  Exclude packages matching the glob from results.\n\n- --format json\n  Outputs machine-readable JSON to stdout.\n\nLockfile support:\n- npm package-lock.json v1/v2/v3\n\nDatabase:\n- pkgscan db update\n  Downloads the vulnerability database.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Lockfile v3 support seems broken. Docs claim package-lock.json v3 is supported, but pkgscan scan errors: \"unsupported lockfileVersion: 3\".\n\nDev2: How does --exclude behave?\nDev1: It only excludes direct deps. If lodash is pulled transitively, it still shows up even if I pass --exclude lodash.\n\nDev1: Another annoyance: with --format json, it prints progress lines to stdout before the JSON unless I also pass --quiet. That breaks parsing.\n\nDev2: DB update failing?\nDev1: At first it timed out. Turned out our corporate proxy blocks it; setting HTTPS_PROXY fixed it. Not pkgscan’s fault.\n\nDev1: Hidden feature: PKGSCAN_OFFLINE=1 skips db update and uses the cached DB.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences)."
        }
      ],
      "ground_truth_turns": [
        {
          "role": "expected",
          "content": "{\n  \"tool_name\": \"pkgscan\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"package-lock.json v3 not actually supported\",\n      \"expected\": \"npm lockfileVersion 3 is supported (per docs)\",\n      \"actual\": \"scan fails with \\\"unsupported lockfileVersion: 3\\\"\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Downgrade lockfileVersion or convert to a supported lockfile format\"\n    },\n    {\n      \"title\": \"--exclude does not apply to transitive dependencies\",\n      \"expected\": \"Excluded packages are removed from results regardless of dependency depth\",\n      \"actual\": \"Excluded packages still appear when they are transitive dependencies\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Filter results post-scan or adjust the dependency tree to remove transitive packages\"\n    },\n    {\n      \"title\": \"--format json polluted by progress output\",\n      \"expected\": \"--format json outputs only JSON to stdout\",\n      \"actual\": \"Progress logs are printed to stdout before the JSON unless --quiet is also provided\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use --quiet with --format json or redirect stderr/stdout appropriately\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Lockfile support: package-lock.json v3\",\n      \"correction\": \"lockfileVersion 3 currently errors as unsupported.\"\n    },\n    {\n      \"location\": \"--exclude option description\",\n      \"correction\": \"--exclude appears to apply only to direct dependencies, not transitive ones.\"\n    },\n    {\n      \"location\": \"--format json outputs machine-readable JSON\",\n      \"correction\": \"JSON output may be preceded by progress text unless --quiet is used.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"PKGSCAN_OFFLINE=1 uses cached DB and skips updates\"\n  ],\n  \"false_alarms\": [\n    \"Database update timeouts were caused by a corporate proxy; setting HTTPS_PROXY resolved the issue\"\n  ]\n}"
        }
      ],
      "input_text": "input: You are exploring a new tool and must produce a structured exploration report.\n\n--- OFFICIAL DOCS ---\n\npkgscan 2.4 - Dependency Vulnerability Scanner\n\nUsage:\n- pkgscan scan <path>\n  Scans direct and transitive dependencies.\n\nOptions:\n- --exclude <glob>\n  Exclude packages matching the glob from results.\n\n- --format json\n  Outputs machine-readable JSON to stdout.\n\nLockfile support:\n- npm package-lock.json v1/v2/v3\n\nDatabase:\n- pkgscan db update\n  Downloads the vulnerability database.\n\n--- EXPLORATION TRANSCRIPT ---\n\nDev1: Lockfile v3 support seems broken. Docs claim package-lock.json v3 is supported, but pkgscan scan errors: \"unsupported lockfileVersion: 3\".\n\nDev2: How does --exclude behave?\nDev1: It only excludes direct deps. If lodash is pulled transitively, it still shows up even if I pass --exclude lodash.\n\nDev1: Another annoyance: with --format json, it prints progress lines to stdout before the JSON unless I also pass --quiet. That breaks parsing.\n\nDev2: DB update failing?\nDev1: At first it timed out. Turned out our corporate proxy blocks it; setting HTTPS_PROXY fixed it. Not pkgscan’s fault.\n\nDev1: Hidden feature: PKGSCAN_OFFLINE=1 skips db update and uses the cached DB.\n\nRequired JSON keys:\n- tool_name, category, quirks[], doc_errors[], hidden_features[], false_alarms[]\nquirks[]: { title, expected, actual, severity, workaround }\ndoc_errors[]: { location, correction }\nOutput a single JSON object only (no code fences).",
      "ground_truth_text": "expected: {\n  \"tool_name\": \"pkgscan\",\n  \"category\": \"cli\",\n  \"quirks\": [\n    {\n      \"title\": \"package-lock.json v3 not actually supported\",\n      \"expected\": \"npm lockfileVersion 3 is supported (per docs)\",\n      \"actual\": \"scan fails with \\\"unsupported lockfileVersion: 3\\\"\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Downgrade lockfileVersion or convert to a supported lockfile format\"\n    },\n    {\n      \"title\": \"--exclude does not apply to transitive dependencies\",\n      \"expected\": \"Excluded packages are removed from results regardless of dependency depth\",\n      \"actual\": \"Excluded packages still appear when they are transitive dependencies\",\n      \"severity\": \"surprising\",\n      \"workaround\": \"Filter results post-scan or adjust the dependency tree to remove transitive packages\"\n    },\n    {\n      \"title\": \"--format json polluted by progress output\",\n      \"expected\": \"--format json outputs only JSON to stdout\",\n      \"actual\": \"Progress logs are printed to stdout before the JSON unless --quiet is also provided\",\n      \"severity\": \"breaking\",\n      \"workaround\": \"Use --quiet with --format json or redirect stderr/stdout appropriately\"\n    }\n  ],\n  \"doc_errors\": [\n    {\n      \"location\": \"Lockfile support: package-lock.json v3\",\n      \"correction\": \"lockfileVersion 3 currently errors as unsupported.\"\n    },\n    {\n      \"location\": \"--exclude option description\",\n      \"correction\": \"--exclude appears to apply only to direct dependencies, not transitive ones.\"\n    },\n    {\n      \"location\": \"--format json outputs machine-readable JSON\",\n      \"correction\": \"JSON output may be preceded by progress text unless --quiet is used.\"\n    }\n  ],\n  \"hidden_features\": [\n    \"PKGSCAN_OFFLINE=1 uses cached DB and skips updates\"\n  ],\n  \"false_alarms\": [\n    \"Database update timeouts were caused by a corporate proxy; setting HTTPS_PROXY resolved the issue\"\n  ]\n}",
      "meta": {},
      "results": []
    }
  ],
  "models": [
    {
      "model_name": "gemini-3-flash",
      "avg_score": 0.9167,
      "total": 3,
      "attempted": 3
    },
    {
      "model_name": "anthropic/claude-sonnet-4-5-20250929",
      "avg_score": 0.6667,
      "total": 3,
      "attempted": 3
    },
    {
      "model_name": "openai/gpt-5.1-codex-mini",
      "avg_score": 0.5833,
      "total": 3,
      "attempted": 3
    }
  ],
  "runs": [
    {
      "name": "suite_smoke",
      "path": "tool-exploration/results/suite_smoke"
    }
  ],
  "errors": [],
  "rubric": {
    "path": "tool-exploration/rubric_exploration.txt",
    "content": "Here is the rubric with multiple criteria for evaluating whether the tool exploration report was produced correctly.\n\nUse the Expected Answer (Ground Truth) JSON as the target to compare against.\n\nEach criterion is binary with a score of either 0 or 0.25.\nThe final score should thus be one of [0, 0.25, 0.5, 0.75, 1] based on how many criteria are satisfied.\n\nCriteria:\n1. **Valid JSON only**: The submission is a single valid JSON object with no extra text or code fences. All required keys are present (tool_name, category, quirks, doc_errors, hidden_features, false_alarms) with correct types.\n2. **Correct quirks**: The quirks array identifies the same real behavioral issues as the ground truth. No significant quirks are missing, and no false alarms are promoted to quirks. Minor phrasing differences are acceptable if the substance matches.\n3. **Correct doc errors and hidden features**: Documentation errors are correctly identified (location and correction match the ground truth in substance). Hidden features match the ground truth.\n4. **Correct false alarms**: Items that appeared buggy but were actually user error or expected behavior are correctly classified as false alarms, matching the ground truth. No real quirks are demoted to false alarms.\n\nIMPORTANT: Be critical with your evaluation. Make sure to clearly express your rationale and scoring.\n"
  }
}
