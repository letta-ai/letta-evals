{"sample_id": "skill-learn-001", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: log-triage\n\nTests (expected outcomes):\n- Given a mixed-format log blob containing both structured JSON lines ({\"level\":\"ERROR\",\"msg\":\"...\",\"timestamp\":\"...\"}) and unstructured syslog entries (e.g., \"May  3 14:22:01 app-server kernel: ...\"), parse each format separately and normalize into a unified schema with fields: timestamp, severity, source, message.\n- Group errors by signature: deduplicate stack traces by root cause (match on exception class + top 3 frames), and for non-exception errors, cluster by message template after stripping variable parts (IPs, UUIDs, timestamps, file paths). Report the top 5 signatures with occurrence counts and first/last seen timestamps.\n- For multi-line Java/Python stack traces, associate the full trace with the originating log line. Do not count continuation lines as separate errors.\n- Detect and redact the following secret patterns in all output: AWS access keys (AKIA...), JWT tokens (eyJ...), database connection strings (containing passwords), and Bearer tokens. Replace each with [REDACTED:<type>] (e.g., [REDACTED:AWS_KEY]).\n- When logs span multiple time zones, normalize all timestamps to UTC before correlation.\n- If fewer than 3 distinct error signatures exist, report all of them and note \"low error diversity -- consider checking log verbosity settings.\"\n\nConstraints:\n- Include a numbered parsing workflow that covers: format detection, normalization, deduplication, and redaction (in that order).\n- Provide a redaction rules table mapping each secret pattern to its regex and replacement string.\n- Include a concrete example showing a raw 5-line log snippet and the expected triage output.\n- Keep under 500 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-002", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: db-migration-planner\n\nTests (expected outcomes):\n- Given a list of PostgreSQL schema changes (ALTER TABLE, CREATE INDEX, DROP COLUMN, RENAME TABLE, ADD CONSTRAINT), produce an ordered migration plan where each step specifies: the DDL statement, estimated lock level (ACCESS EXCLUSIVE, ROW EXCLUSIVE, SHARE, or none), estimated duration category (instant / seconds / minutes / hours based on table size heuristics), and a rollback statement.\n- When a migration includes both an ADD COLUMN with DEFAULT on a large table (>1M rows) and a CREATE INDEX CONCURRENTLY, the plan must order the column addition before the index and note that ADD COLUMN ... DEFAULT on PostgreSQL <11 rewrites the entire table, while >=11 is instant.\n- Flag any step that acquires ACCESS EXCLUSIVE lock and suggest a safer alternative when one exists (e.g., CREATE INDEX CONCURRENTLY instead of CREATE INDEX, or a multi-step column rename via add/copy/drop instead of RENAME COLUMN).\n- For DROP COLUMN or DROP TABLE operations, require an explicit data backup step before execution and mark the step as \"irreversible\" in the rollback column.\n- Generate a pre-migration checklist that includes: pg_stat_activity check for active connections, replication lag check, disk space estimate for index builds, and a pg_dump command for targeted backup.\n- If a NOT NULL constraint is added without a DEFAULT, the plan must include a preceding UPDATE to backfill NULLs and warn about full-table lock during the ALTER.\n\nConstraints:\n- Use a markdown table for the migration plan with columns: Step, DDL, Lock Level, Duration Est., Rollback, Notes.\n- Include a \"Risk Assessment\" section that classifies overall migration risk as LOW / MEDIUM / HIGH based on the presence of destructive operations, lock durations, and table sizes.\n- Provide an example migration plan for a scenario with at least 3 schema changes of different risk levels.\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-003", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: api-client-generator\n\nTests (expected outcomes):\n- Given an OpenAPI 3.0+ specification snippet containing path definitions with requestBody and response schemas, generate a TypeScript client where each endpoint becomes a typed async function. The function signature must use the request body schema as the parameter type and the 200/201 response schema as the return type.\n- When the spec uses oneOf or discriminator in a response schema, generate a TypeScript discriminated union type using a literal type for the discriminator property (e.g., type Result = { kind: \"success\"; data: T } | { kind: \"error\"; code: number }).\n- Implement retry logic: on 429 responses, read the Retry-After header (seconds or HTTP-date format) and wait accordingly; on 5xx responses, use exponential backoff starting at 200ms with jitter, up to 3 retries. Do not retry 4xx errors other than 429.\n- Handle cursor-based pagination: when a response includes a next_cursor or nextPageToken field, generate a helper that returns an AsyncGenerator yielding individual items across all pages.\n- Support multiple auth schemes from the spec's securitySchemes: Bearer token via Authorization header, API key via custom header (e.g., X-API-Key), and OAuth2 client_credentials flow with automatic token refresh when the access token expires.\n- Generate proper error types: define an ApiError class that includes statusCode, responseBody, requestId (from x-request-id header), and the original request URL. Throw this instead of generic Error.\n\nConstraints:\n- Include a \"Generation Workflow\" section with steps: parse spec, extract schemas, generate types, generate endpoint functions, wire auth.\n- Show a concrete before/after example: a minimal OpenAPI path definition (3-4 lines of YAML) and the resulting TypeScript function signature.\n- Include a table of retry behavior per status code range (2xx, 429, 4xx, 5xx).\n- Keep under 550 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-004", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: meeting-notes\n\nTests (expected outcomes):\n- Given a raw meeting transcript (speaker-tagged lines in \"Speaker: utterance\" format), produce a structured summary with exactly these sections in order: TL;DR (1-2 sentences), Key Decisions, Action Items, Open Questions, and Parking Lot.\n- In \"Key Decisions,\" only include items where a speaker explicitly committed or the group reached consensus (e.g., \"let's go with X\", \"agreed\", \"we'll do Y\"). Opinions, suggestions, or hypotheticals (e.g., \"we could try X\", \"I think maybe\") must NOT appear as decisions.\n- Each Action Item must follow the format: \"- [ ] <task description> -- @<owner> (due: <date or 'TBD'>)\". If no owner is named, assign \"@unassigned\". If a speaker says \"I'll do X by Friday\" extract both the owner (that speaker) and the deadline.\n- When speakers disagree, capture both positions under \"Open Questions\" with attribution (e.g., \"@alice favors approach A; @bob prefers approach B\"). Do not silently resolve disagreements.\n- Extract deadlines and dates mentioned in natural language (\"by end of sprint\", \"next Tuesday\", \"before the board meeting on March 15\") and normalize to ISO 8601 format (YYYY-MM-DD) when a specific date is determinable. Use the original text in parentheses when the date is relative and unresolvable (e.g., \"due: TBD (end of sprint)\").\n- \"Parking Lot\" captures topics explicitly deferred (e.g., \"let's table that\", \"we'll revisit\", \"not for this meeting\") with the original speaker attributed.\n- If the transcript contains fewer than 2 speakers, prepend a warning: \"Note: Single-speaker transcript -- decisions and action items may lack group validation.\"\n\nConstraints:\n- Define exact markdown heading levels for each section (## for main sections).\n- Include a decision classification rule: provide 2-3 example phrases that qualify as decisions vs. 2-3 that do not.\n- Include a sample input transcript (4-6 lines of dialogue) and the expected structured output.\n- Keep under 500 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-005", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: security-pr-review\n\nTests (expected outcomes):\n- Given a code diff (unified diff format), identify security vulnerabilities and report each finding with: file path, line number(s), vulnerability type (mapped to CWE ID where applicable), severity (P0-P3), a one-line description, and a suggested fix.\n- Severity classification must follow these rules: P0 (Critical) = RCE, authentication bypass, SQL injection with user input, deserialization of untrusted data (CWE-502); P1 (High) = stored XSS, SSRF, path traversal, hardcoded secrets/credentials; P2 (Medium) = CSRF missing, open redirect, verbose error messages leaking internals, missing rate limiting on auth endpoints; P3 (Low) = missing security headers, cookie without Secure/HttpOnly flags, overly permissive CORS.\n- For dependency changes (package.json, requirements.txt, go.mod, Gemfile), check if any added or upgraded dependency has a known CVE pattern: flag any dependency pinned to a wildcard or range (e.g., \"^1.x\", \">=2.0\"), and flag additions of packages commonly associated with supply chain risk (e.g., packages with typosquatting patterns or very low download counts).\n- When the diff modifies authentication or authorization logic (login handlers, middleware, JWT validation, role checks), always flag for manual review regardless of whether a specific vulnerability is found, with the note: \"Auth logic change -- requires manual security review.\"\n- Detect unsafe patterns per language: Python (eval(), pickle.loads(), subprocess with shell=True, yaml.load() without SafeLoader), JavaScript (innerHTML assignment, eval(), dangerouslySetInnerHTML, child_process.exec with template literals), Go (sql.Query with string concatenation, use of crypto/md5 or crypto/sha1 for security purposes).\n- If no vulnerabilities are found, output: \"No security issues identified. Scope: <N files changed, M lines reviewed>.\" Do not fabricate findings.\n\nConstraints:\n- Output findings in a markdown table with columns: #, File:Line, CWE, Severity, Description, Suggested Fix.\n- Include a \"Review Scope\" header listing files reviewed and files skipped (e.g., test files, documentation) with rationale.\n- Include a decision tree or checklist for the severity classification (P0-P3) so the model can apply it consistently.\n- Provide 2 example findings (one P0/P1, one P2/P3) showing the exact table row format.\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-006", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: oauth-debugging\n\nTests (expected outcomes):\n- Given an OAuth2/OIDC authorization_code flow that fails with invalid_grant at the token endpoint, identify the most common root causes (redirect_uri mismatch, reused/expired code, PKCE verifier mismatch, clock skew) and list the exact values to compare across requests.\n- Given an OIDC ID token, describe how to validate iss, aud, exp, iat, nonce, and signature using the provider JWKS; explicitly distinguish what is validated client-side vs server-side.\n- Given an auth failure that only occurs in production, include checks for environment-specific misconfig (wrong redirect URI, missing allowed origins, different client_id/secret, different issuer/tenant) and logging/observability steps.\n- Provide a step-by-step debugging workflow that starts from the /authorize request and ends at resource API access, including where state/nonce are generated and verified.\n- Include a mapping table from common OAuth errors (invalid_request, unauthorized_client, invalid_client, invalid_grant, invalid_scope, access_denied) to likely causes and next checks.\n- Specify safe logging practices: never log access tokens, refresh tokens, auth codes, or client secrets; show redaction patterns for Bearer tokens and JWTs.\n- When the issue is intermittent, recommend what to retry (network/5xx/timeouts) vs what NOT to retry (invalid_grant due to mismatched redirect_uri/PKCE), with a brief rationale.\n- Explain how PKCE works at a practical level (code_verifier/code_challenge) and the typical failure pattern when code_verifier is lost between steps.\n- Include guidance for SPA vs backend apps: when to use confidential vs public clients, and where token exchange should happen.\n- Provide an example: a short authorize URL + token request snippet and a concise annotated checklist of what to verify.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include an ASCII flow diagram of the OAuth/OIDC steps.\n- Include a 2-column redaction rules table (pattern → replacement).\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-007", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: dockerfile-hardening\n\nTests (expected outcomes):\n- Given a Dockerfile that uses :latest, describe why it is risky and provide a pinning strategy (tag + digest) suitable for production.\n- Given a Dockerfile that runs as root, provide a minimal pattern to create and switch to a non-root user, including file ownership considerations for COPY.\n- Given a Dockerfile that installs packages, include best practices for apt/apk (no extra recommends, cache cleanup) and explain why it matters.\n- Given a build that leaks secrets via ENV/ARG, explain what leaks into layers and how to use BuildKit secrets or runtime env vars instead.\n- Provide a multi-stage build workflow that separates build dependencies from runtime image contents.\n- Include guidance for minimizing attack surface (slim base images, removing shells/tools, dropping capabilities where applicable).\n- Call out at least 5 common anti-patterns (curl|bash, ADD remote URLs, copying the entire repo, leaving package manager caches, running as root).\n- Provide a checklist that a reviewer can apply to any Dockerfile in under 2 minutes.\n- Include an example hardened Dockerfile skeleton for a generic web service.\n- Explain what to do when a container needs to write to disk (use /tmp, volumes, correct permissions) without reverting to root.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a 10-point checklist with concise bullets.\n- Include one short before/after example snippet (5-10 lines each).\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-008", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: k8s-crashloop-triage\n\nTests (expected outcomes):\n- Given a pod in CrashLoopBackOff, provide an ordered triage workflow using kubectl describe and kubectl logs --previous, and specify what signals to look for in each.\n- Recognize OOMKilled patterns and explain what exit code 137 implies; include the next action to confirm memory limits/usage.\n- Distinguish between liveness probe failures vs application crashes and describe how misconfigured probes can cause loops.\n- Handle multi-container pods: explain how to pick the right container logs and when init containers are the culprit.\n- Given ImagePullBackOff vs CrashLoopBackOff, explain the difference and how the next steps differ.\n- Include checks for configuration issues (missing env vars, bad ConfigMap/Secret mounts, wrong command/args) and show how to verify mounts.\n- Include a decision tree that routes to likely root causes (OOM, bad config, dependency unavailable, probe misconfig, crash on startup).\n- Recommend safe mitigations (scale down, disable probe temporarily, increase resources) and clearly label which are temporary.\n- Provide an example scenario with short kubectl outputs (describe + logs excerpt) and the expected diagnosis.\n- Include guidance for making crashes reproducible locally (run the container entrypoint, enable verbose logging).\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a numbered workflow (at least 7 steps).\n- Include a compact decision tree (ASCII ok).\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-009", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: sql-performance-triage\n\nTests (expected outcomes):\n- Given a slow PostgreSQL query and an EXPLAIN ANALYZE plan, explain how to identify the primary bottleneck (Seq Scan, sort, nested loop explosion, hash join spill) and what evidence to cite.\n- Provide a cheat sheet mapping common plan nodes to meaning and typical fixes.\n- Include guidance for index selection: when to use composite indexes, partial indexes, and INCLUDE (covering) indexes.\n- Explain why adding an index may not help when selectivity is low or predicates don’t match index order.\n- Include steps to verify statistics freshness (ANALYZE, autovacuum) and how stale stats mislead the planner.\n- Include safe production practices: avoid long locks; use CREATE INDEX CONCURRENTLY and measure impact.\n- Include guidance on pagination: prefer keyset pagination over OFFSET for large offsets, and explain why.\n- Given a query that is slow only for certain parameters, include how to diagnose parameter sensitivity and when to use prepared statements cautiously.\n- Provide a minimal example with a bad query + an improved query/index and show the reasoning.\n- Include a short section on what not to do (e.g., disabling seqscan globally, adding random indexes without measuring).\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a 3-column cheat sheet table: Node → Meaning → Typical Fix.\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-010", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: release-notes-generator\n\nTests (expected outcomes):\n- Given a list of PR titles/commit messages, rewrite them into user-facing release notes grouped into: Features, Fixes, Performance, Breaking Changes, Deprecations.\n- Deduplicate near-duplicate items and merge related PRs into a single bullet when appropriate.\n- Preserve and surface ticket IDs (e.g., APP-1234) when present.\n- Exclude internal-only changes (refactors, formatting, dependency bumps) unless they have user impact.\n- For breaking changes, include a short migration note that tells a user what action to take.\n- When a PR title is vague (e.g., \"fix stuff\"), describe a strategy to infer impact from context and state assumptions explicitly.\n- Include a consistent tone guideline: plain language, past tense, no engineering jargon.\n- Provide a template that can be reused for any release.\n- Include an example input (5-8 PR titles) and an example output release note.\n- Ensure the output is concise (aim for <15 bullets total for a typical release).\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a reusable template section.\n- Include one example input/output pair.\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-011", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: data-quality-monitoring\n\nTests (expected outcomes):\n- Given a daily partitioned analytics table, define checks for freshness, volume, null rates, uniqueness, referential integrity, and distribution drift.\n- Explain how to set thresholds using baselines (e.g., trailing 7/30 days) and how to handle seasonality.\n- Distinguish hard-fail checks (block downstream) vs soft-fail checks (alert only), and give examples of each.\n- Provide a simple config schema (YAML-like) to declare checks per table/column.\n- Include how to handle time zones: normalize event timestamps and use partition dates consistently.\n- Provide an incident response playbook: verify upstream, identify first bad partition, backfill strategy, communicate impact.\n- Include guidance for noisy alerts (hysteresis, minimum sample size, smoothing) and avoiding alert fatigue.\n- Include an example config for one table with at least 5 checks.\n- Include a short example of an alert message that contains the key context (metric, threshold, current value, affected partition, link placeholders).\n- Include a section on governance: who owns which checks and how to review/retire checks.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include the config schema in a fenced yaml code block.\n- Include one example config + one example alert.\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-012", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: a11y-quick-audit\n\nTests (expected outcomes):\n- Given a web UI description, produce a quick accessibility audit checklist covering keyboard navigation, focus management, labels, contrast, headings/landmarks, and error messaging.\n- Include common fix patterns: label+input association, button semantics, aria-label usage, and avoiding clickable divs.\n- Explain when to use ARIA and when native HTML is preferred.\n- Include guidance for testing with only the keyboard (Tab/Shift+Tab/Enter/Space) and what failures look like.\n- Include guidance for screen reader smoke tests (what to check, not tool-specific commands).\n- Provide a severity rubric (BLOCKER/MAJOR/MINOR) and how to prioritize fixes.\n- Include at least 3 short code snippets showing correct patterns (button, form field, dialog).\n- Include an example mini-report for a page with 3 findings, each with severity and recommended fix.\n- Mention automated tooling as a supplement and list what it typically catches vs misses.\n- Include advice for regression prevention (component library patterns, PR checklist).\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a severity rubric table.\n- Include exactly 3 code snippets (keep each under 8 lines).\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-013", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: feature-flag-rollout\n\nTests (expected outcomes):\n- Given a new risky feature, produce a rollout plan with phases (dark launch, internal, small %, ramp-up, full) and a monitoring checklist per phase.\n- Include guidance for naming flags, scoping (user/workspace), and making a kill switch fast.\n- For features that require data migrations, include a safe strategy (dual-write, backfill, read-gating, cleanup) and how the flag interacts with each step.\n- Explain how to pick ramp percentages and what metrics should gate progression (error rate, latency, key business KPI).\n- Include explicit rollback criteria and what to do when metrics regress.\n- Include guidance for long-lived flags: ownership, expiration dates, and removal plan.\n- Include an example rollout schedule over 1 week with concrete percentages and check-ins.\n- Include how to communicate rollout status to support/CS.\n- Include a risk matrix (impact × likelihood) and how it affects rollout conservatism.\n- Include one example incident scenario (metric spike at 20% rollout) and the expected response.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a phased rollout table.\n- Include one example schedule and one incident response example.\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-014", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: customer-email-triage\n\nTests (expected outcomes):\n- Given an inbound support email, extract a structured internal triage summary: issue, severity, customer impact, suspected area, missing info, next actions.\n- Generate a customer-facing reply that is professional, empathetic, and asks only the minimum clarifying questions needed.\n- Include PII/secret handling: redact tokens, account numbers, addresses; never quote secrets back to the customer.\n- Define escalation triggers (security incident, data loss, widespread outage, VIP customer) and how to route them.\n- Include guidance for setting expectations (ETA vs next update time) without overpromising.\n- Provide a checklist of the most useful diagnostic info to request (timestamps with timezone, request IDs, steps to reproduce, environment, screenshots).\n- Include an example with a short customer email and the expected internal triage + reply.\n- Include guidance for when to move to a call and what to prepare.\n- Include how to tag/classify tickets consistently (billing, auth, performance, UI, integrations).\n- Ensure outputs separate internal notes from external reply clearly.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Use a two-section format: Internal Triage / Customer Reply.\n- Include one example email + example outputs.\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-015", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: pytest-flaky-test-troubleshooting\n\nTests (expected outcomes):\n- Given a flaky pytest in CI, provide a step-by-step workflow to reproduce (repeat runs, isolate tests, run with -k, run with -x) and capture evidence.\n- Enumerate common root cause categories (shared global state, ordering dependence, time, randomness, network, async race conditions, resource leaks) and how to detect each.\n- Provide a command cookbook with at least 6 commands for narrowing down flakes.\n- Provide deterministic fix patterns (freeze time, mock network, use tmp_path, reset global state/fixtures, avoid sleeping, add proper waits).\n- Explain when rerun-on-failure is acceptable as a mitigation vs when it masks real issues.\n- Include how to identify hidden ordering dependencies (e.g., by running tests in different order) and what to do once found.\n- Include guidance on parallelism issues (pytest-xdist style) and how to detect shared filesystem/ports collisions, without requiring extra plugins.\n- Include an example: a flaky test symptom and the expected diagnosis/fix.\n- Include a short section on quarantining a test (marking/skipping) with criteria and follow-up actions.\n- Ensure the workflow works without requiring any third-party pytest plugins.\n\nConstraints:\n- Include YAML frontmatter with name and description.\n- Include a root-cause categories table.\n- Include a command cookbook section.\n- Keep under 650 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
