{"sample_id": "skill-learn-001", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: log-triage\n\nTests (expected outcomes):\n- Given a mixed-format log blob containing both structured JSON lines ({\"level\":\"ERROR\",\"msg\":\"...\",\"timestamp\":\"...\"}) and unstructured syslog entries (e.g., \"May  3 14:22:01 app-server kernel: ...\"), parse each format separately and normalize into a unified schema with fields: timestamp, severity, source, message.\n- Group errors by signature: deduplicate stack traces by root cause (match on exception class + top 3 frames), and for non-exception errors, cluster by message template after stripping variable parts (IPs, UUIDs, timestamps, file paths). Report the top 5 signatures with occurrence counts and first/last seen timestamps.\n- For multi-line Java/Python stack traces, associate the full trace with the originating log line. Do not count continuation lines as separate errors.\n- Detect and redact the following secret patterns in all output: AWS access keys (AKIA...), JWT tokens (eyJ...), database connection strings (containing passwords), and Bearer tokens. Replace each with [REDACTED:<type>] (e.g., [REDACTED:AWS_KEY]).\n- When logs span multiple time zones, normalize all timestamps to UTC before correlation.\n- If fewer than 3 distinct error signatures exist, report all of them and note \"low error diversity -- consider checking log verbosity settings.\"\n\nConstraints:\n- Include a numbered parsing workflow that covers: format detection, normalization, deduplication, and redaction (in that order).\n- Provide a redaction rules table mapping each secret pattern to its regex and replacement string.\n- Include a concrete example showing a raw 5-line log snippet and the expected triage output.\n- Keep under 500 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-002", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: db-migration-planner\n\nTests (expected outcomes):\n- Given a list of PostgreSQL schema changes (ALTER TABLE, CREATE INDEX, DROP COLUMN, RENAME TABLE, ADD CONSTRAINT), produce an ordered migration plan where each step specifies: the DDL statement, estimated lock level (ACCESS EXCLUSIVE, ROW EXCLUSIVE, SHARE, or none), estimated duration category (instant / seconds / minutes / hours based on table size heuristics), and a rollback statement.\n- When a migration includes both an ADD COLUMN with DEFAULT on a large table (>1M rows) and a CREATE INDEX CONCURRENTLY, the plan must order the column addition before the index and note that ADD COLUMN ... DEFAULT on PostgreSQL <11 rewrites the entire table, while >=11 is instant.\n- Flag any step that acquires ACCESS EXCLUSIVE lock and suggest a safer alternative when one exists (e.g., CREATE INDEX CONCURRENTLY instead of CREATE INDEX, or a multi-step column rename via add/copy/drop instead of RENAME COLUMN).\n- For DROP COLUMN or DROP TABLE operations, require an explicit data backup step before execution and mark the step as \"irreversible\" in the rollback column.\n- Generate a pre-migration checklist that includes: pg_stat_activity check for active connections, replication lag check, disk space estimate for index builds, and a pg_dump command for targeted backup.\n- If a NOT NULL constraint is added without a DEFAULT, the plan must include a preceding UPDATE to backfill NULLs and warn about full-table lock during the ALTER.\n\nConstraints:\n- Use a markdown table for the migration plan with columns: Step, DDL, Lock Level, Duration Est., Rollback, Notes.\n- Include a \"Risk Assessment\" section that classifies overall migration risk as LOW / MEDIUM / HIGH based on the presence of destructive operations, lock durations, and table sizes.\n- Provide an example migration plan for a scenario with at least 3 schema changes of different risk levels.\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-003", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: api-client-generator\n\nTests (expected outcomes):\n- Given an OpenAPI 3.0+ specification snippet containing path definitions with requestBody and response schemas, generate a TypeScript client where each endpoint becomes a typed async function. The function signature must use the request body schema as the parameter type and the 200/201 response schema as the return type.\n- When the spec uses oneOf or discriminator in a response schema, generate a TypeScript discriminated union type using a literal type for the discriminator property (e.g., type Result = { kind: \"success\"; data: T } | { kind: \"error\"; code: number }).\n- Implement retry logic: on 429 responses, read the Retry-After header (seconds or HTTP-date format) and wait accordingly; on 5xx responses, use exponential backoff starting at 200ms with jitter, up to 3 retries. Do not retry 4xx errors other than 429.\n- Handle cursor-based pagination: when a response includes a next_cursor or nextPageToken field, generate a helper that returns an AsyncGenerator yielding individual items across all pages.\n- Support multiple auth schemes from the spec's securitySchemes: Bearer token via Authorization header, API key via custom header (e.g., X-API-Key), and OAuth2 client_credentials flow with automatic token refresh when the access token expires.\n- Generate proper error types: define an ApiError class that includes statusCode, responseBody, requestId (from x-request-id header), and the original request URL. Throw this instead of generic Error.\n\nConstraints:\n- Include a \"Generation Workflow\" section with steps: parse spec, extract schemas, generate types, generate endpoint functions, wire auth.\n- Show a concrete before/after example: a minimal OpenAPI path definition (3-4 lines of YAML) and the resulting TypeScript function signature.\n- Include a table of retry behavior per status code range (2xx, 429, 4xx, 5xx).\n- Keep under 550 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-004", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: meeting-notes\n\nTests (expected outcomes):\n- Given a raw meeting transcript (speaker-tagged lines in \"Speaker: utterance\" format), produce a structured summary with exactly these sections in order: TL;DR (1-2 sentences), Key Decisions, Action Items, Open Questions, and Parking Lot.\n- In \"Key Decisions,\" only include items where a speaker explicitly committed or the group reached consensus (e.g., \"let's go with X\", \"agreed\", \"we'll do Y\"). Opinions, suggestions, or hypotheticals (e.g., \"we could try X\", \"I think maybe\") must NOT appear as decisions.\n- Each Action Item must follow the format: \"- [ ] <task description> -- @<owner> (due: <date or 'TBD'>)\". If no owner is named, assign \"@unassigned\". If a speaker says \"I'll do X by Friday\" extract both the owner (that speaker) and the deadline.\n- When speakers disagree, capture both positions under \"Open Questions\" with attribution (e.g., \"@alice favors approach A; @bob prefers approach B\"). Do not silently resolve disagreements.\n- Extract deadlines and dates mentioned in natural language (\"by end of sprint\", \"next Tuesday\", \"before the board meeting on March 15\") and normalize to ISO 8601 format (YYYY-MM-DD) when a specific date is determinable. Use the original text in parentheses when the date is relative and unresolvable (e.g., \"due: TBD (end of sprint)\").\n- \"Parking Lot\" captures topics explicitly deferred (e.g., \"let's table that\", \"we'll revisit\", \"not for this meeting\") with the original speaker attributed.\n- If the transcript contains fewer than 2 speakers, prepend a warning: \"Note: Single-speaker transcript -- decisions and action items may lack group validation.\"\n\nConstraints:\n- Define exact markdown heading levels for each section (## for main sections).\n- Include a decision classification rule: provide 2-3 example phrases that qualify as decisions vs. 2-3 that do not.\n- Include a sample input transcript (4-6 lines of dialogue) and the expected structured output.\n- Keep under 500 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
{"sample_id": "skill-learn-005", "input": "You are given a set of tests with expected outcomes. Write a SKILL.md that would help a model pass these tests.\n\nSkill name: security-pr-review\n\nTests (expected outcomes):\n- Given a code diff (unified diff format), identify security vulnerabilities and report each finding with: file path, line number(s), vulnerability type (mapped to CWE ID where applicable), severity (P0-P3), a one-line description, and a suggested fix.\n- Severity classification must follow these rules: P0 (Critical) = RCE, authentication bypass, SQL injection with user input, deserialization of untrusted data (CWE-502); P1 (High) = stored XSS, SSRF, path traversal, hardcoded secrets/credentials; P2 (Medium) = CSRF missing, open redirect, verbose error messages leaking internals, missing rate limiting on auth endpoints; P3 (Low) = missing security headers, cookie without Secure/HttpOnly flags, overly permissive CORS.\n- For dependency changes (package.json, requirements.txt, go.mod, Gemfile), check if any added or upgraded dependency has a known CVE pattern: flag any dependency pinned to a wildcard or range (e.g., \"^1.x\", \">=2.0\"), and flag additions of packages commonly associated with supply chain risk (e.g., packages with typosquatting patterns or very low download counts).\n- When the diff modifies authentication or authorization logic (login handlers, middleware, JWT validation, role checks), always flag for manual review regardless of whether a specific vulnerability is found, with the note: \"Auth logic change -- requires manual security review.\"\n- Detect unsafe patterns per language: Python (eval(), pickle.loads(), subprocess with shell=True, yaml.load() without SafeLoader), JavaScript (innerHTML assignment, eval(), dangerouslySetInnerHTML, child_process.exec with template literals), Go (sql.Query with string concatenation, use of crypto/md5 or crypto/sha1 for security purposes).\n- If no vulnerabilities are found, output: \"No security issues identified. Scope: <N files changed, M lines reviewed>.\" Do not fabricate findings.\n\nConstraints:\n- Output findings in a markdown table with columns: #, File:Line, CWE, Severity, Description, Suggested Fix.\n- Include a \"Review Scope\" header listing files reviewed and files skipped (e.g., test files, documentation) with rationale.\n- Include a decision tree or checklist for the severity classification (P0-P3) so the model can apply it consistently.\n- Provide 2 example findings (one P0/P1, one P2/P3) showing the exact table row format.\n- Keep under 600 words.\n\nOutput the SKILL.md content only (no code fences).", "ground_truth": ""}
