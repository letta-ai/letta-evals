# Configuration for Letta File Benchmark

# Agent configuration (used by setup_cloud_agent.py)
agent:
  max_files_open: 5
  per_file_view_window_char_limit: 12000

# Question generation configuration
generation:
  # Question type distribution (must sum to 1.0)
  # Each type has a corresponding prompt file in prompts/{type_name}.md
  question_types:
    multi_hop_chain: 0.25       # TWO parallel chains + compare (hardest pattern)
    multi_entity_comparison: 0.30  # TWO parallel chains + compare (hardest pattern)
    aggregation: 0.10           # 3-5 files — total balance, sum records
    set_intersection: 0.10      # 4-5 files — person matching X AND Y AND Z
    comparison_tiebreak: 0.10   # 4-5 files — highest Y, if tied oldest
    negation: 0.05              # 3-4 files — who does NOT own X
    cross_file_counting: 0.05   # 4-5 files — total financial products
    temporal_reasoning: 0.05    # 3-4 files — date comparisons

# Agent question generator configuration
agent_question_generator:
  # Context window management
  trim_threshold: 140000      # Start trimming when we hit this many input tokens

  # Model configuration
  default_model: "claude-opus-4-5-20251101"

  # Token usage
  max_tokens_per_response: 4096

  # Generation parameters
  max_iterations_per_question: 100
  force_registration_on_max_iterations: true

  # LLM summarization settings (for context trimming)
  summary_max_tokens: 500
  summary_temperature: 0.3
  summary_max_conversation_items: 30

  # Display settings
  separator_length: 80
  truncate_result_rows: 3
  truncate_result_string_length: 200

  # Session settings
  messages_to_keep_on_trim: 6
